<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Simon Dobson (old posts, page 1)</title>
<style>
	@font-face {
	    font-family: "libretto-icons";
	    src:url(../assets/fonts/libretto-icons.eot);
	    src:url(../assets/fonts/libretto-icons.eot#iefix) format("embedded-opentype"),
	    url(../assets/fonts/libretto-icons.woff) format("woff"),
	    url(../assets/fonts/libretto-icons.ttf) format("truetype"),
	    url(../assets/fonts/libretto-icons.svg#libretto-icons) format("svg");
	    font-weight: normal;
	    font-style: normal;
	}
    </style>
<link rel="icon" href="../images/favicon.png" sizes="16x16">
<link rel="alternate" type="application/rss+xml" href="../rss.xml">
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Droid+Sans+Mono%7CLibre+Baskerville%7CMontserrat%7CPlayfair+Display%7CTangerine">
<link rel="stylesheet" href="../assets/css/libretto_styles.css">
<link rel="stylesheet" href="../assets/css/baguetteBox.min.css">
<link rel="stylesheet" href="../assets/css/code.css">
<link rel="stylesheet" href="../assets/css/nikola_rst.css">
<link rel="stylesheet" href="../assets/css/nikola_ipython.css">
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.0.13/css/all.css" integrity="sha384-DNOHZ68U8hZfKXOrtjWvjxusGo9WQnrNx2sqG0tfsghAvtVlRW3tvkXWZh58N9jp" crossorigin="anonymous">
</head>
<body>
    <!-- Navigation bar -->
    <header class="nav-bar"><div class="site-branding">
	    <h1 class="site-title">
		<a href="https://simondobson.org/" title="Simon Dobson" rel="home">Simon&nbsp;Dobson</a>
	    </h1>
	</div>
	<nav class="site-navigation" role="navigation"><div class="menu-toggle">
		<span class="mobile-site-title">Simon Dobson</span>
	    </div>
	    <ul class="menu">
<li><a href="../index.html">Home</a></li>
		    <li><a href="../personal/">About&nbsp;me</a></li>
		    <li><a href="../research/">Research</a></li>
		    <li><a href="../development/projects/">Software</a></li>
		    <li><a href="../writing/">Writing</a></li>
		    <li><a href="../personal/contact/">Contact</a></li>
		<li>
<a href="../rss.xml"><i class="fa fa-rss"></i></a>
	    </li>
</ul></nav></header><div class="site-content">
	    <article class="format-standard libretto-long-form"><div class="title-block post-format-icon-pin">
		    <div class="entry-meta">
			<span class="posted-on">
			    Posted on <a href="../2010/03/26/five-big-questions/" rel="bookmark">Friday 26 March, 2010</a>
			</span>
		    </div>
		    <h1><a href="../2010/03/26/five-big-questions/">Five big&nbsp;questions</a></h1>
		</div>
		<div class="entry-content">
			<p>If you try to do everything, you always end up doing nothing. Which is why <a href="http://www.simondobson.org/2010/03/both-ends-of-the-data-intensive-spectrum/">Gray&#8217;s laws</a> suggest searching for the twenty &#8220;big questions&#8221; in a field and then focusing-in the first five as the ones that&#8217;ll generate the biggest return on the effort invested. So what are the five biggest open issues in programming for sensorised systems?

<!--more-->Of course we should start with a big fat disclaimer: these are <em>my</em> five biggest open issues, which probably don&#8217;t relate well to anyone else&#8217;s &#8212; but that&#8217;s what blogs are for, right? :-) So here goes: five questions, with an associated suggestion for a research programme.

<strong>1. Programming with uncertainty.</strong> This is definitely the one I feel is most important. I&#8217;ve <a href="http://www.simondobson.org/2010/02/216/">mentioned before</a> that there&#8217;s a mismatch between traditional computer science and what we have to deal with for sensor systems: the input is uncertain and often of very poor quality, but the output  behaviour has to be a &#8220;best attempt&#8221; based on what&#8217;s available and has to be robust against small perturbations due to noise and the like. But uncertainty is something that computers (and computer scientists) are quite bad at handling, so there&#8217;s a major change that has to happen.

To deal with this we need to re-think the programming models we use, and the ways in which we express behaviour. For example we could look at how programs respond to perturbation, or design languages in which perturbations have a small impact by design. A calculus of <em>stable functions</em> might be a good starting-point, where perturbation tends to die-out over time and space, but long-term changes are propagated. We might also look at how to program more effectively with Bayesian statistics, or how to program with machine leaning: turn things that are currently either libraries or applications into core constructs from which to build programs.

<strong>2. Modeling adaptive systems as a whole.</strong> We&#8217;ve had a huge problem getting systems to behave according to specification: now we propose that they adapt in response to changing circumstances. Clearly the space of possible stimuli and responses are too large for exhaustive testing, or for formal model-checking, so correctness becomes a major issue. What we&#8217;re really interested in, of course, isn&#8217;t so much specifying <em>what happens</em> as much as how what happens <em>changes over time and with context</em>.

Holistic models are common in physics but uncommon in computer science, where more discrete approaches (like model checking) have been more popular. It&#8217;s easy to see why this is the case, but a small-scale, pointwise formal method doesn&#8217;t feel appropriate to the scale of the problem. Reasoning about a system as a whole means re-thinking how we express both specifications and programs. But the difference is target is important too: we don&#8217;t need to capture all the detail of a program&#8217;s behaviour, just those aspects that relate to properties like stability, response time, accuracy and the like &#8212; a macro method for reasoning about macro properties, not something that gets lost in the details. Dynamical systems might be a good model, at least at a conceptual level, with adaptation being seen as a &#8220;trajectory&#8221; through the &#8220;space&#8221; of acceptable parameter values. At the very least this makes adaptation an object of study in its own right, rather than being something that happens within another, less well-targeted model.

<strong>3. Modeling complex space- and time-dependent behaviours.</strong> Reasoning systems and classifiers generally only deal with instants: things that are decided by the state of the system now, or as what immediately follows from now. In many cases what happens is far richer than this, and one can make predictions (or at least calculate probabilities) about the future based on classifying a person or entity as being engaged in a particular process. In pervasive computing this manifests itself as the ways in which people move around a space, the services they access preferentially in some locations rather than others, and so forth. These behaviours are closely tied-up with the way people move and the way their days progress, as it were: complex spatio-temporal processes giving rise to complex behaviours. The complexities come from how we divide-up people&#8217;s actions, and how the possibilities branch to give a huge combinatorial range of possibilities &#8212; not all of which are equally likely, and so can be leveraged.

A first step at addressing this would be to look at how we represent real-world spatio-temporal processes with computers. Of course we represent such processes all the time as programs, but (linking back to point 1 above) the uncertainties involved are such that we need to think about these things in new ways. We have a probabilistic definition of the potential future evolutions, against which we need to be able to express behaviours synthesising the &#8220;best guesses&#8221; we can make and simultaneously use the data we actually observe to validate or refute our predictions and refine our models. The link between programming and the modelingthat underlies it looks surprisingly intimate.

<strong>4. Rich representations of linked data.</strong> Sensors generate a <em>lot</em> of data. Much of it has long-term value, if only for verification and later re-study. Keeping track of all this data is going to become a major challenge. It&#8217;s not something that the scientists for whom it&#8217;s collected are generally very good at &#8212; and why should they be, given that their interests are in the science and not in data management? But the data has to be kept, has to be retrievable, and has to be associated with enough metadata to make its properly and validly interpretable in the future.

Sensor mark-up languages like <a href="http://www.opengeospatial.org/standards/sensorml">SensorML</a> are a first step, but <em>only</em> a first step. There&#8217;s also the issue of the methodology by which the data was collected, and especially (returning to point 2) were the behaviours of the sensors consistent with gaining a valid view of the phenomena of interest? That means linking data to process descriptions, or to code, so that we can track-back through the provenance to ensure integrity. Then we can start applying reasoners to classify and derive information automatically from the data, secure in the knowledge that we have an audit trail for the science.

<strong>5. Making it easier to build domain-specific languages for real.</strong> A <a href="http://en.wikipedia.org/wiki/Domain-specific_language">lot has been said about DSLs</a>, much of it negative: if someone&#8217;s learned C (or Java, or Fortran, or Matlab, or Perl, or&#8230;) they won&#8217;t want to then learn something else just to work in a particular domain. This argument holds that it&#8217;s therefore more appropriate to provide advanced functions as libraries accessed from a common host language (or a range of languages). The counter-argument is that libraries only work around the edges of a language and can&#8217;t provide the strength of optimisation, type-checking and new constructs needed. I suspect that there&#8217;s truth on both sides, and I also suspect that many power users would gladly learn a new language if it <em>really</em> matched their domain and <em>really</em> gave them leverage.

Building DSLs is too complicated, though, especially for real-world systems that need to run with reasonable performance on low-grade hardware. A good starting-point might be a system that would allow libraries to be wrapped-up with language-like features &#8212; like <a href="http://en.wikipedia.org/wiki/Tcl">Tcl</a> was intended for, but with more generality in terms of language constructs and types. A simpler language-design framework would facilitate work on new languages (as <em>per</em> point 1 above), and would allow us to search for modes of expression closer to the semantic constructs we think are needed (<em>per</em> points 2 and 3): starting from semantics and deriving a language rather than <em>vice versa</em>.</p>
		</div>
	    </article>
</div>
	<div class="site-content">
	    <article class="format-standard libretto-long-form"><div class="title-block post-format-icon-pin">
		    <div class="entry-meta">
			<span class="posted-on">
			    Posted on <a href="../2010/03/22/both-ends-of-the-data-intensive-spectrum/" rel="bookmark">Monday 22 March, 2010</a>
			</span>
		    </div>
		    <h1><a href="../2010/03/22/both-ends-of-the-data-intensive-spectrum/">Both ends of the data-intensive&nbsp;spectrum</a></h1>
		</div>
		<div class="entry-content">
			<p>Data-intensive science (or &#8220;web science&#8221; as it is sometimes known) has received a major boost from the efforts of Googlle and others, with the availability of enormous data sets against which we can learn. It&#8217;s easy to see that such large-scale data affects experimental science, but there are lessons further down the scale too.

<!--more-->I spent part of last week at a workshop on data-intensive computing hosted by the <a href="http://www.nesc.ac.uk">National e-Science Centre</a> in Edinburgh. It was a fscinating meeting, and I very grateful for having been invited. Most of the sessions focussed on the challenges of petabyte and exabyte data, but it struck me that many of the points were actually relative rather than absolute: problems that are data-intensive because they have large amounts of data relative to the processing power you can deploy against them. This got me thinking to what extent the characteristics of data-intensive systes can be applied to sensor systems too.

One of the most interesting points was made early on, by <a href="http://www.sdss.jhu.edu/~szalay/">Alex Szalay</a> of the <a href="http://www.sdss.org/">Sloan Digital Sky Survey</a>, who set out some desiderata for data intensive computing first made by the late <a href="http://en.wikipedia.org/wiki/Jim_Gray_%28computer_scientist%29">Jim Gray</a> &#8212; &#8220;Gray&#8217;s&nbsp;laws&#8221;:
</p>
<ol>
<li> Scientific computing revolves around data — <em>not</em>&nbsp;computing</li>
    <li> Build solutions that intrisically can scale-out  as&nbsp;required</li>
    <li> Take the analysis to  the data — because the interesting data&#8217;s almost certainly too big to move, even with  fast&nbsp;backbones</li>
    <li> Start with asking for&#8221;20 queries&#8221; — the most important questions— and recognise that the first 5 will be  by far the most&nbsp;important</li>
    <li> Go from &#8220;working to working&#8221; — assume that  the infrastructure will change every 2 — 5 years, and  design hardware and software&nbsp;accordingly</li>
</ol>
This is advice hard-won from practice, and it&#8217;s easy to see how it affects the largest-scale systems. I think Gray&#8217;s laws also work at the micro-scale of sensor networks, and at points in between.

Data-intensive science is perhaps better envisioned as data-<em>driven</em> science, in which the data drives the design and evolution computation. This view unifies large- and small-scales: a sensor network needs to respond to the observations it makes of the phenomane it&#8217;s sensing, even though the scale of the data (for an individual node) is so small.

By focusing on networking we can scale-out solutions, but we also need to consider that several different networks may be needed to take in the different aspects of systems being observed. It&#8217;s a <a href="../2010/03/things-that-wont-change/">mistake</a> to think that we can grow the capabilities of individual nodes too much, since that starts to eat into power budgets. At a data centre level, scale-out tends to mean virtualisation and replication: proven parallel-processing design patterns. At a network level, though, I suspect it means composition and co-design, which we understand substantially less well.

Taking the analysis to the data means pushing processing down into the network and reducing the amount of raw data that needs to be returned to a base station. This is a slightly contentious point: do I <em>want</em> to limit the raw data I collect, or should I grab it all in case something emerges later that I need to check against a raw stream? In other words, can I define the information I want to extract from the data stream sufficiently clearly to avoid storing it all? This same point was made by <a href="http://www.roe.ac.uk/~afh/">Alan Heavens</a> at the meeting, pointing out that one can do radical data discarding if one has a strong enough model of the pheonmenon against which to evaluate the raw data. Again, the point may be the scale if the data <em>relative to the platform on which it&#8217;s being processed</em> rather than in any absolute sense: astonomical data is too, well, &#8220;astronomical&#8221; to retain even in a large data centre, while sensor data is large relative to node capabilities. It&#8217;s an open question whether many systems have strong enough data models to support aggressive discarding, though.

The &#8220;20 queries&#8221; goal is I think key to many things: identify the large questions and address them first. (Richard Hamming <a href="http://www.cs.virginia.edu/~robins/YouAndYourResearch.html">made a similar point</a> with regard to research as a whole.) Coupling sensing research to the science (and public policy formation) that needs it is the only way to do this, and strikes me as at least as important as theoretical advances in network sensing science. The engineering challenges of (for example) putting a sensor network into a field are at least as valuable &#8212; and worthy of support &#8212; as the basic underpinnings.

The coupling of computer and physical science also speaks the the need for designing systems for upgrade. The techniques for doinjg this &#8212; component design and so forth &#8212; are well-explored by computer scientists and under-understood by many practitioners from other disciplines. Designing sensor <em>and</em> data systems for expansion and re-tasking should form a backbone of any research effort.

So I think Jim Gray&#8217;s pioneering insights into large-scale data may actually be broader than we think, and might also characterise the individually small-scale &#8212; but collectively widespread &#8212; instrumentation of the physical world. It also suggests that there are end-to-end issues that can usefully form part of the research agenda.
		</div>
	    </article>
</div>
	<div class="site-content">
	    <article class="format-standard libretto-long-form"><div class="title-block post-format-icon-pin">
		    <div class="entry-meta">
			<span class="posted-on">
			    Posted on <a href="../2010/03/12/forth-for-sensors/" rel="bookmark">Friday 12 March, 2010</a>
			</span>
		    </div>
		    <h1><a href="../2010/03/12/forth-for-sensors/">Forth for&nbsp;sensors?</a></h1>
		</div>
		<div class="entry-content">
			<p>Most sensor systems are programmed using C: compact and well-known, but low-level and tricky to get right when things get compact and complex. There have been several proposals for alternative languages from across the programming language research spectrum. I haven&#8217;t heard anyone mention Forth, though, and it&#8217;s worth considering &#8212; even if only as a target for other languages.

<!--more-->Many people will never have encountered <a href="http://en.wikipedia.org/wiki/Forth_%28programming_language%29">Forth</a>, a language that&#8217;s enjoyed up-and-down popularity for over four decades without ever hitting the mainstream. Sometimes touted as an alternative to Basic, it even had an <a href="http://en.wikipedia.org/wiki/Jupiter_Ace">early-1980&#8217;s home computer</a> that used it as the introductory language.

Forth has a number of characteristics that are completely different to the vast majority of modern&nbsp;languages:
</p>
<ul>
<li>It uses and explicit data stack and <a href="http://en.wikipedia.org/wiki/Reverse_Polish_notation">Reverse-Polish notation</a> uniformly throughout the&nbsp;language</li>
    <li>There&#8217;s no type system. Everything is represented pretty much using addresses and integers. The programmer is on her own when building complex&nbsp;structures</li>
    <li>It is a threaded interpreter where every construct in the language is a &#8220;word&#8221;. Composing words together generates new words, but (unlike in an interpreter) the definitions are compiled efficiently, so there&#8217;s an immediacy to things without crippling performance&nbsp;overheads</li>
    <li>A standard system usually mixes its &#8220;shell&#8221; and &#8220;compiler&#8221; together, so one can define new words interactively which get compiled&nbsp;immediately</li>
    <li>There&#8217;s a small kernel of machine-code (or C) routines,&nbsp;but&#8230;</li>
    <li>The compiler itself &#8212; and indeed the vast majority of the system &#8212; can be written in Forth, so you can extend the compiler (and hence the language) with new constructs that have first-class status alongside the built-in words. There&#8217;s typically almost no overhead of programmer- <em>versus</em> system-defined words, since they&#8217;re all written in the same (fast)&nbsp;language</li>
    <li>If you&#8217;re careful, you can build a cross-compiler that will generate code for a different target system: just port the kernel and the compiler <em>should</em> be re-usable to generate code that&#8217;ll run on it. (It&#8217;s not that simple, of course, as I&#8217;m finding&nbsp;myself&#8230;)</li>
</ul>
So Forth programs don&#8217;t look like other languages. There&#8217;s no real phase distinction between compilation and run-time, since everything&#8217;s mixed-in together, but that has the advantage that you can write new &#8220;compiler&#8221; words to make it easier to write your &#8220;application&#8221; words, all within the same framework and set of capabilities. You don&#8217;t  write applications so much as extend the language itself towards your problem. That in turn means you can view Forth either as low-level &#8212; a glorified assembler &#8212; or very high-level in terms of its ability to define new syntax and semantics.

That probably sounds like a nightmare, but suspend judgment for a while&#8230;..

One of the problems that concerns a lot of sensor networks people is the programming level at which we have to deal with systems. Typically we&#8217;re forced to write C on a per-node basis: program the individual nodes, and try to set it up so that the network behaves, as a whole, in an intended way. This is clearly possible in many cases, and clearly gets way more difficult as things get bigger and more complex, and especially when we want the network to adapt to the phenomena it&#8217;s sensing, which often requires decisions to be made on a non-local basis.

Writing a new language is a big undertaking, but a substantially smaller undertaking with Forth. It&#8217;s possible to conceive of new language structures (for example a construct that generates <a href="http://www.simondobson.org/2010/02/216/">moving averages</a>) and implement it quickly and simply. This might just be syntactic sugar, or might be something rather more ambitious in terms of control flow. Essentially you can extend the syntax <em>and</em> the semantics of Forth so that it &#8220;understands&#8221;, at the same level as the rest of the compiler, the new construct you want to use.

Which is interesting enough, but what makes it more interesting for sensors is the structure of these compiler words. Typically they&#8217;re what is known as <tt><span class="caps">IMMEDIATE</span></tt> words, which means they <em>execute</em> when encountered <em>compiling</em> a word. That may sound odd, but what it means is that the compiler word executes and generates code, rather than being compiled itself. And <em>that</em> means that, when used with a cross-compiler, the new language constructs don&#8217;t add to the target system&#8217;s footprint, because their action all happens at compile-time to generate code that&#8217;s expressed in terms of lower-level words. In core Forth, constructs like <tt><span class="caps">IF</span></tt> and <tt><span class="caps">LOOP</span></tt> (conditional and counted loops respectively) do exactly this: they compile low-level jumps (the word <tt>(<span class="caps">BRANCH</span>)</tt> and <tt>(?<span class="caps">BRANCH</span>)</tt>, which do non-conditional and conditional branches respectively) implementing the higher-level structured-programming abstraction.

A lot of modern languages use virtual machines as targets for their compilers, and a lot of those VMs are stack machines &#8212; Forth, in other words. If we actually <em>use</em> Forth as the <span class="caps">VM</span> for a compiler, we have an <em>extensible</em> <span class="caps">VM</span> in which we can define new constructs, so we can evolve the <span class="caps">VM</span> better to fit the language that targets it. (There are also some very interesting, close parallels between Forth code and the abstract syntax trees used to represent code within compilers, but that&#8217;s something I need to think about a bit more before I write about it.)

All this is rather speculative, and doesn&#8217;t really address the core problem of programming a network rather than a device, but it does provide a device-level platform that might be more amenable to language research. I&#8217;ve been experimenting with Forth for this purpose, and have a prototype system &#8212; Attila, an abstract, re-targetable threaded interpreter that&#8217;s fully cross-compilable &#8212; in the works, but not quite ready to see the light of day. It&#8217;s taught me a lot about the practicalities of threaded interpreters and cross-compilers. This is a strand of language design that&#8217;s been almost forgotten, and I think it deserves more of a look.
		</div>
	    </article>
</div>
	<div class="site-content">
	    <article class="format-standard libretto-long-form"><div class="title-block post-format-icon-pin">
		    <div class="entry-meta">
			<span class="posted-on">
			    Posted on <a href="../2010/03/05/things-that-wont-change/" rel="bookmark">Friday 5 March, 2010</a>
			</span>
		    </div>
		    <h1><a href="../2010/03/05/things-that-wont-change/">Things that won&#8217;t&nbsp;change</a></h1>
		</div>
		<div class="entry-content">
			<p>Technology always advances, and in most areas the rate of change is also increasing all the time. But there are some areas where technological changes either happen only slowly, or even go into reverse. Not something we&#8217;re used to in computer science, but it&#8217;s a feature of sensor network programming: what are the challenges that technology <em>won&#8217;t</em> solve for us?

<!--more-->Moore&#8217;s law has been a defining feature of computers for the past decades. By and large computing power has doubled every 18 months at constant price; alternatively, the cost of a unit of computing power has halved in the same period. The effects of this on user experience have been plain to see.

Within computer science, Moore&#8217;s law has had an effect on research directions too. In starting on a PhD a student can work on a problem that&#8217;d at the edge of the performance envelope of whatever class of machine she is targeting &#8212; cellphone, laptop, desktop or server &#8212; secure in the knowledge that, but the time she&#8217;s coming to an end, the power available to that machine class will have quadrupled. This doesn&#8217;t open-up <em>every</em> problem, of course &#8212; a four-times speed-up on an <span class="caps">NP</span>-hard search problem might still leave it largely intractable &#8212; but in fields such as middleware, software tools, language design and the like, it&#8217;s enough to overcome many issues.

It&#8217;s therefore something of a shock to come to sensor networks and similar systems, because I suspect these systems aren&#8217;t subject to Moore&#8217;s law in the usual way.

In some ways, the situation on such small systems is actually <em>better</em> than in desktops and enterprise computing. At the higher end, we&#8217;re already hitting at least the suspicion that the performance increases in individual cores will soon start to flatten out. Multicore processors allow us to keep increasing performance, but at the cost of vastly complicating the sorts of programming needed in order to keep all those cores occupied. Sensor motes are still single-core and typically aren&#8217;t using state-of-the-art processes at that, so there&#8217;s still plenty of room for manoeuvre.

But it&#8217;s easy to forget that while the cash cost of a unit of processing power has decreased, the <em>power</em> cost of that unit hasn&#8217;t decreased by nearly as much (and may actually have increased). Those twice-as-powerful chips eighteen months on typically burn significantly more power than their predecessors. You only have to look at the size of heatsinks on chips to realise that there&#8217;s a <em>lot</em> of heat being dissipated.

So for a sensor network, which is using a battery or scavenging for power,  increasing the processor power will almost certainly decrease lifetime, and that&#8217;s not a trade-off designers will accept. Battery, scavenging and renewable power sources like solar cells aren&#8217;t subject to Moore&#8217;s law: their growth curves are those of physics and traditional engineering, not those of <span class="caps">IT</span> systems. Ten years ago my cellphone went for three days without a charge; my new <span class="caps">HTC</span> Hero lasts no more than two days, even if I turn off the data services and wifi. The extra compute cost has a severe power cost.

In many sensor applications, the trade-off will actually be in reverse. Given the choice, a designer might opt for two older, less capable but less power-hungry processors over one more powerful but more hungry. Two motes can provide more coverage, or more robustness, or both.

But this exposes a real programming challenge, since it implies that we&#8217;re going to have to get used to building modern, open, adaptive software on machines whose capabilities are similar to those of a mid-1980&#8217;s vintage home computer &#8212; and which might in fact even <em>decrease</em> over time, since the driving forces are pushing for coverage, lifetime and redundant replication. The performance of a network in aggregate might still increase, of course, but that still means that we have to extract extra performance from co-ordinating distributed processors rather than from improving individual nodes. The history of distributed parallel processing should warn us not to be sanguine about <em>that</em> prospect.

Actually, though, the challenge will do us good. Modern machines encourage sloppy over-engineering and over-generalisation &#8212; building frameworks for situations that we anticipate but which might never occur. Targeting small machines will change this, and instead encourage us to build software that&#8217;s fit for immediate purpose, <em>and</em> that&#8217;s build to be evolved and extended over time alongside changing requirements and constraints. This building evolution into the core of the system will make for better engineering in the long&nbsp;run.</p>
		</div>
	    </article>
</div>
	<div class="site-content">
	    <article class="format-standard libretto-long-form"><div class="title-block post-format-icon-pin">
		    <div class="entry-meta">
			<span class="posted-on">
			    Posted on <a href="../2010/02/26/216/" rel="bookmark">Friday 26 February, 2010</a>
			</span>
		    </div>
		    <h1><a href="../2010/02/26/216/">Programming with limited&nbsp;certainty</a></h1>
		</div>
		<div class="entry-content">
			<p>Sensor networks are all about uncertainty: if the sensor says it&#8217;s 20°C  out there, then it might be 20°C plus-or-minus half a degree or so (limited precision); or it might be some different temperature, and the sensor&#8217;s just reported a duff value for some reason (limited accuracy). By contrast, computers most definitely <em>aren&#8217;t</em> about uncertainty, a fact enshrined in the famous maxim &#8220;garbage in, garbage out&#8221;. What does this mean for our ability to build really large, robust and flexible sensor networks?

<!--more-->All the really foundational models of computing &#8212; λ calculus, Turing machines and the like &#8212; pretty much reflect this notion that input is correct in some sense, and if it&#8217;s wrong then that&#8217;s an error to be corrected outside the computational system. That seems to mean that the computational system can&#8217;t itself either tolerate or express the notions of limited certainty &#8212; precision and accuracy &#8212; that lie at the core of sensor networks (and a lot of other modern systems, or course). That suggests to me that there might be a problem at the heart of computer science as we currently formulate it: it isn&#8217;t a realistic model of the world we&#8217;re trying to compute over.

In some ways this is nether surprising nor threatening. Any mathematical or computational model is only a simplified abstraction of the real world, for which we have to make often staggeringly bold simplifications if we&#8217;re to get anywhere. We should however always be prepared to challenge the <em>validity</em> and <em>necessity</em> of these simplifications, and that&#8217;s what I&#8217;d like to do here.

As far as validity is concerned, the simplification is quite patently <em>invalid</em> when it comes to any system that operates with real-world data: some of it is <em>bound</em> to be &#8220;wrong&#8221; in some sense. This isn&#8217;t the same as being tolerant of mistakes, such as when someone presses the delete key by mistake: that&#8217;s a  action that certainly happened and to which the system responded correctly, albeit &#8220;wrongly&#8221; from the user&#8217;s perspective. Interesting problem, but different: we&#8217;re talking here about responding to inherently erroneous input &#8212; the delete key seeming to press itself, if you like.

Necessity, then: is it necessary to handle computation in this way? Clearly not: we can easily conjecture a computational model that&#8217;s more tolerant of input with limited certainty.

Consider precision first. If the input is only known to a limited precision, then we don&#8217;t want that error margin to cause enormous errors. If we have a function $latex f$, then we want $latex f$ to exhibit a tolerance of imprecision such that $latex \delta x &lt; tol_x \Rightarrow \left | f(x + \delta x) - f(x) \right | &lt; s \left | \delta x \right|$ for some scaling factor $latex s &lt; 1$. $latex f$ doesn&#8217;t cause errors to blow-up in unpredictable ways. A lot of functions behave in exactly this way: for example, in a sliding-window average function $latex f_w(\overline{x}, x) = \frac{x + \overline{x}(w - 1)}{w}$ for an average $latex \overline{x}$ computed from $latex w$ recent observations, we have that $latex s = \frac{1}{w}$. Small errors therefore perturb the result significantly less than the input is perturbed. If the errors are uniformly distributed, the function should converge on the &#8220;true&#8221; value.

Conversely, a large, accurate new observation will perturb the average only slowly, so large step-changes will be detected only slowly. It&#8217;s hard to distinguish such a change when it first happens from an inaccurate reading. There are various ways of dealing with this, such as using a weighted sliding window with non-linear weighting.

This is a rather topological idea. Rather than simply mapping points in an input space (such as temperature) to an output space (average temperature over the past few minutes), we&#8217;re also requiring that the mapping take elements close in the input space to elements close in the result space: we require that it be a <em>contraction mapping</em>. Building systems from contraction mappings, perhaps combined with contraction-preserving operators, yields systems that are robust to small errors in precision from their sensors.

Of course not all systems <em>are</em> actually like this, and in many cases we <em>want</em> rapid changes to be detected quickly and/or propagated. The point, perhaps, is that this is a <em>choice we should make</em> rather than a <em>consequence</em> of choosing a particular model of computation. There might actually be a model of computation lurking about here, in which we define functions coupled with a model of how their input and output errors should behave. At the very least, this yields systems in which we can predict the consequences of errors and imprecisions, which is a major challenge to deal with at&nbsp;present.</p>
		</div>
	    </article>
</div>
    <!-- Lower Navigation links -->
    <nav class="site-content navigation-post" role="navigation"><div class="previous">
		<a href="../" rel="prev" onclick="return false;">
		    <span class="meta-nav">No More Older Entries</span>		</a>
	    </div>
	    <div class="next">
		<a href="index-2.html" rel="next">
		    <span class="meta-nav">Newer Entries</span>		</a>
	    </div>
    </nav><!-- Page Footer --><section class="footer-sidebar clear" role="complementary"><div class="widget-block">
	    <aside class="widget"><h2 class="widget-title">Simon&nbsp;Dobson</h2>
		<div class="widget-text">Aut tace aut loquere meliora silentio</div>
	    </aside>
</div>
    </section><!-- Extra JavaScript --><!-- Site Attributions --><footer class="site-footer" role="contentinfo"><div class="site-info">
	    <p></p>
	    <p>
	      Built with free and open-source software.
	      Powered by <a href="https://getnikola.com/">Nikola</a> using a theme based on
	      <a href="https://themes.getnikola.com/v7/libretto/">Libretto</a>.
	    </p>
	    <p>
	      All content Copyright © 2010–2024 Simon Dobson and licensed under
	      <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><span class="caps">CC</span>-<span class="caps">BY</span>-<span class="caps">NC</span>-<span class="caps">SA</span>-4.0</a>
	      unless otherwise&nbsp;noted.
	    </p>
	</div>
	<div class="social">
	    <ul class="menu"></ul>
</div>
    </footer>
</body>
</html>
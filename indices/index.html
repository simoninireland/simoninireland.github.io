<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Simon Dobson</title>
<style>
	@font-face {
	    font-family: "libretto-icons";
	    src:url(../assets/fonts/libretto-icons.eot);
	    src:url(../assets/fonts/libretto-icons.eot#iefix) format("embedded-opentype"),
	    url(../assets/fonts/libretto-icons.woff) format("woff"),
	    url(../assets/fonts/libretto-icons.ttf) format("truetype"),
	    url(../assets/fonts/libretto-icons.svg#libretto-icons) format("svg");
	    font-weight: normal;
	    font-style: normal;
	}
    </style>
<link rel="icon" href="../images/favicon.png" sizes="16x16">
<link rel="alternate" type="application/rss+xml" href="../rss.xml">
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Droid+Sans+Mono|Libre+Baskerville|Montserrat|Playfair+Display|Tangerine">
<link rel="stylesheet" href="../assets/css/libretto_styles.css">
<link rel="stylesheet" href="../assets/css/baguetteBox.min.css">
<link rel="stylesheet" href="../assets/css/code.css">
<link rel="stylesheet" href="../assets/css/nikola_rst.css">
<link rel="stylesheet" href="../assets/css/nikola_ipython.css">
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.0.13/css/all.css" integrity="sha384-DNOHZ68U8hZfKXOrtjWvjxusGo9WQnrNx2sqG0tfsghAvtVlRW3tvkXWZh58N9jp" crossorigin="anonymous">
</head>
<body>
    <!-- Navigation bar -->
    <header class="nav-bar"><div class="site-branding">
	    <h1 class="site-title">
		<a href="https://simondobson.org/" title="Simon Dobson" rel="home">Simon&nbsp;Dobson</a>
	    </h1>
	</div>
	<nav class="site-navigation" role="navigation"><div class="menu-toggle">
		<span class="mobile-site-title">Simon Dobson</span>
	    </div>
	    <ul class="menu">
<li><a href="../index.html">Home</a></li>
		    <li><a href="../personal/">About&nbsp;me</a></li>
		    <li><a href="../research/">Research</a></li>
		    <li><a href="../development/projects/">Software</a></li>
		    <li><a href="../writing/">Writing</a></li>
		    <li><a href="../personal/contact/">Contact</a></li>
		<li>
<a href="../rss.xml"><i class="fa fa-rss"></i></a>
	    </li>
</ul></nav></header><div class="site-content">
	    <article class="format-standard libretto-long-form"><div class="title-block post-format-icon-pin">
		    <div class="entry-meta">
			<span class="posted-on">
			    Posted on <a href="../2024/06/10/processing-the-data-offline/" rel="bookmark">Monday 10 June, 2024</a>
			</span>
		    </div>
		    <h1><a href="../2024/06/10/processing-the-data-offline/">Processing MicroMoth recordings&nbsp;offline</a></h1>
		</div>
		<div class="entry-content">
			<div id="outline-container-orgfccf4b6" class="outline-2">
<h2 id="orgfccf4b6">Processing MicroMoth recordings&nbsp;offline</h2>
<div class="outline-text-2" id="text-orgfccf4b6">
<p>
The uMoth&nbsp;generates <code>.wav</code> files, uncompressed waveforms of what it
records. These need to be processed to identify any bird calls
within&nbsp;them.
</p>

<p>
This function is integrated in BirdNET-Pi, which does recording and
classification, and provides a web <span class="caps">GUI</span>. With the uMoths we need to
provide classification as part of a data processing pipeline. We
can however make direct use of the classifier &#8220;brain&#8221; within
BirdNET-<span class="caps">PI</span>, which is unsurprisingly called <a href="https://github.com/kahst/BirdNET-Analyzer">BirdNET-Analyzer</a>.
</p>
</div>

<div id="outline-container-org1ca5243" class="outline-3">
<h3 id="org1ca5243">Installation</h3>
<div class="outline-text-3" id="text-org1ca5243">
<p>
I&#8217;m working on a 16-core Intel Core i7@3.8GHz running Arch&nbsp;Linux.
</p>

<p>
First we clone the BirdNET-Analyzer repo. This takes a long time
as it includes the <span class="caps">ML</span> models, some of which are <span class="caps">40MB</span> or&nbsp;more.
</p>

<div class="highlight"><pre><span></span>    git clone https://github.com/kahst/BirdNET-Analyzer.git
    cd BirdNET-Analyzer
</pre></div>

<p>
The repo includes a Docker file that we can use to build the
analyser in a&nbsp;container.
</p>

<div class="highlight"><pre><span></span>    docker build .
</pre></div>

<p>
The container setup is quite basic and is probably intended for
testing rather than production, but it gives a usable system that
could then be embedded into something more usable. The core of the
system is&nbsp;the <code>analyze.py</code> script.
</p>
</div>
</div>

<div id="outline-container-orge6466b1" class="outline-3">
<h3 id="orge6466b1">Analysing some data (<span class="caps">AKA</span> identifying some&nbsp;birds!)</h3>
<div class="outline-text-3" id="text-orge6466b1">
<p>
The container as defined looks into&nbsp;its <code>/example</code> directory for
waveforms and analyses them, generating text file for each sample.
The easiest way to get it to analyse captured data is to mount a
data directory of files onto this mount point (thereby shadowing
the example waveform&nbsp;provided).
</p>

<p>
There are various parameters that configure the classifier. I
copied the <a href="/2024/05/19/first-installation/">defaults I was using with BirdNET-Pi</a>, only accepting classifications
at or above 0.7&nbsp;confidence.
</p>

<div class="highlight"><pre><span></span>    docker run -v /var/run/media/sd80/DATA:/example birdnet-analyzer analyze.py --rtype=csv --min_conf=0.7 --sensitivity=1.25
</pre></div>

<p>
This crunches through all the files (982 of them from my first
run) and generates a <span class="caps">CSV</span> file for each. An example&nbsp;is:
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<colgroup>
<col class="org-right">
<col class="org-right">
<col class="org-left">
<col class="org-left">
<col class="org-right">
</colgroup>
<tbody>
<tr>
<td class="org-right">Start (s)</td>
<td class="org-right">End (s)</td>
<td class="org-left">Scientific name</td>
<td class="org-left">Common name</td>
<td class="org-right">Confidence</td>
</tr>
<tr>
<td class="org-right">6.0</td>
<td class="org-right">9.0</td>
<td class="org-left">Corvus monedula</td>
<td class="org-left">Eurasian Jackdaw</td>
<td class="org-right">0.9360</td>
</tr>
<tr>
<td class="org-right">9.0</td>
<td class="org-right">12.0</td>
<td class="org-left">Corvus monedula</td>
<td class="org-left">Eurasian Jackdaw</td>
<td class="org-right">0.8472</td>
</tr>
<tr>
<td class="org-right">12.0</td>
<td class="org-right">15.0</td>
<td class="org-left">Corvus monedula</td>
<td class="org-left">Eurasian Jackdaw</td>
<td class="org-right">0.8681</td>
</tr>
<tr>
<td class="org-right">15.0</td>
<td class="org-right">18.0</td>
<td class="org-left">Corvus monedula</td>
<td class="org-left">Eurasian Jackdaw</td>
<td class="org-right">0.8677</td>
</tr>
<tr>
<td class="org-right">24.0</td>
<td class="org-right">27.0</td>
<td class="org-left">Columba palumbus</td>
<td class="org-left">Common Wood-Pigeon</td>
<td class="org-right">0.9198</td>
</tr>
<tr>
<td class="org-right">27.0</td>
<td class="org-right">30.0</td>
<td class="org-left">Columba palumbus</td>
<td class="org-left">Common Wood-Pigeon</td>
<td class="org-right">0.7716</td>
</tr>
<tr>
<td class="org-right">45.0</td>
<td class="org-right">48.0</td>
<td class="org-left">Corvus monedula</td>
<td class="org-left">Eurasian Jackdaw</td>
<td class="org-right">0.8023</td>
</tr>
<tr>
<td class="org-right">48.0</td>
<td class="org-right">51.0</td>
<td class="org-left">Corvus monedula</td>
<td class="org-left">Eurasian Jackdaw</td>
<td class="org-right">0.7696</td>
</tr>
</tbody>
</table>
<p>
Those are entirely credible identifications. The start- and
end-point offsets allow rough location within the recording.
(BirdNET segments the recordings into 3s chunks for&nbsp;analysis.)
</p>

<p>
This is clearly not as straightforward as BirdNET-Pi, nor as
immediately satisfying. But it does scale to analysing lots of
data (and could be made to do so even better, with a better
front-end to the container), which is important for any
large-scale&nbsp;deployment.
</p>
</div>
</div>
</div>
		</div>
	    </article>
</div>
	<div class="site-content">
	    <article class="format-standard libretto-long-form"><div class="title-block post-format-icon-pin">
		    <div class="entry-meta">
			<span class="posted-on">
			    Posted on <a href="../2024/06/10/configuring-the-board/" rel="bookmark">Monday 10 June, 2024</a>
			</span>
		    </div>
		    <h1><a href="../2024/06/10/configuring-the-board/">Deploying a&nbsp;MicroMoth</a></h1>
		</div>
		<div class="entry-content">
			<div id="outline-container-orgf2790be" class="outline-2">
<h2 id="orgf2790be">Deploying a&nbsp;MicroMoth</h2>
<div class="outline-text-2" id="text-orgf2790be">
<p>
The MicroMoth (or uMoth) from <a href="personal/notebook/open_acoustic_devices-contact.html#ID-c0a38f8e-970a-499d-a30e-56ec235b6ea9">Open Acoustic Devices</a> is the same as
their better-known AudioMoth recorder but with a significantly
smaller footprint. It&#8217;s just a traditional recorder or data-logger,
with now on-board analysis and no wireless connectivity. I got hold
of some to use in a larger project we&#8217;re thinking about running, and
they&#8217;re not kidding about the &#8220;micro&#8221;&nbsp;part.
</p>

<p>
<img src="../attachments/4b/0ad2d7-5cb3-4b93-a0ec-4963e2868155/IMG_20240528_144150.jpg" alt="nil"></p>

<p>
The uMoth uses the same software as the AudioMoth, and therefore
the same configuration app available from the <a href="https://www.openacousticdevices.info/applications">apps page</a> â€“ for
64-bit Linux in my case. It downloads as&nbsp;a <code>.appimage</code> file, which
is simply a self-contained archive. It needed to be marked as
executable, and then ran directly from a double-click. (The page
suggests that there may be some extra steps for some Linux
distros: there weren&#8217;t for&nbsp;Arch.)
</p>

<p>
I then followed the <a href="https://www.openacousticdevices.info/config-app-guide">configuration guide</a>. The time is set
automatically from the computer&#8217;s clock when you configure the&nbsp;device.
</p>

<p>
For testing I chose two recording periods, 0400â€“0800 and&nbsp;1400â€“1600.
</p>

<p>
<img src="../attachments/4b/0ad2d7-5cb3-4b93-a0ec-4963e2868155/config.png" alt="nil"></p>

<p>
As shown this will, with the default 48KHz sampling, generate
about <span class="caps">2GB</span> of data per day and use about 70mAh of energy. For my
tests I just hung the device out of the window on a <span class="caps">USB</span> tether for
power: it works fine drawing power from the <span class="caps">USB</span> rather than from
the battery&nbsp;connector.
</p>

<p>
<img src="../attachments/4b/0ad2d7-5cb3-4b93-a0ec-4963e2868155/IMG_20240608_141527.jpg" alt="nil"></p>

<p>
This turned out not to record anything, because the time is lost
if the power is disconnected, even though the configuration is
retained. (The manual does actually say this, with a suitably
close reading. It could be clearer.) There&#8217;s a smartphone app that
can reset the time once the device is in the field and powered-up,
though, by making an audio chime that encodes the current time and
location in a way the board can understand. Flashing the device
with the &#8220;Always require acoustic chime on switching to <span class="caps">CUSTOM</span>&#8221;
makes it wait after power is applied until its time is&nbsp;set.
</p>

<p>
The red <span class="caps">LED</span> flashes when the device is recording. The green <span class="caps">LED</span>
flashes when the device is waiting for a recording period to
start. The red <span class="caps">LED</span> stays lit while the time is&nbsp;unset.
</p>
</div>
</div>
		</div>
	    </article>
</div>
	<div class="site-content">
	    <article class="format-standard libretto-long-form"><div class="title-block post-format-icon-pin">
		    <div class="entry-meta">
			<span class="posted-on">
			    Posted on <a href="../2024/05/27/pascal-costanza1s-highly-opinionated-guide-to-lisp/" rel="bookmark">Monday 27 May, 2024</a>
			</span>
		    </div>
		    <h1><a href="../2024/05/27/pascal-costanza1s-highly-opinionated-guide-to-lisp/">Pascal Costanza&#8217;s highly opinionated guide to&nbsp;Lisp</a></h1>
		</div>
		<div class="entry-content">
			<div id="outline-container-org9a991ba" class="outline-2">
<h2 id="org9a991ba">Pascal Costanza&#8217;s highly opinionated guide to&nbsp;Lisp</h2>
<div class="outline-text-2" id="text-org9a991ba">
<p>
<a href="https://www.p-cos.net/lisp/guide.html">Pascal Costanza&#8217;s Highly Opinionated Guide to&nbsp;Lisp</a>
</p>

<p>
Part introduction, part paean to the language&#8217;s power, part study
guide, while dipping into an eclectically-chosen subset of Lisp
features that really illustrate what makes it&nbsp;different.
</p>
</div>
</div>
		</div>
	    </article>
</div>
	<div class="site-content">
	    <article class="format-standard libretto-long-form"><div class="title-block post-format-icon-pin">
		    <div class="entry-meta">
			<span class="posted-on">
			    Posted on <a href="../2024/05/27/a-road-to-common-lisp/" rel="bookmark">Monday 27 May, 2024</a>
			</span>
		    </div>
		    <h1><a href="../2024/05/27/a-road-to-common-lisp/">A road to Common&nbsp;Lisp</a></h1>
		</div>
		<div class="entry-content">
			<div id="outline-container-orga7343e8" class="outline-2">
<h2 id="orga7343e8">A road to Common&nbsp;Lisp</h2>
<div class="outline-text-2" id="text-orga7343e8">
<p>
<a href="https://stevelosh.com/blog/2018/08/a-road-to-common-lisp/">A Road to Common&nbsp;Lisp</a>
</p>

<p>
This a really brief, yet really interesting, approach to
introducing Lisp to someone. Interesting because it covers all the
usual ground, but also has copious pointers to other material
slightly-beyond-introductory (&#8220;Where to go from here&#8221;). It also
links to material that&#8217;s essential to modern practice, such as
Lisp packages and systems, and the essential &#8220;standard libraries&#8221;
such as Alexandria, Bordeaux, <span class="caps">CL</span>-<span class="caps">PPCRE</span>, usocket, and the like: the
things that are needed in practice and which in other languages
would probably be built-in and included directly in an&nbsp;introduction.
</p>
</div>
</div>
		</div>
	    </article>
</div>
	<div class="site-content">
	    <article class="format-standard libretto-long-form"><div class="title-block post-format-icon-pin">
		    <div class="entry-meta">
			<span class="posted-on">
			    Posted on <a href="../2024/05/19/first-installation/" rel="bookmark">Sunday 19 May, 2024</a>
			</span>
		    </div>
		    <h1><a href="../2024/05/19/first-installation/">First installation of&nbsp;BirdNET-Pi</a></h1>
		</div>
		<div class="entry-content">
			<div id="outline-container-orgbcc6fc2" class="outline-2">
<h2 id="orgbcc6fc2">First installation of&nbsp;BirdNET-Pi</h2>
<div class="outline-text-2" id="text-orgbcc6fc2">
<p>
The BirdNET-Pi system aims to provide out-of-the-box bird
identification. It&#8217;s slightly more awkward than that, but still
pretty straightforward to get up and&nbsp;running.
</p>

<p>
My first hardware plan was to use a Raspberry Pi Zero as the
compute host with a <a href="https://www.waveshare.com/wm8960-audio-hat.htm">Waveshare <span class="caps">WM8960</span> <span class="caps">HAT</span></a> for the sound capture. It
turns out that BirdNET needs a 64-bit platform â€“ why I&#8217;m not sure
â€“ and the Pi Zero only runs 32-bit Linux. I therefore moved to a
Raspberry Pi B that I had lying around, and put a 64-bit &#8220;lite&#8221;
install on it to run&nbsp;headless.
</p>

<p>
I then basically just followed the <a href="https://github.com/mcguirepr89/BirdNET-Pi/wiki/Installation-Guide">installation guide</a>. There was an
issue with the installation script when cloning the GitHub repo: I
suspect this was because of limited memory on the Pi. I downloaded
manually, and manually ran the rest of the install script, which did
a <i>lot</i> of setup of services and a <span class="caps">PHP</span> web&nbsp;server.
</p>

<p>
I compiled the <a href="https://www.waveshare.com/wiki/WM8960_Audio_HAT">drivers</a> for the <span class="caps">HAT</span>, which worked fine. The new
sound card is recognised but is not the system&nbsp;default.
</p>

<p>
The installed components seem to&nbsp;include:
</p>

<ul class="org-ul">
<li>
<code>icecast2</code>, a streaming server, used to replay&nbsp;recordings</li>
<li>
<code>caddy</code> web&nbsp;server</li>
<li><span class="caps">PHP</span> for serving the web&nbsp;pages</li>
<li>
<code>arecord</code> to actually record&nbsp;audio</li>
<li>
<code>ffmpeg</code> to extract&nbsp;waveforms</li>
<li><code>sqlite for the database</code></li>
<li>the actual machine learning model used for&nbsp;recognition</li>
</ul>
<p>
The recognition models are built with TensorFlow. This is a great
example of how the standard Linux tools and services can be combined
to get a scientific-grade sensor platform. (Caddy doesn&#8217;t seem to be
running over <span class="caps">TLS</span> by default, which would be an issue outside a&nbsp;firewall.)
</p>

<p>
Since the sound card isn&#8217;t the default, the easiest way to get the
system listening to the right mics is to change the device in the
&#8220;advanced settings&#8221; panel: in my case I changed from &#8220;default&#8221; to
&#8220;hw:2,0&#8221;, reflecting the output&nbsp;of <code>arecord -l</code> that shows the sound
card&nbsp;devices.
</p>

<p>
I then deployed the Pi out of the kitchen&nbsp;window.
</p>

<p>
<img src="../attachments/18/408cf7-5f1f-4b07-81ef-e8f94d1c38d3/screenshot.png" alt="nil"></p>

<p>
To start with it wasn&#8217;t hearing anything, which I think may be
because of the waterfall in the courtyard: turning this off made
things much more&nbsp;effective:
</p>

<p>
<img src="../attachments/18/408cf7-5f1f-4b07-81ef-e8f94d1c38d3/screenshot-1.png" alt="nil"></p>

<p>
That&#8217;s an appropriate set of birds being seen â€“ and we hardly ever
<i>see</i> magpies, but know they&#8217;re around. There&#8217;s actually quite a lot
of background noise even in such a quiet village, but the bird
calls do stand&nbsp;out.
</p>

<p>
I can&#8217;t see any reason for the manual installation on bare metal:
as far as I can see everything could be containerised, which would
make deployment and management a <i>lot</i>&nbsp;easier.
</p>
</div>
</div>
		</div>
	    </article>
</div>
    <!-- Lower Navigation links -->
    <nav class="site-content navigation-post" role="navigation"><div class="previous">
		<a href="index-144.html" rel="prev">
		    <span class="meta-nav">Older Entries</span>		</a>
	    </div>
    </nav><!-- Page Footer --><section class="footer-sidebar clear" role="complementary"><div class="widget-block">
	    <aside class="widget"><h2 class="widget-title">Simon&nbsp;Dobson</h2>
		<div class="widget-text">Aut tace aut loquere meliora silentio</div>
	    </aside>
</div>
    </section><!-- Extra JavaScript --><!-- Site Attributions --><footer class="site-footer" role="contentinfo"><div class="site-info">
	    <p></p>
	    <p>
	      Built with free and open-source software.
	      Powered by <a href="https://getnikola.com/">Nikola</a> using a theme based on
	      <a href="https://themes.getnikola.com/v7/libretto/">Libretto</a>.
	    </p>
	    <p>
	      All content Copyright Â© 2010â€“2024 Simon Dobson and licensed under
	      <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><span class="caps">CC</span>-<span class="caps">BY</span>-<span class="caps">NC</span>-<span class="caps">SA</span>-4.0</a>
	      unless otherwise&nbsp;noted.
	    </p>
	</div>
	<div class="social">
	    <ul class="menu"></ul>
</div>
    </footer>
</body>
</html>
<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Simon Dobson (Posts about network science)</title><link>https://simondobson.org/</link><description></description><atom:link href="https://simondobson.org/categories/network-science.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2022 &lt;a href="mailto:simoninireland@gmail.com"&gt;Simon Dobson&lt;/a&gt; </copyright><lastBuildDate>Wed, 02 Nov 2022 16:43:14 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Complex networks, complex processes</title><link>https://simondobson.org/2015/02/13/book/</link><dc:creator>Simon Dobson</dc:creator><description>&lt;p&gt;&lt;/p&gt;&lt;p&gt;I'm writing a book on my sabbatical. Or trying to, anyway. So I thought I'd publicise the fact so people can hassle me to keep at it.&lt;/p&gt;
&lt;!--more--&gt;

&lt;p&gt;I've been working on complex systems for a couple of years, especially on complex networks: things like the way people move through a road and rail network, or how diseases spread through social networks. It's a bit of a change from my previous work on sensor data interpretation, although not as much as you might think: I'm wondering whether we could combine sensing and simulation, to use sensors to confirm predictions or to drive and condition further simulations.&lt;/p&gt;
&lt;p&gt;Getting into this area has been -- and is -- a head-wreck. It's both highly mathematical and highly computational. I understand the computing; the maths, not so much. Many computer scientists would have the same reaction, but conversely, so would many mathematicians: the maths would be familiar, the computing a challenge. So effectively in order to make progress you have to climb two learning curves simultaneously: some unusual and challenging mathematics about stochastic processes, simulated using cluster or cloud computing which poses a lot of challenges even for someone used to programming.&lt;/p&gt;
&lt;p&gt;This is made harder by the research literature, though, which tends towards sparse mathematical descriptions, which is frustrating at two levels: the computing is probably interesting (to people like me), and it's hard to re-create the results when the computational approach underlying the graphs and results is unclear.&lt;/p&gt;
&lt;p&gt;So with this in mind, and because I've never done it before, I've decided to write a textbook: &lt;em&gt;Complex networks, complex processes&lt;/em&gt;. (No, I'm not very imaginative when it comes to titles...) The idea is to link the maths to the code, providing everything a research would need to get started with the maths and the computing. Since this is likely to be a book with, shall we say, &lt;em&gt;limited circulation&lt;/em&gt;, I've decided not to bother with a publisher and instead make it completely open. You can look at the current state on the web &lt;a title="Book home page" href="http://www.simondobson.org/research/complex-networks-complex-processes/" target="_blank"&gt;here&lt;/a&gt;, download the sources, copy and run the code, or anything needed to get started.&lt;/p&gt;
&lt;p&gt;It's a work in progress and it's not very usual to advertise books before they're in a fit state to be read, but I suppose that's just a part of open science: make the process visible, warts and all. It also means I'll hopefully get comments and encouragement to keep at it when it starts to fall by the wayside of other things I have to do. The goal is to get the majority done while I'm on research leave (until September), and comments on style, content, and progress will be most welcome.&lt;/p&gt;</description><category>Blog</category><category>book</category><category>complex systems</category><category>network science</category><guid>https://simondobson.org/2015/02/13/book/</guid><pubDate>Fri, 13 Feb 2015 11:54:41 GMT</pubDate></item><item><title>The edge of computer science</title><link>https://simondobson.org/2013/07/10/edge/</link><dc:creator>Simon Dobson</dc:creator><description>&lt;p&gt;&lt;/p&gt;&lt;p style="text-align: left"&gt;Where does mathematics end and computer science begin?&lt;/p&gt;
&lt;!--more--&gt;

&lt;p&gt;I don't seem to have posted to this blog recently, so let's re-start with a big question: where is the edge of computer science? That is to say, what separates it from maths? How do mathematicians and computer scientists see problems differently, and indeed select differently what constitutes an interesting problem?&lt;/p&gt;
&lt;p&gt;This has been on my mind recently because of some work I've been doing with my student &lt;a href="http://saray.host.cs.st-andrews.ac.uk/" target="_blank"&gt;Saray&lt;/a&gt; on adaptive complex networks. A complex network is one that has statistical regularity in the distribution of the wires, or links, between its nodes. The internet is a complex network where the links obey a power-law distribution: a small number of sites (Yahoo, Google, IBM, ...) have a huge number of links to them, while the majority (this site) have almost none. Complex networks are created naturally by lots of processes, and are useful for describing a whole range of phenomena. (An accessible introduction is Albert-László Barabási. &lt;a href="http://www.amazon.co.uk/Linked-Everything-Connected-Business-Everyday/dp/0452284392" target="_blank"&gt;Linked: how everything is connected to everything else, and what it means for business and daily life&lt;/a&gt;. 2003.) An &lt;em&gt;adaptive&lt;/em&gt; complex network is one where the way the network is wired changes with time. A good example is a meeting-with-friends network where there is a link between you and those people you meet in a particular timeframe. You might change the people you meet if you discover that one of them is ill, for example, so the the friend-meeting network would be re-wired to remove links to your sick friends. If we were to model the spread of an illness through this network, there would be two processes at work: a spreading process that made people who met sick people ill themselves (with some probability); and a re-wiring process that tried to remove links to those friends known to be sick (again perhaps with some probability). Our paper (Saray Shai and Simon Dobson. &lt;a href="http://dx.doi.org/10.1103/PhysRevE.87.042812" target="_blank"&gt;Complex adaptive coupled networks&lt;/a&gt;. Physical Review E &lt;strong&gt;87&lt;/strong&gt;(4). 2013.) shows how there are unsuspected subtleties in the way spreading processes work on such networks, where common simplifications can actually hide crucial features of the network's behaviour.&lt;/p&gt;
&lt;p&gt;The literature on network science is full of papers analysing such processes. Typically the analysis is both analytic and numerical. That is to say, a mathematical model is developed that describes the state of the network after lots of time has passed (its &lt;em&gt;equilibrium behaviour&lt;/em&gt;); and numerical simulation is then performed by creating a large number of networks, running the spreading processes on them, and seeing whether the results obtained match the analytical model. (It was an unexpected mis-match between analytical and numerical results that led us to the main result reported in our paper.) Typically the community finds analytical results more interesting than numerical results, and with good reason: an analytic result provides both a final, closed-form solution that can be used to describe &lt;em&gt;any&lt;/em&gt; network with particular statistical properties, without simulation; and it often also provides insight into &lt;em&gt;why&lt;/em&gt; a given equilibrium behaviour occurs. These are the sorts of general statements that can lead to profound understanding of wide ranges of phenomena.&lt;/p&gt;
&lt;p&gt;There's a sting in the tail of analysis, however, which is this. In order to be able to form an analytic model, the process being run over the network has to be simple enough that the mathematics converges properly. A typical analysis might use a probabilistic re-wiring function, for example, where nodes are re-wired with a fixed probability, or one that varies only slowly. Anything more complex than this defeats analysis, and as a result one never encounters anything other than simple spreading processes in the literature.&lt;/p&gt;
&lt;p&gt;As a computer scientist rather than a mathematician I find that unsatisfying, and I think my dissatisfaction may actually define the boundary between computing and mathematics. The boundary is the halting problem -- or, more precisely, sustaining your interest in a problem once you've hit it.&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://en.wikipedia.org/wiki/Halting_problem" target="_blank"&gt;halting problem&lt;/a&gt; is one of the definitive results in computer science, and essentially says that there are some problems for which it's impossible to predict ahead of time whether they'll complete with a solution or simply go on forever. Put another way, there are some problems where the only way to determine what the solution is is to run a piece of code that computes it, and that may or may not deliver a solution. Put yet another way, there are problems for which the code that computes the solution is the most concise description available.&lt;/p&gt;
&lt;p&gt;What this has to do with complex systems is the following. When a computer scientist sees a problem, they typically try to abstract it as far as possible. So on encountering a complex network, our first instinct is to build the network and then build the processes running on it &lt;em&gt;as separate descriptions that can be defined independently&lt;/em&gt;. That is, we don't limit what kind of functions can hang off each node to define the spreading process: we just allow &lt;em&gt;any&lt;/em&gt; function -- any piece of code -- and then run the dynamics of the network with that code defining what happens at each node at each timestep. The immediate consequence of this approach is that we can't say anything &lt;em&gt;a priori&lt;/em&gt; about the macroscopic properties of the spreading process, because to do so would run into the fact that there isn't anything general one can say about an arbitrary piece of code. The generality we typically seek precludes our making global statements about behaviour.&lt;/p&gt;
&lt;p&gt;Mathematicians don't see networks this way, because they want to make precisely the global statements that the general approach precludes -- and so don't allow arbitrary functions into the mix. Instead they use functions that aggregate cleanly, like fixed-probability infection rates, about which one &lt;em&gt;can&lt;/em&gt; make global statements. One way to look at this is that well-behaved functions allow one to make global statements about their aggregate behaviour without having to perform any computation &lt;em&gt;per se&lt;/em&gt;: they remain within an envelope whose global properties are known. A mathematician who used an ill-behaved function would be unable to perform analysis, &lt;em&gt;and that's precisely what they're interested in doing&lt;/em&gt;, even though by doing so they exclude a range of possible network behaviours.In fact, it's worse than that: the range of behaviours excluded is infinite, and contains a lot of networks that seem potentially very interesting, for example those whose behaviours depend on some transmitted value, or one computed from values spread by the process.&lt;/p&gt;
&lt;p&gt;So a mathematician's (at least as represented in most of the complex systems literature) interest in a problem is lost at precisely the point that a computer scientist's interest kicks in: where the question is about behaviour of arbitrary computations. The question this leads to is, what model do real-world networks follow more closely? Are they composed of simple, well-behaved spreading processes? Or do they more resemble arbitrary functions hanging off a network of relationships, whose properties can only be discovered numerically? And what effect does the avoidance of arbitrary computation have on the choice of problems to which scientists apply themselves? Perhaps the way forward here is to try to find the boundary of the complexity of functions that remain analytic when used as part of a network dynamics, to get the best of both worlds: global statements about large categories of networks, without needing numerical simulation of individual cases.&lt;/p&gt;
&lt;p&gt;Such a classification would have useful consequences for general computer science as well. A lot of the problems in systems design come from the arbitrariness of code and its interactions, and from the fact that we're uncomfortable restricting that generality &lt;em&gt;a priori&lt;/em&gt; because we don't know what the consequences will be for the re-usability and extensibility of the systems being designed. A more nuanced understanding of behavioural envelopes might help.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;</description><category>Blog</category><category>computer science</category><category>mathematics</category><category>network science</category><guid>https://simondobson.org/2013/07/10/edge/</guid><pubDate>Wed, 10 Jul 2013 10:55:45 GMT</pubDate></item></channel></rss>
<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Simon Dobson (Posts about big data)</title><link>https://simondobson.org/</link><description></description><atom:link href="https://simondobson.org/categories/big-data.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2024 &lt;a href="mailto:simoninireland@gmail.com"&gt;Simon Dobson&lt;/a&gt; </copyright><lastBuildDate>Mon, 01 Jan 2024 19:20:16 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>PhD data science studentships available at Trinity College Dublin</title><link>https://simondobson.org/2015/09/18/phd-data-science/</link><dc:creator>Simon Dobson</dc:creator><description>&lt;p&gt;There are five PhD posts on offer to work with the ADAPT Centre.

&lt;!--more--&gt;

Benefits: Payment of tax-free stipend for 4 years.

In addition, payment of academic fees; fully for EU students and partially for non-EU students.

General enquires concerning this post can be addressed to &lt;a class="moz-txt-link-abbreviated" href="mailto:aoife.brady@adaptcentre.ie"&gt;aoife.brady@adaptcentre.ie&lt;/a&gt;.

Initial deadline is October 7th 2015, but with each position remaining open until filled. This and other positions listed under vacancies at: &lt;a class="moz-txt-link-freetext" href="http://adaptcentre.ie"&gt;http://adaptcentre.ie&lt;/a&gt;
&lt;/p&gt;&lt;h4&gt;Research Topic&lt;/h4&gt;
Increasingly in data-driven enterprises, organisations are having to cope with a wide variety of information sources and standards, leading to a lack of interoperability and increasing data integration costs. Resulting labour-intensive data integration practices are brittle in the face of accelerating innovation in data-driven applications and growing demand for agile data analytics. At the same time organisations must react to increasing public and legislative focus on privacy and data protection. Empowering users to control the information that flows around them in a privacy-sensitive and personalised manner also offers many challenges.

These 5 PhD positions will advance the state of the art in linked data, semantic web and personalisation technologies to explore new approaches to data integration that are self-managing (towards autonomic), and in addition explore new ways to deliver personalised multimodal information (towards a digital companion).

These posts are part of the new ADAPT Centre (&lt;a class="moz-txt-link-freetext" href="http://www.adaptcentre.ie"&gt;http://www.adaptcentre.ie&lt;/a&gt;). ADAPT has received €50 Million research funding from Industry and Science Foundation Ireland to support 120 researchers across 4 universities in Dublin. The ADAPT Centre’s mission is to produce world class research that delivers disruptive innovations for the digital media and intelligent content industry. These PhD positions will be supervised by a member of the following ADAPT Centre academics; Professor Vincent Wade, Professor Declan O’Sullivan, Professor Dave Lewis, Professor Owen Conlan and Dr Rob Brennan.

ADAPT is Ireland’s global centre of excellence for digital content and media innovation. Led by TCD, it combines the expertise of researchers at four universities (Trinity College Dublin, Dublin City University, University College Dublin, and Dublin Institute of Technology) with that of its industry partners to produce ground-breaking digital content innovation.

ADAPT brings together more than 120 researchers who collectively have won more than €100m in funding and have a strong track record of transferring world-leading research and innovations to more than 140 companies. With EURO 50M in new research funding from Science Foundation Ireland and industry, ADAPT is seeking talented individuals to join its growing research team. Our research and technologies will continue to help businesses in all sectors and drive back the frontiers of future Web engagement.
&lt;h4&gt;Why join ADAPT @ TCD&lt;/h4&gt;
&lt;ul&gt;
    &lt;li&gt;Work on hard, relevant problems in an interdisciplinary and exciting research environment. The ADAPT Centre combines expertise of researchers at Trinity College Dublin, Dublin City University, University College Dublin and Dublin Institute of Technology. It brings together more than 120 researchers who have collectively won more than €100M in competitive research funding and have an international track record of bridging research and innovations to more than 140 companies. With €50M in new research funding from SFI and industry, ADAPT research and technologies will help businesses in all sectors to manage, personalise and deliver digital content more effectively.&lt;/li&gt;
    &lt;li&gt;Work in a University where excellence of research is valued. Trinity College Dublin is Ireland’s premier University and is ranked in 71st position in the top 100 world universities by the QS World University Rankings 2014.&lt;/li&gt;
    &lt;li&gt;Work in a centre focussed on advancing your career. Whether you want to take an academic, industrial, or entrepreneurial career path, ADAPT prides itself in the support and mentoring that enables all its Students and early-stage researchers to reach their full potential. This year alone its postdoc-to-PI programme has helped three postdocs transition to be Principal Investigators on their own H2020 projects, while four others have recently won funding with ADAPT support to realise the commercialisation of their research through spin outs and licensing.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Requirements:&lt;/h4&gt;
The successful candidate will have an excellent academic record (first class or II.1 primary degree) in Computer Science or a related discipline. Experience in Knowledge Engineering is a distinct advantage. The successful candidate will be highly motivated, with strong written and oral communication skills and a demonstrated proficiency in software development, with strong design and programming skills. They must be eager to work in and learn from multi-disciplinary and multi-organisation teams. They should have English language certification if English is not their first language, the requirement being: IELTS: 7.0+, TOEFL iBT: 100+, TOEFL pBT: 600+, CEF: C1+, or equivalent.
&lt;h4&gt;Application Procedure&lt;/h4&gt;
For further information and informal contact, please refer to the PhD topic details. Please apply via email to &lt;a class="moz-txt-link-abbreviated" href="mailto:vacancies@adaptcentre.ie"&gt;vacancies@adaptcentre.ie&lt;/a&gt;, referencing this advert, and including:
&lt;ul&gt;
    &lt;li&gt;A targeted cover letter (600-1000 words) expressing your suitability for a position&lt;/li&gt;
    &lt;li&gt;A complete CV&lt;/li&gt;
&lt;/ul&gt;
There will be an interview process; a successful candidate will then be invited to apply via the TCD graduate studies admission system.</description><category>big data</category><category>data science</category><category>studentships</category><guid>https://simondobson.org/2015/09/18/phd-data-science/</guid><pubDate>Fri, 18 Sep 2015 10:54:17 GMT</pubDate></item><item><title>Let's teach everyone about big data</title><link>https://simondobson.org/2014/04/05/big-data/</link><dc:creator>Simon Dobson</dc:creator><description>&lt;p&gt;Demolishing the straw men of big data.

&lt;!--more--&gt;

This post comes about from reading &lt;a href="http://www.ft.com/cms/s/2/21a6e7d8-b479-11e3-a09a-00144feabdc0.html" target="_blank"&gt;Tim Harford's opinion piece in the &lt;em&gt;Financial Times&lt;/em&gt;&lt;/a&gt; in which he offers a critique of "big data", the idea that we can perform all the science we want to simply by collecting large datasets and then letting machine learning and other algorithms loose on it. Harford deploys a whole range of criticisms against this claim, all of which are perfectly valid: sampling bias will render a lot of datasets worthless; correlations will appear without causation; the search goes on without hypotheses to guide it, and so isn't well-founded in falsifiable predictions; and an investigator without a solid background in the science underlying the data is going to have no way to correct these errors.

The critique is, in other words, damning. The only problem is, that's not what most scientists with an interest in data-intensive research are claiming to do.

Let's consider the biggest data-driven project to date, the Large Hadron Collider's search for the Higgs boson. This project involved building a huge experiment that then generated huge data volumes that were trawled for the signature of Higgs interactions. The challenge was so great that the consortium had to develop new computer architectures, data storage, and triage techniques just to keep up with the avalanche of data.

None of this was, however, an "hypothesis-free" search through the data for correlation. On the contrary, the theory underlying the search for the Higgs made quite definite predictions as to what its signature should look like. Nonetheless, there would have been no way of confirming or refuting the correctness of those predictions without collecting the data volumes necessary to make the signal stand out from the noise.

&lt;em&gt;That's&lt;/em&gt; data-intensive research: using new data-driven techniques to confirm or refute hypotheses about the world. It gives us another suite of techniques to deploy, changing both the way we do science and the science that we do. It doesn't replace the other ways of doing science, any more than the introduction of any other technology necessarily invalidates hat came before. Microscopes did not remove the need for, or value of, searching for or classifying new species: they just provided a new, complementary approach to both.

That's not to say that all the big data propositions are equally appropriate, and I'm certainly with Harford in the view that approaches like &lt;a href="http://www.google.org/flutrends/" target="_blank"&gt;Google Flu&lt;/a&gt; are deeply and fundamentally flawed, over-hyped attempts to grab the limelight. Where he and I diverge is that Harford is worried that &lt;em&gt;all&lt;/em&gt; data-driven research falls into this category, and that's clearly not true. He may be right that a lot of big data research is a corporate plot to re-direct science, but he's wrong to worry that all projects working with big data are similarly driven.

I've argued before that &lt;a href="https://simondobson.org/2014/03/data-scientists/" target="_blank"&gt;"data scientist" is a nonsense term&lt;/a&gt;, and I still think so. Data-driven research is just research, and needs the same skills of understanding and critical thinking. The fact that some companies and others with agendas are hijacking the term worries me a little, but in reality is no more significant than the New Age movement's hijacking of terms like "energy" and "quantum" -- and one doesn't stop doing physics because of that.

In fact, I think Harford's critique is a valuable and significant contribution to the debate precisely because it highlights the need for understanding &lt;em&gt;beyond&lt;/em&gt; the data: it's essentially a call for scientists to only use data-driven techniques in the service of science, not as a replacement for it. An argument, in other words, for a broadly-based education in data-driven techniques for &lt;em&gt;all&lt;/em&gt; scientists, and indeed all researchers, since the techniques are equally (if not more) applicable to social sciences and humanities. The new techniques open-up new areas, and we have to understand their strengths and limitations, and use them to bring our subjects forwards -- not simply step away because we're afraid of their potential for misuse.

UPDATE 7Apr2014: &lt;a href="http://www.nytimes.com/2014/04/07/opinion/eight-no-nine-problems-with-big-data.html?_r=2" target="_blank"&gt;An opinion piece in the New York Times&lt;/a&gt; agrees: "big data can work well as an adjunct to scientific inquiry but rarely succeeds as a wholesale replacement." The number of statistical land mines is enormous, but the right approach is to be aware of them and make the general research community aware too, so we can use the data properly and to best effect.&lt;/p&gt;</description><category>big data</category><category>data-intensive</category><guid>https://simondobson.org/2014/04/05/big-data/</guid><pubDate>Sat, 05 Apr 2014 12:09:48 GMT</pubDate></item><item><title>It's the tooling, not the terabytes</title><link>https://simondobson.org/2014/03/17/tooling-terabytes/</link><dc:creator>Simon Dobson</dc:creator><description>&lt;p&gt;With all the hype about big data it's sometimes hard to realise that it's about more than just data. In fact, it's real interest doesn't come from big-ness at all.

&lt;!--more--&gt;
The term &lt;em&gt;big data&lt;/em&gt; is deceptively hard to parse. It's a relative term, for a start: when you get down to it, all it really means is data that's large relative to the available storage and processing capacity. From that perspective, big data has always been with us -- and always will be. It's also curiously technology-specific for something that's garnering such broad interest. A volume of data may be "big" for one platform (a laptop, for example) and not for others (a computing cluster with a large network-attached store).

Getting away from the data, what researchers typically mean by big data is a set of computational techniques that can be applied broadly to data somewhat independent of its origins and subject. &lt;a href="https://simondobson.org/2014/03/data-scientists/" target="_blank"&gt;I'm not a fan of "data science"&lt;/a&gt;, but the term does at least focus on techniques and not on data size alone. This is much more interesting, and poses a set of challenges to disciplines -- and to computer science -- to identify and expand the set of techniques and tools researchers can make use of.

What often frightens people about big data (or makes it an attractive career niche, depending on your point of view) is that there is a step-change in how you interact with data beyond a certain size -- and it's this change in tooling requirements that I think is a more significant indicator of a big-data problem than simply the data size.

Suppose you collect a dataset consisting of a few hundred samples, each with maybe ten variables -- something that could come from a small survey, for example. This size of data can easily be turned into a spreadsheet model, and a large class of researchers have become completely comfortable with building such models. (This didn't used to be the case: I had a summer job in the late 1980's building spreadsheet models for a group that didn't have the necessary experience. Research skills evolve.) Even a survey with thousands of rows would be possible.

But what if the survey has several million rows? -- for example because it came from an online survey, or because it was sensed automatically in some way. No spreadsheet program can ingest that much data.

A few million rows does not constitute what many people would regard as "big data": it'll be at most several megabytes. But that's not the point: rather, the point is that, in order to deal with it, the researchers have to change tools -- and change them quite radically. Rather than using a spreadsheet, they have to become programmers; not just that, they have to become programmers who are familiar with languages like &lt;a href="https://www.python.org/" target="_blank"&gt;Python&lt;/a&gt; (to get the libraries), and &lt;a href="https://hadoop.apache.org/" target="_blank"&gt;Hadoop&lt;/a&gt; and cloud computing (to be able to scale-out the solutions). They could employ programmers, of course, but that removes them from the immediate and intimate contact with their data that many researchers find extremely valuable, and that personal computers have given us. To many people, hiring a programmer and running calculations in the cloud is suspiciously like a return to the mainframe computing we thought was rendered obsolete decades ago.

&lt;em&gt;It's not the terabytes, it's the tooling.&lt;/em&gt; Big data starts when you cross the threshold from your familiar world of interactive tools and into a more specialised, programmers world.

This transition is becoming common in humanities and social sciences, as well as in science and medicine, and to my mind it's the major challenge of big data. The basic problem is simple: changing tools takes time, expertise, and mental effort, that is taken away from what's a researcher's actual research interest. A further disincentive is that the effort may not be rewarded: after all, if this really is research, one is often not sure whether there actually is anything valuable on the other side of a data analysis. In fields where competition is really competitive, like medicine, this feels like a lot of risk for an uncertain reward. There's evidence to suggest -- and I can't prove this contention -- that people are steering clear of doing the experiments they know they should do because they know they'll generate data volumes they're uncomfortable with.

This, then, is where the big data challenge actually is: minimising the cost of transition from tools that are familiar to tools that are effective on the larger data volumes that are now becoming commonplace. This is a programming and user interface challenge, to make the complex infrastructure appear easy and straightforward to people who want to accomplish tasks on it. A large challenge, but not an unprecedented one: I'm writing this just after the 25th birthday of the World Wide Web that took a complex infrastructure (the internet) and made it usable by everyone. Let's just not get too hung-up on the idea that data needs to be really big to be interesting in this new data-driven world.&lt;/p&gt;</description><category>big data</category><category>strategy</category><guid>https://simondobson.org/2014/03/17/tooling-terabytes/</guid><pubDate>Mon, 17 Mar 2014 08:30:06 GMT</pubDate></item><item><title>No data scientists</title><link>https://simondobson.org/2014/03/12/data-scientists/</link><dc:creator>Simon Dobson</dc:creator><description>&lt;p&gt;"Data science" and "data scientist" are not good terms -- for anything.

&lt;!--more--&gt;The recent revolution in our ability to monitor more and more human (and indeed non-human) activity has spawned a whole new field of study. Known variously as "big data", "data science", and "digital humanities", the idea is that -- by studying the data collected -- we can perform feats of prediction and customisation that defeat other approaches.

This is not all hype. There's no doubt that deriving algorithms from data -- known as "data-driven design" -- can often work better than &lt;em&gt;a priori&lt;/em&gt; design. The best-known example of this is &lt;a href="http://translate.google.com/" target="_blank"&gt;Google Translate&lt;/a&gt;, whose translation approach is driven almost entirely by applying statistical learning to a large corpus of documents with for which exact translations exist across languages, and using the correlations found as the basis for translation. (The documents in question were actually the core agreements governing the operation of the EU, the &lt;em&gt;acquis communautaire&lt;/em&gt;, which explains why Google Translate works best on bureaucratese and worst on poetry.) It turns out that data-driven learning works better in most cases than grammar-directed parsing.

The data-driven approach rests on several pillars. Chief among them is applied machine learning as mentioned above, allowing algorithms to look through bodies of data and learn the correlations that exist between events. (We use similar techniques to analyse sensor data and perform situation recognition. See Ye, Dobson and McKeever. &lt;a href="http://dx.doi.org/10.1016/j.pmcj.2011.01.004" target="_blank"&gt;Situation identification techniques in pervasive computing: a review&lt;/a&gt;. Pervasive and Mobile Computing &lt;strong&gt;8&lt;/strong&gt;(1), pp. 36--66. 2012.) Various other statistical techniques can also be applied, ranging up from simple mean and variance calculations. One can usually augment the basic techniques using more structural methods: if you know the structure of the links in a web site, for example, you can learn more about how people navigate because you know that some routes are more probable (by clicking hyperlinks) than others. The results of these analyses then need to be presented as analytics for consumption by managers and decision-makers, and distilling large volumes of information into visually compelling and comprehensible form is a skill in itself.

Going a stage further, one may conduct experiments directly. If you see people searching for the same term, present the results to different people in different ways and see how that influences the links they click. Google again are at the forefront.

The excitement of these techniques has spilled over into the wider science and humanities landscapes. Just within St Andrews this week I've talked about projects for analysing DNA data using machine learning, improving clinical trials, building a concept map for two centuries-worth of literature pertaining to Edinburgh, detecting intruders in computer systems, and mapping the births, deaths, and marriages of Scotland from parish records -- and it's only Wednesday. All these activities are inherently data-driven: they only become possible because we can ingest and analyse large data volumes at high-enough speeds.

However, none of this implies that there is a subject called "data science", and I'm starting to worry that a false premise is being established.

The term "data science" is a tricky one to parse. How many sciences do you know that aren't based in data? "Data-driven" is the same. That's not to say that they're meaningless, exactly: they identify a sub-set of techniques that are qualitatively different to the more theory-driven approaches, and even differentiate themselves from experimentally-driven approaches by their attempt to correlate across datasets rather than being based on a single homogeneous sampling (however large). But that's a nuanced reading, and a general reader would be forgiven for believing that "data science" was somehow a separate discipline from "ordinary" science, instead of it denoting a set of computational techniques with general applicability across the range of (non-)traditional sciences.

But it's "data scientist" that really troubles me. This seems to suggest that one can find scientific meaning in the data and the data alone. It seems to suggest that there's a short-cut to scientific insight through the data (and by implication, computer science) rather than through traditional scientific training. And this simply isn't true.

The problem is the age-old different between correlation and causality. Correlation is when things happen together: you leave a cup of a tea and a biscuit on a table for long enough, the tea goes cold and the biscuit gets stale. Causality is when one thing happening triggers another thing happening: the tea got cold, and that made the biscuit become stale. Mistaking one for the other leads to all sorts of interesting possibilities: if we put the tea in an insulated cup, the biscuit will stay fresh longer.

Now, the final tea-and-biscuit example is clearly meaningless, but ask yourself this: how did you &lt;em&gt;recognise&lt;/em&gt; it as meaningless? Because you understand that the two effects are happening independently because of the passage of time, not because of each other: they are correlated but not causative. You understand this because you have insight into the processes of the world.

And it's here that the problems start for data scientists. In order to interpret the machine learning, statistics, analytics, or other results, you have to have an understanding of the underlying processes. &lt;em&gt;It doesn't happen in the data at all.&lt;/em&gt; That's fine for tea and biscuits, and is also probably fairly fine for sales of consumer goods on mass-market web sites, where we have a good intuitive understanding of the processes involved, but will drop-off rapidly as we approach areas that are more complex, more noisy, less intuitive, and less well-understood. How can you differentiate correlation from causality if you don't understand what's possible in the underlying domain? How, in fact, can you determine anything from the data in and of itself?

This suggests to me that data scientist isn't a job description, or even an aspiration: it's a misnomer that should really be read as "a trained scientist working with lots of rich empirical data". Not as sexy, but probably more useful and less likely to disappoint everyone involved.&lt;/p&gt;</description><category>big data</category><guid>https://simondobson.org/2014/03/12/data-scientists/</guid><pubDate>Wed, 12 Mar 2014 11:00:29 GMT</pubDate></item><item><title>Big, or just rich?</title><link>https://simondobson.org/2013/07/31/big-rich/</link><dc:creator>Simon Dobson</dc:creator><description>&lt;p&gt;The current focus on "big data" may be obscuring something more interesting: it's often not the pure size of a dataset that's important.

&lt;!--more--&gt;

The idea of extracting insight from large bodies of data promises significant advances in science and commerce. Given a large dataset, "big data" techniques cover a number of possible approaches:
&lt;/p&gt;&lt;ul&gt;
    &lt;li&gt;Look through the data for recurring patterns (data mining)&lt;/li&gt;
    &lt;li&gt;Present a summary of the data to highlight features (analytics)&lt;/li&gt;
    &lt;li&gt;(Less commonly) Identify automatically from the dataset what's happening in the real world (situation recognition)&lt;/li&gt;
&lt;/ul&gt;
There's a wealth of &lt;a href="http://data.gov.uk/" target="_blank"&gt;UK government data available&lt;/a&gt;, for example. Making it machine-readable means it can be presented in different ways, for example &lt;a href="http://www.telegraph.co.uk/news/uknews/crime/8294450/Crime-maps-are-just-the-beginning.html" target="_blank"&gt;geographically&lt;/a&gt;. The real opportunities seem to come from cross-overs between datasets, though, where they can be mined and manipulated to find relationships that might otherwise remain hidden, for example the effects of crime on house prices.

Although the size and availability of datasets clearly makes a difference here -- big open data -- we might be confusing two issues. In some circumstances we might be better looking for smaller but richer datasets, and for richer connections between them.

&lt;em&gt;Big data&lt;/em&gt; is a strange name to start with: when is data "big"? The only meaningful definition I can think of is "a dataset that's large relative to the current computing and storage capacity being deployed against it" -- which of course means that big data has always been with us, and indeed always will be. It also suggests that data might become less "big" if we become sufficiently interested in it to deploy more computing power to processing it. The alternative term popular in some places, &lt;em&gt;data science&lt;/em&gt;, is equally tautologous, as I can't readily name a science that &lt;em&gt;isn't&lt;/em&gt; based on data. (This isn't just academic pedantry, by the way: terms matter, if only to distinguish what topics are, and aren't, covered by big data/data science research.)

It's worth reviewing what big data lets us do. Having more data is useful when looking for patterns, since it makes the pattern stand out from the background noise. Those patterns in turn can reveal important processes at work in the world underlying the data, processes whose reach, significance, or even existence may be unsuspected. There may be patterns in the patterns, suggesting correlation or causality in the underling processes, and these can then be used for prediction: if pattern A almost always precedes pattern B in the dataset, then when I see a pattern A in the future I may infer that there's an instance of B coming. The statistical machine learning techniques that let one do this kind of analysis are powerful, but dumb: it still requires human identification and interpretation of the underlying processes to to conclude that A &lt;em&gt;causes&lt;/em&gt; B, as opposed to A and B simply occurring together through some acausal correlation, or being related by some third, undetected process. A data-driven analysis won't reliably help you to distinguish between these options without further, non-data-driven insight.

Are there are cases in which less data is better? Our experience with situation recognition certainly suggests that this is the case. When you're trying to relate data to the the real world, it's essential to have &lt;em&gt;ground truth&lt;/em&gt;, a record of what &lt;em&gt;actually&lt;/em&gt; happened. You can then make a prediction about what the data indicates about the real world, and verify that this prediction is true or not against known circumstances. Doing this well over a dataset provides some confidence that the technique will work well against other data, where your prediction is all you have. In this case, what matters is not simply the size of the dataset, but its relationship to another dataset recording the actual state of the world: it's the &lt;em&gt;richness&lt;/em&gt; that matters, not strictly the size (although having more data to train against is always welcome).

Moreover, rich connections may help with the more problematic part of data science, the identification of the processes underlying the dataset. While there may be no way to distinguish causality from correlation within a single dataset -- because they look indistinguishably alike -- the patterns of data points in the one dataset may often be related to patterns and data points in another dataset in which they &lt;em&gt;don't&lt;/em&gt; look alike. So the richness provides a translation from one system to another, where the second provides discrimination not available in the first.

I've been struggling to think of an example of this idea, and this is the best I've come up with (and it's not all that good). Suppose we have tracking data for people around an area, and we see that person A repeatedly seems to follow person B around. Is A following B? Stalking them? Or do they live together, or work together (or even just close together)? We can distinguish between these alternatives by having a link from people to their jobs, homes, relationships and the like.

There's a converse concern, which is that poor discrimination can lead to the wrong conclusions being drawn: classifying person B as a potential stalker when he's actually an innocent who happens to follow a similar schedule. An automated analysis of a single dataset risks finding spurious connections, and it's increasingly the case that these false-positives (or -negatives, for that matter) could have real-world consequences.

Focusing on connections between data has its own dangers, of course, since we already know that we can make very precise classifications of people's actions from relatively small, but richly connected, datasets. Maybe the point here is that focusing exclusively on the size of a dataset masks both the advantages to be had from richer connections with other datasets, and the benefits and risks associated with smaller but better-connected datasets. Looking deeply can be as effective (or more so) as looking broadly.</description><category>big data</category><category>machine learning</category><category>uncertainty</category><guid>https://simondobson.org/2013/07/31/big-rich/</guid><pubDate>Wed, 31 Jul 2013 13:58:49 GMT</pubDate></item></channel></rss>
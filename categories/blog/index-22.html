<!DOCTYPE html>
<html prefix="
	og: http://ogp.me/ns# article: http://ogp.me/ns/article#
    " vocab="http://ogp.me/ns" lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Posts about Blog (old posts, page 22) | Simon Dobson</title>
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.0.13/css/all.css" integrity="sha384-DNOHZ68U8hZfKXOrtjWvjxusGo9WQnrNx2sqG0tfsghAvtVlRW3tvkXWZh58N9jp" crossorigin="anonymous">
<link rel="stylesheet" href="assets/css/normalize.css">
<link rel="stylesheet" href="assets/css/main.css">
<link href="../../assets/css/baguetteBox.min.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/rst_base.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/nikola_rst.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/code.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/fonts.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/theme.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" hreflang="en" href="../../rss.xml">
<link rel="canonical" href="https://simondobson.org/categories/blog/index-22.html">
<link rel="icon" href="../../images/favicon.png" sizes="32x32">
<link rel="prev" href="index-23.html" type="text/html">
<link rel="next" href="index-21.html" type="text/html">
<!--[if lt IE 9]><script src="../../assets/js/html5shiv-printshiv.min.js"></script><![endif]--><!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=UA-10943215-1"></script><script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-10943215-1');
</script><link rel="alternate" type="application/rss+xml" title="RSS for tag Blog" hreflang="en" href="../blog.xml">
</head>
<body>
  <a href="#page-content" class="sr-only sr-only-focusable">
    Skip to main content
  </a>

  

  <section id="header"><a href="../../">
      <div class="logo">
	  <div id="nologoimage">
	    Simon Dobson
	  </div>
      </div>
    </a>
  </section><section id="socialNav"><!-- Navigation Menu - on top on small screens, down the left on larger --><div class="navlinks">
<!--
      <a href="/">
	<div id="titleauth">
	    by Simon Dobson
	</div>
      </a>
-->

      <!-- Navigation links (hidden by default) -->
      <div id="navmenuitems">
	      <a href="../../index.html" title="Home">
	<span class="menuitemtext">Home</span>
	<span class="menuitemicon"><i class="fa fa-home"></i></span>
      </a>
      <a href="../../personal/" title="About me">
	<span class="menuitemtext">About me</span>
	<span class="menuitemicon"><i class="fa fa-user"></i></span>
      </a>
      <a href="../../research/" title="Research">
	<span class="menuitemtext">Research</span>
	<span class="menuitemicon"><i class="fa fa-lightbulb"></i></span>
      </a>
      <a href="../../development/projects/" title="Software">
	<span class="menuitemtext">Software</span>
	<span class="menuitemicon"><i class="fa fa-cogs"></i></span>
      </a>
      <a href="../../writing/" title="Writing">
	<span class="menuitemtext">Writing</span>
	<span class="menuitemicon"><i class="fa fa-feather"></i></span>
      </a>
      <a href="../../personal/contact/" title="Contact">
	<span class="menuitemtext">Contact</span>
	<span class="menuitemicon"><i class="fa fa-info-circle"></i></span>
      </a>
      <a href="../../rss.xml" title="RSS">
	<span class="menuitemtext">RSS</span>
	<span class="menuitemicon"><i class="fa fa-rss"></i></span>
      </a>
  <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">
     <img alt="Creative Commons License CC-BY-NC-SA-4.0" style="border-width:0" src="../../images/cc-by-nc-sa-4.0.png"></a>
  
  

      </div>

      <!-- "Hamburger menu" / "Bar icon" to toggle the navigation links -->
      <a href="javascript:void(0);" id="hamburger" class="icon" onclick="toggleNav()">
	<i class="fa fa-bars">
	</i>
      </a>
    </div>
  </section><section class="page-content"><div class="content" rel="main">
    <header><h1>Posts about Blog (old posts, page 22)</h1>
        <div class="metadata">
                            <p class="feedlink">
                                    <a href="../blog.xml" hreflang="en" type="application/rss+xml">RSS feed</a>

                </p>

            
        </div>
    </header><div class="postindex">
    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="../../2014/04/05/big-data/" class="u-url">Let's teach everyone about big data</a></h1>

        
        <div class="metadata">
            <p class="dateline"><a href="../../2014/04/05/big-data/" rel="bookmark"><i class="fa fa-clock"></i> <time class="published dt-published" datetime="2014-04-05T13:09:48+01:00" title="2014-04-05 13:09">2014-04-05 13:09</time></a></p>

                <p class="commentline"><i class="fa fa-comment"></i>         <a href="../../2014/04/05/big-data/#disqus_thread" data-disqus-identifier="cache/posts/2014/04/05/big-data.html">Comments</a>


        </p>
</div>
    </header><div class="p-summary entry-summary">
    <p></p>
<p>Demolishing the straw men of big data.</p>
<!--more-->

<p>This post comes about from reading <a href="http://www.ft.com/cms/s/2/21a6e7d8-b479-11e3-a09a-00144feabdc0.html" target="_blank">Tim Harford's opinion piece in the <em>Financial Times</em></a> in which he offers a critique of "big data", the idea that we can perform all the science we want to simply by collecting large datasets and then letting machine learning and other algorithms loose on it. Harford deploys a whole range of criticisms against this claim, all of which are perfectly valid: sampling bias will render a lot of datasets worthless; correlations will appear without causation; the search goes on without hypotheses to guide it, and so isn't well-founded in falsifiable predictions; and an investigator without a solid background in the science underlying the data is going to have no way to correct these errors.</p>
<p>The critique is, in other words, damning. The only problem is, that's not what most scientists with an interest in data-intensive research are claiming to do.</p>
<p>Let's consider the biggest data-driven project to date, the Large Hadron Collider's search for the Higgs boson. This project involved building a huge experiment that then generated huge data volumes that were trawled for the signature of Higgs interactions. The challenge was so great that the consortium had to develop new computer architectures, data storage, and triage techniques just to keep up with the avalanche of data.</p>
<p>None of this was, however, an "hypothesis-free" search through the data for correlation. On the contrary, the theory underlying the search for the Higgs made quite definite predictions as to what its signature should look like. Nonetheless, there would have been no way of confirming or refuting the correctness of those predictions without collecting the data volumes necessary to make the signal stand out from the noise.</p>
<p><em>That's</em> data-intensive research: using new data-driven techniques to confirm or refute hypotheses about the world. It gives us another suite of techniques to deploy, changing both the way we do science and the science that we do. It doesn't replace the other ways of doing science, any more than the introduction of any other technology necessarily invalidates hat came before. Microscopes did not remove the need for, or value of, searching for or classifying new species: they just provided a new, complementary approach to both.</p>
<p>That's not to say that all the big data propositions are equally appropriate, and I'm certainly with Harford in the view that approaches like <a href="http://www.google.org/flutrends/" target="_blank">Google Flu</a> are deeply and fundamentally flawed, over-hyped attempts to grab the limelight. Where he and I diverge is that Harford is worried that <em>all</em> data-driven research falls into this category, and that's clearly not true. He may be right that a lot of big data research is a corporate plot to re-direct science, but he's wrong to worry that all projects working with big data are similarly driven.</p>
<p>I've argued before that <a href="../../2014/03/data-scientists/" target="_blank">"data scientist" is a nonsense term</a>, and I still think so. Data-driven research is just research, and needs the same skills of understanding and critical thinking. The fact that some companies and others with agendas are hijacking the term worries me a little, but in reality is no more significant than the New Age movement's hijacking of terms like "energy" and "quantum" -- and one doesn't stop doing physics because of that.</p>
<p>In fact, I think Harford's critique is a valuable and significant contribution to the debate precisely because it highlights the need for understanding <em>beyond</em> the data: it's essentially a call for scientists to only use data-driven techniques in the service of science, not as a replacement for it. An argument, in other words, for a broadly-based education in data-driven techniques for <em>all</em> scientists, and indeed all researchers, since the techniques are equally (if not more) applicable to social sciences and humanities. The new techniques open-up new areas, and we have to understand their strengths and limitations, and use them to bring our subjects forwards -- not simply step away because we're afraid of their potential for misuse.</p>
<p>UPDATE 7Apr2014: <a href="http://www.nytimes.com/2014/04/07/opinion/eight-no-nine-problems-with-big-data.html?_r=2" target="_blank">An opinion piece in the New York Times</a> agrees: "big data can work well as an adjunct to scientific inquiry but rarely succeeds as a wholesale replacement." The number of statistical land mines is enormous, but the right approach is to be aware of them and make the general research community aware too, so we can use the data properly and to best effect.</p>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="../../2014/03/17/tooling-terabytes/" class="u-url">It's the tooling, not the terabytes</a></h1>

        
        <div class="metadata">
            <p class="dateline"><a href="../../2014/03/17/tooling-terabytes/" rel="bookmark"><i class="fa fa-clock"></i> <time class="published dt-published" datetime="2014-03-17T08:30:06Z" title="2014-03-17 08:30">2014-03-17 08:30</time></a></p>

                <p class="commentline"><i class="fa fa-comment"></i>         <a href="../../2014/03/17/tooling-terabytes/#disqus_thread" data-disqus-identifier="cache/posts/2014/03/17/tooling-terabytes.html">Comments</a>


        </p>
</div>
    </header><div class="p-summary entry-summary">
    <p></p>
<p>With all the hype about big data it's sometimes hard to realise that it's about more than just data. In fact, it's real interest doesn't come from big-ness at all.</p>
<!--more-->
<p>The term <em>big data</em> is deceptively hard to parse. It's a relative term, for a start: when you get down to it, all it really means is data that's large relative to the available storage and processing capacity. From that perspective, big data has always been with us -- and always will be. It's also curiously technology-specific for something that's garnering such broad interest. A volume of data may be "big" for one platform (a laptop, for example) and not for others (a computing cluster with a large network-attached store).</p>
<p>Getting away from the data, what researchers typically mean by big data is a set of computational techniques that can be applied broadly to data somewhat independent of its origins and subject. <a href="../../2014/03/data-scientists/" target="_blank">I'm not a fan of "data science"</a>, but the term does at least focus on techniques and not on data size alone. This is much more interesting, and poses a set of challenges to disciplines -- and to computer science -- to identify and expand the set of techniques and tools researchers can make use of.</p>
<p>What often frightens people about big data (or makes it an attractive career niche, depending on your point of view) is that there is a step-change in how you interact with data beyond a certain size -- and it's this change in tooling requirements that I think is a more significant indicator of a big-data problem than simply the data size.</p>
<p>Suppose you collect a dataset consisting of a few hundred samples, each with maybe ten variables -- something that could come from a small survey, for example. This size of data can easily be turned into a spreadsheet model, and a large class of researchers have become completely comfortable with building such models. (This didn't used to be the case: I had a summer job in the late 1980's building spreadsheet models for a group that didn't have the necessary experience. Research skills evolve.) Even a survey with thousands of rows would be possible.</p>
<p>But what if the survey has several million rows? -- for example because it came from an online survey, or because it was sensed automatically in some way. No spreadsheet program can ingest that much data.</p>
<p>A few million rows does not constitute what many people would regard as "big data": it'll be at most several megabytes. But that's not the point: rather, the point is that, in order to deal with it, the researchers have to change tools -- and change them quite radically. Rather than using a spreadsheet, they have to become programmers; not just that, they have to become programmers who are familiar with languages like <a href="https://www.python.org/" target="_blank">Python</a> (to get the libraries), and <a href="https://hadoop.apache.org/" target="_blank">Hadoop</a> and cloud computing (to be able to scale-out the solutions). They could employ programmers, of course, but that removes them from the immediate and intimate contact with their data that many researchers find extremely valuable, and that personal computers have given us. To many people, hiring a programmer and running calculations in the cloud is suspiciously like a return to the mainframe computing we thought was rendered obsolete decades ago.</p>
<p><em>It's not the terabytes, it's the tooling.</em> Big data starts when you cross the threshold from your familiar world of interactive tools and into a more specialised, programmers world.</p>
<p>This transition is becoming common in humanities and social sciences, as well as in science and medicine, and to my mind it's the major challenge of big data. The basic problem is simple: changing tools takes time, expertise, and mental effort, that is taken away from what's a researcher's actual research interest. A further disincentive is that the effort may not be rewarded: after all, if this really is research, one is often not sure whether there actually is anything valuable on the other side of a data analysis. In fields where competition is really competitive, like medicine, this feels like a lot of risk for an uncertain reward. There's evidence to suggest -- and I can't prove this contention -- that people are steering clear of doing the experiments they know they should do because they know they'll generate data volumes they're uncomfortable with.</p>
<p>This, then, is where the big data challenge actually is: minimising the cost of transition from tools that are familiar to tools that are effective on the larger data volumes that are now becoming commonplace. This is a programming and user interface challenge, to make the complex infrastructure appear easy and straightforward to people who want to accomplish tasks on it. A large challenge, but not an unprecedented one: I'm writing this just after the 25th birthday of the World Wide Web that took a complex infrastructure (the internet) and made it usable by everyone. Let's just not get too hung-up on the idea that data needs to be really big to be interesting in this new data-driven world.</p>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="../../2014/03/12/data-scientists/" class="u-url">No data scientists</a></h1>

        
        <div class="metadata">
            <p class="dateline"><a href="../../2014/03/12/data-scientists/" rel="bookmark"><i class="fa fa-clock"></i> <time class="published dt-published" datetime="2014-03-12T11:00:29Z" title="2014-03-12 11:00">2014-03-12 11:00</time></a></p>

                <p class="commentline"><i class="fa fa-comment"></i>         <a href="../../2014/03/12/data-scientists/#disqus_thread" data-disqus-identifier="cache/posts/2014/03/12/data-scientists.html">Comments</a>


        </p>
</div>
    </header><div class="p-summary entry-summary">
    <p></p>
<p>"Data science" and "data scientist" are not good terms -- for anything.</p>
<!--more-->
<p>The recent revolution in our ability to monitor more and more human (and indeed non-human) activity has spawned a whole new field of study. Known variously as "big data", "data science", and "digital humanities", the idea is that -- by studying the data collected -- we can perform feats of prediction and customisation that defeat other approaches.</p>
<p>This is not all hype. There's no doubt that deriving algorithms from data -- known as "data-driven design" -- can often work better than <em>a priori</em> design. The best-known example of this is <a href="http://translate.google.com/" target="_blank">Google Translate</a>, whose translation approach is driven almost entirely by applying statistical learning to a large corpus of documents with for which exact translations exist across languages, and using the correlations found as the basis for translation. (The documents in question were actually the core agreements governing the operation of the EU, the <em>acquis communautaire</em>, which explains why Google Translate works best on bureaucratese and worst on poetry.) It turns out that data-driven learning works better in most cases than grammar-directed parsing.</p>
<p>The data-driven approach rests on several pillars. Chief among them is applied machine learning as mentioned above, allowing algorithms to look through bodies of data and learn the correlations that exist between events. (We use similar techniques to analyse sensor data and perform situation recognition. See Ye, Dobson and McKeever. <a href="http://dx.doi.org/10.1016/j.pmcj.2011.01.004" target="_blank">Situation identification techniques in pervasive computing: a review</a>. Pervasive and Mobile Computing <strong>8</strong>(1), pp. 36--66. 2012.) Various other statistical techniques can also be applied, ranging up from simple mean and variance calculations. One can usually augment the basic techniques using more structural methods: if you know the structure of the links in a web site, for example, you can learn more about how people navigate because you know that some routes are more probable (by clicking hyperlinks) than others. The results of these analyses then need to be presented as analytics for consumption by managers and decision-makers, and distilling large volumes of information into visually compelling and comprehensible form is a skill in itself.</p>
<p>Going a stage further, one may conduct experiments directly. If you see people searching for the same term, present the results to different people in different ways and see how that influences the links they click. Google again are at the forefront.</p>
<p>The excitement of these techniques has spilled over into the wider science and humanities landscapes. Just within St Andrews this week I've talked about projects for analysing DNA data using machine learning, improving clinical trials, building a concept map for two centuries-worth of literature pertaining to Edinburgh, detecting intruders in computer systems, and mapping the births, deaths, and marriages of Scotland from parish records -- and it's only Wednesday. All these activities are inherently data-driven: they only become possible because we can ingest and analyse large data volumes at high-enough speeds.</p>
<p>However, none of this implies that there is a subject called "data science", and I'm starting to worry that a false premise is being established.</p>
<p>The term "data science" is a tricky one to parse. How many sciences do you know that aren't based in data? "Data-driven" is the same. That's not to say that they're meaningless, exactly: they identify a sub-set of techniques that are qualitatively different to the more theory-driven approaches, and even differentiate themselves from experimentally-driven approaches by their attempt to correlate across datasets rather than being based on a single homogeneous sampling (however large). But that's a nuanced reading, and a general reader would be forgiven for believing that "data science" was somehow a separate discipline from "ordinary" science, instead of it denoting a set of computational techniques with general applicability across the range of (non-)traditional sciences.</p>
<p>But it's "data scientist" that really troubles me. This seems to suggest that one can find scientific meaning in the data and the data alone. It seems to suggest that there's a short-cut to scientific insight through the data (and by implication, computer science) rather than through traditional scientific training. And this simply isn't true.</p>
<p>The problem is the age-old different between correlation and causality. Correlation is when things happen together: you leave a cup of a tea and a biscuit on a table for long enough, the tea goes cold and the biscuit gets stale. Causality is when one thing happening triggers another thing happening: the tea got cold, and that made the biscuit become stale. Mistaking one for the other leads to all sorts of interesting possibilities: if we put the tea in an insulated cup, the biscuit will stay fresh longer.</p>
<p>Now, the final tea-and-biscuit example is clearly meaningless, but ask yourself this: how did you <em>recognise</em> it as meaningless? Because you understand that the two effects are happening independently because of the passage of time, not because of each other: they are correlated but not causative. You understand this because you have insight into the processes of the world.</p>
<p>And it's here that the problems start for data scientists. In order to interpret the machine learning, statistics, analytics, or other results, you have to have an understanding of the underlying processes. <em>It doesn't happen in the data at all.</em> That's fine for tea and biscuits, and is also probably fairly fine for sales of consumer goods on mass-market web sites, where we have a good intuitive understanding of the processes involved, but will drop-off rapidly as we approach areas that are more complex, more noisy, less intuitive, and less well-understood. How can you differentiate correlation from causality if you don't understand what's possible in the underlying domain? How, in fact, can you determine anything from the data in and of itself?</p>
<p>This suggests to me that data scientist isn't a job description, or even an aspiration: it's a misnomer that should really be read as "a trained scientist working with lots of rich empirical data". Not as sexy, but probably more useful and less likely to disappoint everyone involved.</p>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="../../2014/02/14/nocost-journals/" class="u-url">No-cost journals</a></h1>

        
        <div class="metadata">
            <p class="dateline"><a href="../../2014/02/14/nocost-journals/" rel="bookmark"><i class="fa fa-clock"></i> <time class="published dt-published" datetime="2014-02-14T08:30:06Z" title="2014-02-14 08:30">2014-02-14 08:30</time></a></p>

                <p class="commentline"><i class="fa fa-comment"></i>         <a href="../../2014/02/14/nocost-journals/#disqus_thread" data-disqus-identifier="cache/posts/2014/02/14/nocost-journals.html">Comments</a>


        </p>
</div>
    </header><div class="p-summary entry-summary">
    <p></p>
<p>What have scientific journals ever done for us? And can we get the benefits without the access issues?</p>
<!--more-->

<p>"Open access" is a big thing in scientific publishing these days. The UK research councils, who fund a large fraction of the UK's academic research, have decided that papers arising from their research have to made available to any interested reader at no charge. The argument is that publicly-funded research results are a public good, and that other researchers should not be impeded in building on results. Since science progresses by researchers building on each others' work, there is plenty of justification for this view.</p>
<p>You would think that open access wouldn't be a problem in these days of personal web pages and Google. However, when publishing a paper in a major journal, the authors typically sign away their copyright to the journal publisher, meaning that they can't legally make the paper freely available. The publishers in turn lock the papers away, either in dead-tree form (which they then sell to university libraries at exorbitant cost) or behind paywalls requiring individual or institutional subscription. The journals who do this are often the most important and prestigious venues, places where you want your work to appear, and scientists aren't going to stop publishing in these places any time soon.</p>
<p>To address open access, some journals have started charging open access fees, whereby an author can pay to have their article made open access (i.e., to appear outside the paywall). Of course, anyone funded by a UK research council basically has to pay these fees to be compliant with their funding. Effectively, though, it means that institutions typically pay twice for publication: they pay the open access fee for individual articles, but still need to subscribe to the paid-for journal to get access to other papers. There are also open access journals that charge a publication fee for each accepted paper, but these are still quite new and with some exceptions (most notably <a href="http://www.plosone.org/" target="_blank">PLoS</a>) still fairly low-grade.</p>
<p>These issues got me thinking: what do the journals actually give us? And could we get the benefit using internet technology without the costs?</p>
<p>Historically journals served as the primary means of academic communication, but clearly that time has passed. Nowadays journals give us two things:
</p>
<ol>
<li>An editor and editorial board acting as guardians of the quality of the papers accepted. As a general rule, you never publish in a journal where you don't recognise any of the names on the editorial board: you want a journal managed by people known in your field.</li>
    <li>A brand that gives readers confidence that this journal will contain significant work that justifies the time spent reading it.</li>
    <li>Persistent storage of articles to give confidence that they can be found, referenced, and accessed in the future.</li>
</ol>
Clearly (2) is a function of (1), in the sense that the brand is built by the demonstrated consistently by the editorial board. It typically takes time to develop for a new journal. As to (3), persistent storage isn't much of a problem these days, but finding a copy of a paper could well be.
<p>In building our now-cost journal, we therefore need to replicate (1) and (3) in order to build (2).</p>
<p>Here's a possible workflow. We establish the journal's web site: the <em>St Andrews Journal of Interesting Things</em>, perhaps. Like most journals, this allows prospective authors to submit their manuscripts, which are passed to the editorial board for review.</p>
<p>Academics typically serve on editorial boards for free. They are self-organising, in the sense that the editor-in-chief (EIC) appoints a set of trusted lieutenants consisting of his friends, colleagues, and people well-known in the field of the journal. Doing this well is a major skill -- but an individual one, dependent on the selection of a good EIC. The editorial board are assigned incoming papers, which they ask their friends and colleagues to review and provide comments one. Again, the selection of reviewers is critical to the quality of papers, as the reviewers are expected to assess the work presented and to suggest changes (or reject the paper completely). Academics typically review for free, too, so you'll notice that, for a typical journal, the total cost so far has been running the web server that manages the editorial process.</p>
<p>Papers typically go around one or two rounds of revision before being accepted and published. The problem here is that we need to show readers that the paper has passed through the quality assurance of review. Anyone can put an article on the web, but journals guarantee that the work has been looked at and approved by the authors' scientific peers. (This doesn't guarantee that journal-published work is correct, merely that it's sufficiently convincing to a suitably qualified set of reviewers. There is always a steady stream of corrections, retractions, and withdrawals as flaws are found in work post-publication.) In a paid-for journal, the guarantee comes from printing on paper: you can check whether a paper purporting to appear in a journal actually does so by checking the appropriate volume. This of course implies printing and distribution, which publishers claim is source of their need for fees. For the St Andrews Journal of Interesting Things we want to avoid this cost.</p>
<p>Actually, this is technically straightforward in a number of ways. The complicated way is to create a machine-readable metadata file containing the paper's title, authors, abstract, journal reference, associate editor in charge, and maybe some other details. We then bind this file cryptographically to the final ("camera-ready") manuscript. The cryptography guarantees two things: that the binding was done by the journal editor, and that neither file has been changed since being bound together. Anyone downloading the file bundle can then check that the metadata and manuscript match, and therefore knows that the paper is the one "published".</p>
<p>(The simple way is to add a header to the manuscript text and then cryptographically sign the resulting file. This is trivially accomplished using a tool like Adobe Acrobat Pro, but is less attractive than the metadata approach because the header isn't machine-readable, making it harder to index the paper.)</p>
<p>There is no cost associated with either of these approaches. We can then give the signed file back to the authors and tell them to place it on any web server that Google will index. This will let anyone searching for the file to find it: that's what search engines do. If we want to be really thorough, we would keep track of where the files are stored, and/or perform regular searches to locate them (easy enough given machine-readable metadata), and maintain a journal web page listing the published papers and linking to them. (Total cost: one web page.) If we want to be <em>really</em> thorough we can mint <a href="http://www.doi.org/" target="_blank">Digital Object Identifiers</a> that resolve through our web server to the paper locations. (Total cost: a small database and a single CGI script on our web server.)</p>
<p>We've now recreated the publication side of the journal industry, essentially for free. This leaves the branding issue. There are two sides to establishing a brand: quality and visibility. The quality of the product, as mentioned above, relies on the selection of editorial and review teams and their willingness to serve at no cost, as is normal in academic publishing. The visibility issue is harder to crack, but could be addressed using the web, by viral marketing and appearances at conferences that editorial board members were attending, by word of mouth through the research community -- and even by advertising in the paid-for journals themselves, possibly. One great thing about the web and social media is that word <em>will</em> get out: after that, it's a matter of the quality of papers accepted and the willingness of authors to contribute.</p>
<p>I'm not actually planning on setting up a new journal. The point is that 21st century research doesn't need the friction and costs imposed by journals whose main editorial services are provided free by their consumers anyway. We should be able to do away with these costs without sacrificing the quality of material that we read or the reliance we place upon it.</p>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="../../2013/12/09/funded-phd-positions/" class="u-url">Funded PhD positions available</a></h1>

        
        <div class="metadata">
            <p class="dateline"><a href="../../2013/12/09/funded-phd-positions/" rel="bookmark"><i class="fa fa-clock"></i> <time class="published dt-published" datetime="2013-12-09T09:46:01Z" title="2013-12-09 09:46">2013-12-09 09:46</time></a></p>

                <p class="commentline"><i class="fa fa-comment"></i>         <a href="../../2013/12/09/funded-phd-positions/#disqus_thread" data-disqus-identifier="cache/posts/2013/12/09/funded-phd-positions.html">Comments</a>


        </p>
</div>
    </header><div class="p-summary entry-summary">
    <p></p>
<p>The School of Computer Science at the University of St Andrews has around eight  fully-funded PhD positions available. I'd welcome applicants interested in sensor networks, complex systems, and data science.</p>
<!--more-->

<p>We welcome students from a wide range of countries, our only major requirements being that you're excited by the idea of research and are able to conduct a complex programme within a small, friendly, and supportive environment.</p>
<p>In my case, I'm interested in hearing from potential students with interests in the following areas:
</p>
<ul>
<li>Sensor networks, especially deploying sensors into the environment;</li>
    <li>Complex system modelling, trying to model phenomena that operate on a range of scales; and</li>
    <li>Data science, particularly for how we collect, categorise, and work with large scientific datasets.</li>
</ul>
An early conversation by skype or email could be followed by a formal application, the details of which are available <a href="http://www.jobs.ac.uk/job/AHU246/funded-phd-research-studentships/" target="_blank">here</a>.
<p></p>
    </div>
    </article>
</div>
        <nav class="postindexpager"><ul class="pager">
<li class="next">
                <a href="index-23.html" rel="next">Newer posts</a>
            </li>
            <li class="previous">
                <a href="index-21.html" rel="prev">Older posts</a>
            </li>
        </ul></nav><script>var disqus_shortname="simoninireland";(function(){var a=document.createElement("script");a.async=true;a.src="https://"+disqus_shortname+".disqus.com/count.js";(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(a)}());</script>
</div>
  </section><script src="../../assets/js/baguetteBox.min.js"></script><script src="../../assets/js/moment-with-locales.min.js"></script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(2, "YYYY-MM-DD HH:mm");
  </script><!-- end fancy dates --><script>
    baguetteBox.run('div#content', {
    ignoreClass: 'islink',
    captions: function(element) {
    return element.getElementsByTagName('img')[0].alt;
    }});
  </script><script src="../../assets/js/ToggleNav.js"></script>
</body>
</html>

<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Simon Dobson (Posts about context)</title><link>https://simondobson.org/</link><description></description><atom:link href="https://simondobson.org/categories/context.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2024 &lt;a href="mailto:simoninireland@gmail.com"&gt;Simon Dobson&lt;/a&gt; </copyright><lastBuildDate>Mon, 01 Jan 2024 19:20:16 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>The semantic web: good ideas poorly supported?</title><link>https://simondobson.org/2012/02/02/semantic-web/</link><dc:creator>Simon Dobson</dc:creator><description>&lt;p&gt;The semantic web and open linked data open-up the vision of scientific data published in machine-readable form. But their adoption faces some challenges, many self-inflicted.

&lt;!--more--&gt;

Last week I taught &lt;a href="https://simondobson.org/teaching/ovronnaz/" target="_blank"&gt;a short course on programming context-aware systems&lt;/a&gt; as a Swiss doctoral winter school. The idea was to follow the development process from the ideas of context, through modelling and representation, to reasoning and maintenance.

Context has a some properties that make it challenging for software development. The data available tends to be heterogeneous, dynamic and richly linked. All these properties can impede the use of normal approaches like object-oriented design, which tend to favour systems that can be specified in a static object model up-front. Rather than use an approach that's highly structured from its inception, am alternative approach is to use an open, unstructured representation and then add structure later using auxiliary data.This leads naturally into the areas of &lt;a href="http://linkeddata.org" target="_blank"&gt;linked open data&lt;/a&gt; and the semantic web.

The &lt;a href="https://en.wikipedia.org/wiki/Semantic_Web" target="_blank"&gt;semantic web&lt;/a&gt; is a term coined by Tim Berners-Lee as a natural follow-on from his original web design. Web pages are great for browsing but don't typically lend themselves to automated processing. You may be able to extract my phone number from &lt;a href="https://simondobson.org/contact-details/" target="_blank"&gt;my contact details page&lt;/a&gt;, for example, but that's because you understand typography and abbreviation: there's nothing that explicitly marks the number out from the other characters on the page. Just as the web makes information readily accessible to people, the semantic web aims to make information equally accessible to machines. It does this by allowing web pages to be marked-up using a format that's more semantically rich than the usual HTML. This uses two additional technologies: the &lt;a href="http://www.w3.org/TR/2004/REC-rdf-primer-20040210/" target="_blank"&gt;Resource Description  Framework (RDF)&lt;/a&gt; to assert facts about objects, &lt;a href="http://www.w3.org/TR/rdf-sparql-query/" target="_blank"&gt;SPARQL&lt;/a&gt; to access to model using queries, and the &lt;a href="http://www.w3.org/TR/owl-guide/" target="_blank"&gt;Web Ontology  Language (OWL)&lt;/a&gt; to describe the structure of a particular domain of  discourse. Using the example above, RDF would mark-up the phone number, email address &lt;em&gt;etc&lt;/em&gt; explicitly, using terminology described in OWL to let a computer understand the relationships between, for example, a name, an email address, an employing institution and so on. Effectively the page, as well as conveying content for human consumption, can carry content marked-up semantically for machines to use autonomously. And of course you can also create pages that are &lt;em&gt;purely&lt;/em&gt; for machine-to-machine interaction, essentially treating the web as a storage and transfer mechanism with RDF and OWL as semantically enriched transport formats.

So far so good. But RDF, SPARQL and OWL are far from universally accepted "in the trade", for a number of quite good reasons.

The first is verbosity. RDF uses XML as an encoding, which is quite a verbose, textual format. Second is complexity: RDF makes extensive use of XML namespaces, which add structure and prevent misinterpretation but make pages harder to create and parse. Third is the exchange overhead, whereby data has to be converted from in-memory form that programs work with into RDF for exchange and then back again at the other end, each step adding &lt;em&gt;more&lt;/em&gt; complexity and risks of error. Fourth is the unfamiliarity of many of the concepts, such as the dynamic non-orthogonal classification used in OWL rather than the static class hierarchies of common object-oriented approaches. Fifth is the disconnect between program data and model, with SPARQL sitting off to one side like SQL. Finally there is the need for all these technologies &lt;em&gt;en masse&lt;/em&gt; (in addition to understanding HTTP, XML and XML Schemata) to perform even quite simple tasks, leading to a steep learning curve and a high degree of commitment in a project ahead of any obvious returns.

So the decision to use the semantic web isn't without pain, and one needs to place sufficient value on its advantages -- open, standards-based representation, easy exchange and integration -- to make it worthwhile. It's undoubtedly attractive to be able to define a structure for knowledge that exactly matches a chosen sub-domain, to describe the richness of this structure, and to have it compose more or less cleanly with &lt;em&gt;other&lt;/em&gt; such descriptions of complementary sub-domains defined independently -- and to be able to exchange all this knowledge with anyone on the web. But this flexibility comes with a cost and (often) no obvious immediate, high-value benefits.

Having taught this stuff, I think the essential problem is one of tooling and integration, not core behaviour. The semantic web does include some really valuable concepts, but their realisation is currently poor and this poses a hazard to their adoption.

In many ways the use of XML is a red herring: no sane person holds data to be used programmatically as XML. It is -- and was always intended to be -- an exchange format, not a data structure. So the focus needs to be on the data model underlying RDF (subject-predicate-object triples with subjects and predicates represented using URIs) rather than on the use of XML.

While there are standard libraries and tools for use with the semantic web -- in Java these include &lt;a href="http://incubator.apache.org/jena/" target="_blank"&gt;Jena&lt;/a&gt; for representing models, &lt;a href="http://clarkparsia.com/pellet/" target="_blank"&gt;Pellet&lt;/a&gt; and other reasoners providing ontological reasoning, and &lt;a href="http://protege.stanford.edu/" target="_blank"&gt;Protégé&lt;/a&gt; for ontology development -- their level of abstraction and integration with the rest of the language remain quite shallow. It is hard to ensure the validity of an RDF graph against an ontology, for example, and even harder to validate updates. The type systems also don't match, either statically or dynamically: OWL performs classification based on attributes rather than by defining hard classes, and the classification may change unpredictably as attributes are changed. (This isn't just a problem for statically-typed programming languages, incidentally: having the objects you're working with re-classified can invalidate the operations you're performing at a semantic level, regardless of whether the type system complains.) The separation of querying and reasoning from representation is awkward, rather like the use of SQL embedded into programs: the query doesn't fit naturally into the host language, which typically has no syntactic support for constructing queries.

Perhaps the solution is to step back and ask: what problem does the semantic web solve? In essence it addresses the open and scalable mark-up of data across the web according to semantically meaningful schemata. &lt;em&gt;But programming languages don't do this&lt;/em&gt;: they're about nailing-down data structures, performing local operations efficiently, and letting developers share code and functionality. So there's a mis-match between the goals  of the two system components, and their strengths don't complement each other in the way one might like.

This suggests that we re-visit the integration of RDF, OWL and SPARQL into programming languages; or, alternatively, that we look at for what features would provide the best computational capabilities alongside these technologies. A few characteristics spring to mind:
&lt;/p&gt;&lt;ul&gt;
    &lt;li&gt;Use classification throughout, a more dynamic type structure than classical type systems&lt;/li&gt;
    &lt;li&gt;Access RDF data "native", using binding alongside querying&lt;/li&gt;
    &lt;li&gt;Adopt the XML Schemata types "native" as well&lt;/li&gt;
    &lt;li&gt;Make code polymorphic in the manner of OWL ontologies, so that code can be exchanged and re-used. This implies basing typing on reasoning rather than being purely structural&lt;/li&gt;
    &lt;li&gt;Hiding namespaces, URIs and the other elements of RDF and OWL behind more familiar (and less intrusive) syntax (while keeping the semantics)&lt;/li&gt;
    &lt;li&gt;Allow programmatic data structures, suited to local use in a program, to be layered onto the data graph without forcing the graph itself into convoluted structures&lt;/li&gt;
    &lt;li&gt;Thinking about the global, non-local data structuring issues&lt;/li&gt;
    &lt;li&gt;Make access to web data intrinsic, not something that's done outside the normal flow of control and syntax&lt;/li&gt;
&lt;/ul&gt;
The challenges here are quite profound, not least from relatively pedestrian matters like concurrency control, but at least we would then be able to leverage the investment in data mark-up and exchange to obtain some of the benefits the semantic web clearly offers.</description><category>context</category><category>owl</category><category>programming</category><category>rdf</category><category>semantics</category><category>sparql</category><guid>https://simondobson.org/2012/02/02/semantic-web/</guid><pubDate>Thu, 02 Feb 2012 08:00:06 GMT</pubDate></item><item><title>The next challenges for situation recognition</title><link>https://simondobson.org/2011/03/23/situations/</link><dc:creator>Simon Dobson</dc:creator><description>&lt;p&gt;As a pervasive systems research community we're doing quite well at  automatically identifying simple things happening in the world. What is  the state of the art, and what are the next steps?

&lt;!--more--&gt;

Pervasive  computing is about letting computers see and respond to human activity.  In healthcare applications, for example, this might involve monitoring  an elderly person in their home and checking for signs of normality:  doors opening, the fridge being accessed, the toilet flushing, the  bedroom lights going on and off at the right times, and so on. A  collection of simple sensors can provide the raw observational data, and  we can monitor this stream of data for the "expected" behaviours. If we  don't see them -- no movement for over two hours during daytime, for  example -- then we can sound an alarm and alert a carer. Done correctly  this sort of system can literally be a life-saver, but also makes all  the difference for people with degenerative illnesses living in their  own homes.

The science behind these systems is often referred to as &lt;em&gt;activity&lt;/em&gt; and situation &lt;em&gt;recognition&lt;/em&gt;, both of which are forms of &lt;em&gt;context fusion&lt;/em&gt;.  To deal with these in the wrong order: context fusion is the ability to  take several streams of raw sensor (and other) data and use it to make  inferences; activity recognition is the detection of simple actions in  this data stream (lifting a cup, chopping with a knife); and situation  recognition is the semantic interpretation of a high-level process  (making tea, watching television, in a medical emergency). Having identified the situation we can then provide an appropriate &lt;em&gt;behaviour&lt;/em&gt; for the system, which might involve changing the way the space is  configured (dimming the lights, turning down the sound volume), providing information ("here's the recipe you chose for tonight") or taking some external action (calling for help). This sort of &lt;em&gt;context-aware behaviour&lt;/em&gt; is the overall goal.

The state of the art in context fusion uses some sort of  uncertain reasoning including machine learning and other techniques that  are broadly in the domain of artificial intelligence. These are  typically more complicated than the complex event processing techniques  used in financial systems and the like, because they have to deal with  significant noise in the data stream. (Ye, Dobson and McKeever.  &lt;a href="https://simondobson.org/softcopy/situation-recognition-pmc11.pdf"&gt;Situation recognition techniques in pervasive computing: a review&lt;/a&gt;.  Pervasive and Mobile Computing. 2011.) The results are rather mixed,  with a typical technique (a naive Bayesian classifier, for example)  being able to identify some situations well and others far more poorly:  there doesn't seem to be a uniformly "good" technique yet. Despite this  we can now achieve 60-80% accuracy (by &lt;a href="https://secure.wikimedia.org/wikipedia/en/wiki/F1_score"&gt;F-measure&lt;/a&gt;, a unified measure of  the "goodness" of a classification technique) on simple activities and  situations.

That sounds good, but the next steps are going to be far harder.

To  see what the nest step is, consider that most of the systems explored  have been evaluated under laboratory conditions. These allow fine  control over the environment --&lt;em&gt; and that's precisely the problem&lt;/em&gt;. The next challenges for situation recognition come directly from the &lt;em&gt;loss&lt;/em&gt; of control of what's being observed.

Let's break down what needs to happen. Firstly, we need to be able to &lt;em&gt;describe&lt;/em&gt; situations in a way that lets us capture human processes. This is easy  to do to another human but tricky to a computer: the precision with  which we need to express computational tasks gets in the way.

For  example we might describe the process of making lunch as retrieving the  bread from the cupboard, the cheese and butter from the fridge, a plate  from the rack, and then spreading the butter, cutting the cheese, and  assembling the sandwich. That's a good enough description for a human,  but most of the time isn't exactly what happens. One might retrieve the  elements in a different order, or start making the sandwich (get the  bread, spread the butter) only to remember that you forgot the filling,  and therefore go back to get the cheese, then re-start assembling the  sandwich, and so forth. The point is that this isn't programming: people  don't do what you expect them to do, and there are so many variations  to the basic process that they seem to defy capture -- although no human  observer would have the slightest difficulty in classifying what they  were seeing. A first challenge is therefore &lt;em&gt;a way of expressing the real-world processes and situations we want to recognise&lt;/em&gt; in a way that's robust to the things people actually do.

(Incidentally,  this way of thinking about situations shows that it's the dual of  traditional workflow. In a workflow system you specify a process and  force the human agents to comply; in a situation description the humans  do what they do and the computers try to keep up.)

The second  challenge is that, even when captured, situations don't occur in  isolation. We might define a second situation to control what happens  when the person answers the phone: quiet the TV and stereo, maybe. But  this situation could be happening at the same time as the lunch-making  situation and will inter-penetrate with it. There are dozens of possible  interactions: one might pause lunch to deal with the phone call, or one  might continue making the sandwich while chatting, or some other  combination. Again, fine for a human observer. But a computer trying to  make sense of these happenings only has a limited sensor-driven view,  and has to try to associate events with interpretations &lt;em&gt;without knowing what it's seeing ahead of time&lt;/em&gt;.  The fact that many things can happen simultaneously enormously  complicates the challenge of identifying what's going on robustly,  damaging what is often already quite a tenuous process. We therefore  need techniques for describing &lt;em&gt;situation compositions and interactions&lt;/em&gt; on top of the basic descriptions of the processes themselves.

The  third challenge is also one of interaction, but this time involving  multiple people. One person might be making lunch whilst another watches  television, then the phone rings and one of them answers, realises the  call is for the other, and passes it over. So as well as  interpenetration we now have multiple agents generating sensor events,  perhaps without being able to determine exactly which person caused  which event. (A motion sensor sees movement: it doesn't see who's  moving, and the individuals may not be tagged in such a way that they  can be identified or even differentiated between.) Real spaces involve  multiple people, and this may place limits on the behaviours we can  demonstrate. But at the very least we need to be able to &lt;em&gt;describe processes involving multiple agents&lt;/em&gt; and to &lt;em&gt;support simultaneous situations in the same or different populations&lt;/em&gt;.

So for me the next challenges of situation recognition boil down to how we describe what we're expecting to observe in a way that reflects noise, complexity and concurrency of real-world conditions.  Once we have these we can explore and improve the techniques we use to  map from sensor data (itself &lt;a href="https://simondobson.org/2010/02/216/"&gt;noisy and hard to program with&lt;/a&gt;) to identified situations, and thence  to behaviour. In many ways this is a real-world version of the  concurrency theories and process algebras that were developed to  describe concurrent computing processes: process languages brought into  the real world, perhaps. This is the approach we're taking in a  European-funded research project, &lt;a href="http://www.sapere-project.eu/"&gt;SAPERE&lt;/a&gt;, in which we're hoping to understand how to engineer smart, context-aware systems on large and flexible scales.&lt;/p&gt;</description><category>context</category><category>pervasive systems</category><category>programming</category><guid>https://simondobson.org/2011/03/23/situations/</guid><pubDate>Wed, 23 Mar 2011 09:45:18 GMT</pubDate></item><item><title>Contextual processes</title><link>https://simondobson.org/2010/06/18/contextual-processes/</link><dc:creator>Simon Dobson</dc:creator><description>&lt;p&gt;Context-aware systems are intended to follow and augment user-led, real-world processes. These differ somewhat from traditional workflow processes, but share some characteristics. Might the techniques used to implement business processes &lt;em&gt;via&lt;/em&gt; web service orchestration fit into the context-aware landscape too?

&lt;!--more--&gt;These ideas arose as a result of discussions at the &lt;a href="http://www.pmmps.org"&gt;PMMPS&lt;/a&gt; workshop at &lt;a href="https://simondobson.org/2010/05/impressions-pervasive-2010/"&gt;PERVASIVE 2010 in Helsinki&lt;/a&gt;. In particular, I was thinking about comments &lt;a href="http://aquigley.blogspot.com/2010/04/april-2009-keynote-talk-ides-for.html"&gt;Aaron Quigley made in his keynote&lt;/a&gt; about the need to build platforms and development environments if we're to avoid forever building just demos. The separation of function from process seems to be working in the web world: might it work in the pervasive world too?

In building a pervasive system we need to integrate several components:
&lt;/p&gt;&lt;ol&gt;
    &lt;li&gt;A collection of &lt;em&gt;sensors&lt;/em&gt; that allow us to observe users in the real world&lt;/li&gt;
    &lt;li&gt;A &lt;em&gt;user&lt;/em&gt; or &lt;em&gt;situation model&lt;/em&gt; describing what the users are supposed to be doing, in terms of the possible observations we might make and inferences we might draw&lt;/li&gt;
    &lt;li&gt;A &lt;em&gt;context model&lt;/em&gt; that brings together sensor observations and situations, allowing us to infer the latter from a sequence of the former&lt;/li&gt;
    &lt;li&gt;Some &lt;em&gt;behaviour&lt;/em&gt; that's triggered depending on the situation we believe we're observing&lt;/li&gt;
&lt;/ol&gt;
Most current pervasive systems have quite simple versions of all these components. The number of sensors is often small -- sometimes only one or two, observing one user. The situation model is more properly an &lt;em&gt;activity model&lt;/em&gt; in that it classifies a user's immediate current activity, independently of any other activity at another time. The context model encapsulates a mapping from sensors to activities, which then manifest themselves in a activating or deactivating a single behaviour. Despite their simplicity, such systems can perform a lot of useful tasks.

However, pervasive activities clearly live on a timeline: you leave home &lt;em&gt;and then&lt;/em&gt; walk to work &lt;em&gt;and then&lt;/em&gt; enter your office &lt;em&gt;and then&lt;/em&gt; check your email, and so forth. One can treat these activities as independent, but that might lose continuity of behaviour, when what you want to do depends on the route by which you got to a particular situation. Alternatively we could treat the timeline as a process, and track the user's progress along it, in the manner of an office workflow.

Of course the problem is that users don't actually follow workflows like this -- or, rather, they tend to interleave actions, perform them in odd orders, leave bits out, drop one process and do another before picking-up the first (or not), and so on. So pervasive workflows aren't at all like "standard" office processes. They aren't discriminated from &lt;em&gt;other&lt;/em&gt; workflows (and non-workflow activities) happening simultaneously in the same space, with the same people and resources involved. In some simple systems the workflow actually is "closed", for example computer theatre (Pinhanez, Mase and Bobick. Interval scripts: a design paradigm for story-based interactive systems., Proceedings of CHI'97. 1997.) -- but in most cases its "open". So the question becomes, how do we describe "loose" workflows in which there is a sequence of activities, each one of which reinforces our confidence in later ones, but which contain noise and extraneous activities that interfere with the inferencing?

There are several formalisms for describing sequences of activities. The one that underlies Pinhanez' work mentioned above is Allen algebra (Allen and Ferguson. Actions and events in interval temporal logic. Journal of Logic and Computation &lt;strong&gt;4&lt;/strong&gt;(5), pp.531--579. 1994.) which provides a notation for specifying how intervals of time relate: an interval &lt;em&gt;a&lt;/em&gt; occurs strictly before another &lt;em&gt;b&lt;/em&gt;, for example, which in turn contains wholly within it another interval &lt;em&gt;c&lt;/em&gt;. It's easy to see how such a process provides a model for how events from the world &lt;em&gt;should&lt;/em&gt; be observed: if we see that &lt;em&gt;b&lt;/em&gt; has ended, we can infer that &lt;em&gt;c&lt;/em&gt; has ended also because we know that &lt;em&gt;c&lt;/em&gt; is contained within &lt;em&gt;b&lt;/em&gt;, and so forth. We can do this if we don't -- or can't -- directly observe the end of &lt;em&gt;c&lt;/em&gt;. However, this implies that we can specify the relationships between intervals precisely. If we have multiple possible relationships the inferencing power degrades rapidly.

Another way to look at things is to consider what "noise" means. In terms of the components we set out earlier, noise is the observation of events that don't relate to the process we're trying to observe. Suppose I'm trying to support a "going to work" process. If I'm walking to work and stop at a shop, for example, this doesn't interfere with my going to work -- it's "noise" in the sense of "something that happened that's non-contradictory of what we expected to see". On the other hand if, after leaving the shop, I go home again, that might be considered as "not noise", in the sense of "something that happened that contradicts the model we have of the process".&lt;em&gt;&lt;/em&gt; As well as events that support a process, we also have events that contradict it, and events that provide no information.

Human-centred processes are therefore stochastic, and we need a stochastic process formalism. I'm not aware of any that really fit the bill: process algebras seem too rigid. Markov processes are probably the closest, but they're really designed to capture frequencies with which paths are taken rather than detours and the like. Moreover we need to enrich the event space so that observations support or refute hypotheses as to which process is being followed and where we are in it. This is rather richer than is normal, where events are purely confirmatory. In essence what we have is &lt;em&gt;process as hypothesis&lt;/em&gt; in which we try to confirm that this process is indeed happening, and where we are in it, using the event stream.

It's notable that we can describe a process separately from the probabilities that constrain how it's likely to evolve, though. That suggests to me that we might need an approach like &lt;a href="http://en.wikipedia.org/wiki/Business_Process_Execution_Language"&gt;BPEL&lt;/a&gt;, where we separate the description of the process from the actions we take as a result, and also form the ways in which we move the process forward. In other words, we have a description of &lt;em&gt;what it means&lt;/em&gt; to go to work, expressed separately from &lt;em&gt;how&lt;/em&gt; we confirm that this is what's being observed in terms of sensors and events, and separated further from &lt;em&gt;what happens&lt;/em&gt; as a result of this process being followed. That sounds  a lot easier than it is, because some events are confirmatory and some aren't. Furthermore we may have several processes that can be supported  by observations up to a point and then diverge: going to work and going shopping are pretty similar until I go into a shop, and/or until I leave the shop and don't go to work. How do we handle this? We could enforce common-prefix behaviour, but that breaks the separation between process and action. We could insist on "undo" actions for "failed", no-longer-supported-by-the-observations processes, which severely complicates programming and might lead to interactions between different failed processes. Clearly there's something missing from our understanding of how to structure more complex, temporally elongated behaviours that'll need significant work to get right.</description><category>context</category><category>pervasive computing</category><category>programming</category><category>web services</category><guid>https://simondobson.org/2010/06/18/contextual-processes/</guid><pubDate>Fri, 18 Jun 2010 05:00:53 GMT</pubDate></item><item><title>Sensing financial problems</title><link>https://simondobson.org/2010/04/27/sensing-financial-problems/</link><dc:creator>Simon Dobson</dc:creator><description>&lt;p&gt;Just as we can think of large-scale, detailed financial modeling as &lt;a href="https://simondobson.org/2010/04/computer-science-financial-crisis/"&gt;an exercise in simulation and linked data&lt;/a&gt;, we can perhaps also see the detection of hazards as an exercise in sensor fusion and pervasive computing, turning a multitude of sensor data into derived values for risk and/or situations requiring attention. There's a wealth of research in these areas that might be applicable.

&lt;!--more--&gt;

Environmental and other sensing is intended to make real-world phenomena accessible directly to computers. Typically we simply collect data and archive it for later analysis; increasingly we also move decision-making closer to the data in order to take decisions in close to real time about the ways in which the data is sensed in the future (so-called &lt;em&gt;adaptive&lt;/em&gt; sensing) or to allow the computers to respond directly in terms of the services they provide (&lt;em&gt;autonomic&lt;/em&gt; or &lt;em&gt;pervasive&lt;/em&gt; systems).

Can we treat the financial markets as the targets of sensing? Well, actually, we already do. An index like the &lt;a href="http://www.ftse.com/"&gt;FTSE&lt;/a&gt; is basically providing an abstracted window onto the behaviour of an underlying process -- in this case a basket of shares from the top 100 companies listed on the London exchange -- that can be treated as an observation of an underlying phenomenon. This further suggests that the technology developed for autonomic and pervasive computing could potentially be deployed to observe financial markets.

In some sense, pricing is already based on sensing. A put option, for example -- where the buyer  gains the right to compel  the seller to buy some goods at some  point in the future at some  defined cost -- will, if exercised, have a definite value to the buyer &lt;em&gt;then&lt;/em&gt; (when executed). It's value &lt;em&gt;now&lt;/em&gt; (when sold) will  be less than this, however,  because of the risk that the option will not be exercised (because, for  example, the buyer can sell the goods to someone else for more than the  seller has contracted to pay for them). Deciding what value to assign to  this contract is then a function over the expected future behaviour of  the market for the underlying goods. This expectation is formed in part by observing the behaviour of the market in the past, combined with the traders' knowledge of (or guesses about) external factors that might affect the price.

These external factors are referred to in pervasive computing as &lt;em&gt;context&lt;/em&gt;, and are used to condition the ways in which sensor streams are interpreted (see &lt;a href="https://simondobson.org/research/publications/#ContextIsCentral"&gt;Coutaz &lt;em&gt;et alia&lt;/em&gt;&lt;/a&gt; for an overview). One obtains context from a number of sources, typically combining expert knowledge and sensor data. A typical pervasive system will build and maintain a &lt;em&gt;context model&lt;/em&gt; bringing together all the information it knows about in a single database. We can further decompose context into &lt;em&gt;primary&lt;/em&gt; context sensed directly from a data source and &lt;em&gt;secondary&lt;/em&gt; context derived by some reasoning process. If we maintain this database in a semantically tractable format such as &lt;a href="http://www.w3.org/RDF/"&gt;RDF&lt;/a&gt;, we can then reason about what's happening in order to classify what's happening in the real world (&lt;em&gt;situation recognition&lt;/em&gt;) and respond accordingly. Crucially, this kind of context processing can treat &lt;em&gt;all&lt;/em&gt; context as being sensed, not just real-world data: we often "sense" calendars, for example, to look for clues about intended activities and locations, integrating web mining into sensing. Equally crucially, we use context as evidence to support model hypotheses ("Simon is in a meeting with Graeme and Erica") given by the situations we're interested in.

A lot of institutions already engage in automated trading, driven by the behaviour of indices and individual stocks. Cast into sensor-driven systems terminology, the institutions develop a number of situations of interest (a time to buy, hold, sell and so forth for different portfolios) and recognise which are currently active using primary context sensed from the markets (stock prices, indices) and secondary context derived from this sensed data (stock plummeting, index in free-fall). Recognising a situation leads to a particular behaviour being triggered.

Linked data opens-up richer opportunities for collecting context, and so for the management of individual products such as mortgages. We could, for example, sense a borrower's repayment history (especially for missed payments) and use this both to generate secondary context (revised risk of default) and to identify situations of interest (default, impaired, at-risk). Banks do this already, of course, but there are advantages to the sensor perspective. For one, context-aware systems show us that it's the richness of links between  context that is the key to its usefulness. The more links we have, the more semantics we have over which to reason. Secondly, migrating to a context-aware platform means that additional data streams, inferences and situations can be added as-and-when required, without needing to re-architect the system. Given the ever-increasing amount of information available on-line, this is certainly something that might become useful.

Of course there are massive privacy implications here, not least in the use of machine classifiers to evaluate -- and of course inevitably &lt;em&gt;mis&lt;/em&gt;-evaluate -- individuals' circumstances. It's important to realise that this is going on anyway and isn't going to go away: the rational response is therefore to make sure we use the best approaches available, and that we enforce audit trails and transparency to interested parties. Credit scoring systems are notoriously opaque at present -- I've had experience of this myself recently, since credit history doesn't move easily across borders -- so there's a screaming need for systems that can explain and justify their decisions.

I suspect that the real value of a sensor perspective comes not from considering an individual institution but rather an entire marketplace. To use an example I'm familiar with from Ireland, one bank at one stage pumped its share price by having another bank make a large deposit -- but then loaned this second bank the money to fund the deposit. Contextualised analysis might have picked this up, for example by trying to classify what instruments or assets each transaction referred to. Or perhaps not: no system is going to be fully robust against the actions of ingenious insiders. The point is not to suggest that there's a foolproof solution, but rather to increase the amount and intelligence of surveillance in order to raise the bar. Given the costs involved in unwinding failures when detected late, it might be an investment worth making.&lt;/p&gt;</description><category>context</category><category>finance</category><category>ireland</category><category>linked data</category><category>risk</category><guid>https://simondobson.org/2010/04/27/sensing-financial-problems/</guid><pubDate>Tue, 27 Apr 2010 12:32:57 GMT</pubDate></item></channel></rss>
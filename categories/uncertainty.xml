<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Simon Dobson (Posts about uncertainty)</title><link>https://simondobson.org/</link><description></description><atom:link href="https://simondobson.org/categories/uncertainty.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2025 &lt;a href="mailto:simoninireland@gmail.com"&gt;Simon Dobson&lt;/a&gt; </copyright><lastBuildDate>Thu, 23 Jan 2025 17:16:12 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Big, or just rich?</title><link>https://simondobson.org/2013/07/31/big-rich/</link><dc:creator>Simon Dobson</dc:creator><description>&lt;p&gt;The current focus on "big data" may be obscuring something more interesting: it's often not the pure size of a dataset that's important.

&lt;!--more--&gt;

The idea of extracting insight from large bodies of data promises significant advances in science and commerce. Given a large dataset, "big data" techniques cover a number of possible approaches:
&lt;/p&gt;&lt;ul&gt;
    &lt;li&gt;Look through the data for recurring patterns (data mining)&lt;/li&gt;
    &lt;li&gt;Present a summary of the data to highlight features (analytics)&lt;/li&gt;
    &lt;li&gt;(Less commonly) Identify automatically from the dataset what's happening in the real world (situation recognition)&lt;/li&gt;
&lt;/ul&gt;
There's a wealth of &lt;a href="http://data.gov.uk/" target="_blank"&gt;UK government data available&lt;/a&gt;, for example. Making it machine-readable means it can be presented in different ways, for example &lt;a href="http://www.telegraph.co.uk/news/uknews/crime/8294450/Crime-maps-are-just-the-beginning.html" target="_blank"&gt;geographically&lt;/a&gt;. The real opportunities seem to come from cross-overs between datasets, though, where they can be mined and manipulated to find relationships that might otherwise remain hidden, for example the effects of crime on house prices.

Although the size and availability of datasets clearly makes a difference here -- big open data -- we might be confusing two issues. In some circumstances we might be better looking for smaller but richer datasets, and for richer connections between them.

&lt;em&gt;Big data&lt;/em&gt; is a strange name to start with: when is data "big"? The only meaningful definition I can think of is "a dataset that's large relative to the current computing and storage capacity being deployed against it" -- which of course means that big data has always been with us, and indeed always will be. It also suggests that data might become less "big" if we become sufficiently interested in it to deploy more computing power to processing it. The alternative term popular in some places, &lt;em&gt;data science&lt;/em&gt;, is equally tautologous, as I can't readily name a science that &lt;em&gt;isn't&lt;/em&gt; based on data. (This isn't just academic pedantry, by the way: terms matter, if only to distinguish what topics are, and aren't, covered by big data/data science research.)

It's worth reviewing what big data lets us do. Having more data is useful when looking for patterns, since it makes the pattern stand out from the background noise. Those patterns in turn can reveal important processes at work in the world underlying the data, processes whose reach, significance, or even existence may be unsuspected. There may be patterns in the patterns, suggesting correlation or causality in the underling processes, and these can then be used for prediction: if pattern A almost always precedes pattern B in the dataset, then when I see a pattern A in the future I may infer that there's an instance of B coming. The statistical machine learning techniques that let one do this kind of analysis are powerful, but dumb: it still requires human identification and interpretation of the underlying processes to to conclude that A &lt;em&gt;causes&lt;/em&gt; B, as opposed to A and B simply occurring together through some acausal correlation, or being related by some third, undetected process. A data-driven analysis won't reliably help you to distinguish between these options without further, non-data-driven insight.

Are there are cases in which less data is better? Our experience with situation recognition certainly suggests that this is the case. When you're trying to relate data to the the real world, it's essential to have &lt;em&gt;ground truth&lt;/em&gt;, a record of what &lt;em&gt;actually&lt;/em&gt; happened. You can then make a prediction about what the data indicates about the real world, and verify that this prediction is true or not against known circumstances. Doing this well over a dataset provides some confidence that the technique will work well against other data, where your prediction is all you have. In this case, what matters is not simply the size of the dataset, but its relationship to another dataset recording the actual state of the world: it's the &lt;em&gt;richness&lt;/em&gt; that matters, not strictly the size (although having more data to train against is always welcome).

Moreover, rich connections may help with the more problematic part of data science, the identification of the processes underlying the dataset. While there may be no way to distinguish causality from correlation within a single dataset -- because they look indistinguishably alike -- the patterns of data points in the one dataset may often be related to patterns and data points in another dataset in which they &lt;em&gt;don't&lt;/em&gt; look alike. So the richness provides a translation from one system to another, where the second provides discrimination not available in the first.

I've been struggling to think of an example of this idea, and this is the best I've come up with (and it's not all that good). Suppose we have tracking data for people around an area, and we see that person A repeatedly seems to follow person B around. Is A following B? Stalking them? Or do they live together, or work together (or even just close together)? We can distinguish between these alternatives by having a link from people to their jobs, homes, relationships and the like.

There's a converse concern, which is that poor discrimination can lead to the wrong conclusions being drawn: classifying person B as a potential stalker when he's actually an innocent who happens to follow a similar schedule. An automated analysis of a single dataset risks finding spurious connections, and it's increasingly the case that these false-positives (or -negatives, for that matter) could have real-world consequences.

Focusing on connections between data has its own dangers, of course, since we already know that we can make very precise classifications of people's actions from relatively small, but richly connected, datasets. Maybe the point here is that focusing exclusively on the size of a dataset masks both the advantages to be had from richer connections with other datasets, and the benefits and risks associated with smaller but better-connected datasets. Looking deeply can be as effective (or more so) as looking broadly.</description><category>big data</category><category>machine learning</category><category>uncertainty</category><guid>https://simondobson.org/2013/07/31/big-rich/</guid><pubDate>Wed, 31 Jul 2013 13:58:49 GMT</pubDate></item><item><title>What computer science can learn from finance</title><link>https://simondobson.org/2010/04/16/computer-science-learn-finance/</link><dc:creator>Simon Dobson</dc:creator><description>&lt;p&gt;Science should be a two-way street. In the same way that we might view the financial system as &lt;a href="https://simondobson.org/2010/04/computer-science-financial-crisis/"&gt;a computational model that can be simulated&lt;/a&gt;, there are undoubtedly things that computing can learn from finance. In particular, a lot of financial instruments and techniques evolved because of uncertainty and risk: issues that are absolutely a part of computer science in general and of &lt;a href="https://simondobson.org/2010/02/216/"&gt;sensorised systems in particular&lt;/a&gt;. The financial crunch hasn't invalidated the  significance that these techniques may have for computer science and systems architecture.

&lt;!--more--&gt;

There 's an increasing acceptance that economics as a whole has lessons for computing. The connection may not be obvious, but if one considers that most modern systems of any scale are built from components, and often from agents whose goals are not necessarily completely aligned and whose actions need not be trusted, one begins to see how a model that assumes only self-interest might have relevance. Typically economics is used to provide models for large-scale behaviour, rather than being treated as a way to formulate or program the agents themselves. (For a survey of recent results see Neumann, Baker, Altmann and Rana (eds). &lt;a href="http://www.springer.com/birkhauser/computer+science/book/978-3-7643-8896-6" target="_blank"&gt;Economic models and algorithms for distributed systems&lt;/a&gt;. Birkhäuser. 2010.)

How about the individual instruments, though?

Let's take a step back and think what a financial instrument actually &lt;em&gt;is&lt;/em&gt;, or rather what it's goals are. To make money, clearly, but making money is easy &lt;em&gt;if&lt;/em&gt; you know the exact circumstances in which a transaction will take place. It's generally accepted in the economics of ordinary trade, for example, that &lt;em&gt;both&lt;/em&gt; sides of the transaction win from the trade: the seller will only sell if the goods sell for more than he spent in creating them, while the buyer will only buy if the goods have a higher utility value to him than the cost of their purchase. (It's interesting that it's necessary to point this out, since a lot of people tacitly assume that trade is only good for one side or the other, not for both.) The different between this sort of economic activity and financial instruments such as shares and derivatives is that the traders know (most of) the circumstances that affect their valuations, whereas the value of a mortgage or share is affected by events that haven't happened yet.

If we represent instruments as computational objects, then we can look at the ways in which this uncertainty might be managed. A common way is to identify the risks that affect the value of an instrument, and &lt;em&gt;hedge&lt;/em&gt; them. To hedge a risk, you find another instrument whose value will change in a way that balances that of the initial risk. The idea is that a loss on the first is compensated, at least in part, by a gain on the second.

The best example I can find -- given my very limited understanding -- is airline fuel. An airline knows it needs fuel in the future, and can even estimate how much it will need, but doesn't want to buy it all up-front because things might change. This means that the airline doesn't know how much the fuel will cost, since the price may change. A large increase would cost money since tickets already sold would reflect the earlier, lower fuel price. Passing this on later as a surcharge is extremely unpopular. What the airline &lt;em&gt;can&lt;/em&gt; do is to buy a fuel future, a &lt;em&gt;call option&lt;/em&gt;, that gives it the right to buy fuel from someone at a fixed price per litre. That is, someone guarantees that they will sell the airline the fuel it needs in (say) six months at a given price. The option itself costs money, above and beyond the cost of the fuel, but if the prices are right the airline is insulated against surges in the fuel price. If in six months the cost of fuel is higher than the cost in the call option, the airline exercises the option, buys the fuel, and makes money &lt;em&gt;versus&lt;/em&gt; what it would have paid; if the price is less than that in the call option, the airline just writes-off the cost of the option and buys the fuel in the market. Either way the airline caps its exposure and so controls its future risk. (If this all sounds one-sided, the entity selling the option to the airline needs to make money too -- perhaps by having a source of fuel at a known price, or having a stockpile on hand, or simply betting that the prices will move in a certain way.)

There is already experience in writing financial instruments, including derivatives and the like, using programming languages. (See, for example, Peyton Jones and Eber. Composing contracts: an adventure in financial engineering. Proc. ICFP. Montreal, CA. 2000.) Instruments represented as computational objects can be linked to their hedges. If we're &lt;a href="https://simondobson.org/2010/04/computer-science-financial-crisis/"&gt;simulating the the market&lt;/a&gt;, we can also simulate the values of hedges and see under what circumstances their values would fall alongside the original assets. That shows up risks of large crashes. At present this is done at a coarse statistical level, but if we link instruments to their metadata we can get a much finer simulation.

We can potentially use a similar technique for &lt;em&gt;any&lt;/em&gt; system that's exposed to future uncertainty, though. Systems that do  inference have to live the risk that their inferences will be wrong. In a pervasive computing system such as a smart building, for example, one typically looks at a range of sensors to try to determine what's happening and respond accordingly (by opening doors, providing information, sounding alarms or whatever else is needed by the application). Most such actions occur by inference, and can typically be wrong.

How might the lessons of finance help? We have a process that is dealing with future uncertainty, and whose utility is governed by how well it manages to address that uncertainty relative to what's "really happening" in the world. If we rely on our single inference process, and it makes a mistake -- as it inevitably will -- we're left completely exposed: the utility of the system is reliant on the correctness of the single inference function. Put in the terms above, it isn't hedged.

So we might require each inference process to come with a hedge. That is to say, each process not only specifies &lt;em&gt;what&lt;/em&gt; &lt;em&gt;should happen&lt;/em&gt; as a result of its determination of the current situation; it also specifies &lt;em&gt;what should happen if we get it wrong&lt;/em&gt;, if later evidence shows that the inference we made was wrong. This might be something simple like reversing the action we took: we turned the lights on because we thought a room was occupied, and in mitigation if we're wrong we turn the lights off again. The "cost" of this action is some wasted power. Some processes aren't reversible, of course: if we (think we) recognise someone at a door, and open it to admit them, and then discover we made a mistake and it's not the person we thought, just locking the door again won't work. We could however, take some other action (sound an alarm?), and accept a "cost" that is expressed in terms of the inappropriate or malicious actions the person might have taken as a result of our mistake. Moreover because we've combined the action and its hedge, we can assess the potential costs involved and perhaps change the original inference algorithm to for example require more certainty.

Essentially each behaviour that's triggered comes with another, related behaviour that's triggered in the event that the first one shouldn't have been. There are some interesting lifecycle issues involved turning this into a programming system, since the lifetime of the hedging behaviour might be longer than that of the original behaviour: even after the person has left the room, we might want to sound the alarm if we realise we made a mistake letting them in. The costs of mistakes might also be a function of time, so that problems detected later are more costly.

The point here is not that pervasive and sensor systems can get things wrong -- of course they can -- but that we need to design systems on the assumption that they &lt;em&gt;will&lt;/em&gt; get things wrong, and account for the costs (financial and otherwise) that this exposes us to and the actions we can take to mitigate them. The financial system does this: it recognises that there is uncertainty in the world and tries to construct a network of agents which will gain value regardless of the uncertain events that happen. The process isn't perfect, as we've seen in recent financial events, but it's at least a recognition within the system of the impact that uncertainty will have. That's something we could learn in, and incorporate into, computer science and systems engineering.&lt;/p&gt;</description><category>finance</category><category>uncertainty</category><guid>https://simondobson.org/2010/04/16/computer-science-learn-finance/</guid><pubDate>Fri, 16 Apr 2010 05:00:58 GMT</pubDate></item><item><title>Programming with limited certainty</title><link>https://simondobson.org/2010/02/26/216/</link><dc:creator>Simon Dobson</dc:creator><description>&lt;p&gt;Sensor networks are all about uncertainty: if the sensor says it's 20°C  out there, then it might be 20°C plus-or-minus half a degree or so (limited precision); or it might be some different temperature, and the sensor's just reported a duff value for some reason (limited accuracy). By contrast, computers most definitely &lt;em&gt;aren't&lt;/em&gt; about uncertainty, a fact enshrined in the famous maxim "garbage in, garbage out". What does this mean for our ability to build really large, robust and flexible sensor networks?

&lt;!--more--&gt;All the really foundational models of computing -- λ calculus, Turing machines and the like -- pretty much reflect this notion that input is correct in some sense, and if it's wrong then that's an error to be corrected outside the computational system. That seems to mean that the computational system can't itself either tolerate or express the notions of limited certainty -- precision and accuracy -- that lie at the core of sensor networks (and a lot of other modern systems, or course). That suggests to me that there might be a problem at the heart of computer science as we currently formulate it: it isn't a realistic model of the world we're trying to compute over.

In some ways this is nether surprising nor threatening. Any mathematical or computational model is only a simplified abstraction of the real world, for which we have to make often staggeringly bold simplifications if we're to get anywhere. We should however always be prepared to challenge the &lt;em&gt;validity&lt;/em&gt; and &lt;em&gt;necessity&lt;/em&gt; of these simplifications, and that's what I'd like to do here.

As far as validity is concerned, the simplification is quite patently &lt;em&gt;invalid&lt;/em&gt; when it comes to any system that operates with real-world data: some of it is &lt;em&gt;bound&lt;/em&gt; to be "wrong" in some sense. This isn't the same as being tolerant of mistakes, such as when someone presses the delete key by mistake: that's a  action that certainly happened and to which the system responded correctly, albeit "wrongly" from the user's perspective. Interesting problem, but different: we're talking here about responding to inherently erroneous input -- the delete key seeming to press itself, if you like.

Necessity, then: is it necessary to handle computation in this way? Clearly not: we can easily conjecture a computational model that's more tolerant of input with limited certainty.

Consider precision first. If the input is only known to a limited precision, then we don't want that error margin to cause enormous errors. If we have a function $latex f$, then we want $latex f$ to exhibit a tolerance of imprecision such that $latex \delta x &amp;lt; tol_x \Rightarrow \left | f(x + \delta x) - f(x) \right | &amp;lt; s \left | \delta x \right|$ for some scaling factor $latex s &amp;lt; 1$. $latex f$ doesn't cause errors to blow-up in unpredictable ways. A lot of functions behave in exactly this way: for example, in a sliding-window average function $latex f_w(\overline{x}, x) = \frac{x + \overline{x}(w - 1)}{w}$ for an average $latex \overline{x}$ computed from $latex w$ recent observations, we have that $latex s = \frac{1}{w}$. Small errors therefore perturb the result significantly less than the input is perturbed. If the errors are uniformly distributed, the function should converge on the "true" value.

Conversely, a large, accurate new observation will perturb the average only slowly, so large step-changes will be detected only slowly. It's hard to distinguish such a change when it first happens from an inaccurate reading. There are various ways of dealing with this, such as using a weighted sliding window with non-linear weighting.

This is a rather topological idea. Rather than simply mapping points in an input space (such as temperature) to an output space (average temperature over the past few minutes), we're also requiring that the mapping take elements close in the input space to elements close in the result space: we require that it be a &lt;em&gt;contraction mapping&lt;/em&gt;. Building systems from contraction mappings, perhaps combined with contraction-preserving operators, yields systems that are robust to small errors in precision from their sensors.

Of course not all systems &lt;em&gt;are&lt;/em&gt; actually like this, and in many cases we &lt;em&gt;want&lt;/em&gt; rapid changes to be detected quickly and/or propagated. The point, perhaps, is that this is a &lt;em&gt;choice we should make&lt;/em&gt; rather than a &lt;em&gt;consequence&lt;/em&gt; of choosing a particular model of computation. There might actually be a model of computation lurking about here, in which we define functions coupled with a model of how their input and output errors should behave. At the very least, this yields systems in which we can predict the consequences of errors and imprecisions, which is a major challenge to deal with at present.&lt;/p&gt;</description><category>programming</category><category>sensor networks</category><category>topology</category><category>uncertainty</category><guid>https://simondobson.org/2010/02/26/216/</guid><pubDate>Fri, 26 Feb 2010 09:00:18 GMT</pubDate></item></channel></rss>
<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Simon Dobson (Posts about programming)</title><link>https://simondobson.org/</link><description></description><atom:link href="https://simondobson.org/categories/programming.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2023 &lt;a href="mailto:simoninireland@gmail.com"&gt;Simon Dobson&lt;/a&gt; </copyright><lastBuildDate>Thu, 18 May 2023 07:29:32 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Backporting Python type annotations</title><link>https://simondobson.org/2020/12/09/backporting-python-types/</link><dc:creator>Simon Dobson</dc:creator><description>&lt;div&gt;&lt;p&gt;I recently added type annotations to my
&lt;a href="https://simondobson.org/development/epyc/"&gt;&lt;code&gt;epyc&lt;/code&gt;&lt;/a&gt; and
&lt;a href="https://simondobson.org/development/epydemic/"&gt;&lt;code&gt;epydemic&lt;/code&gt;&lt;/a&gt; libraries. Making these work
while not sacrificing interoperability a wide range of Python versions
is quite delicate.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://simondobson.org/2020/12/09/backporting-python-types/"&gt;Read more…&lt;/a&gt; (5 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>epyc</category><category>epydemic</category><category>programming</category><category>python</category><guid>https://simondobson.org/2020/12/09/backporting-python-types/</guid><pubDate>Wed, 09 Dec 2020 10:45:43 GMT</pubDate></item><item><title>The type wheel turns again</title><link>https://simondobson.org/2012/10/05/typewheel/</link><dc:creator>Simon Dobson</dc:creator><description>&lt;p&gt;It's slightly spooky when you're discussing a topic and evidence for (or against) your position seems to spontaneously appear. The fashion for strong &lt;em&gt;versus&lt;/em&gt; weak type systems seems to change on a cycle of about a decade. It might be turning again.

&lt;!--more--&gt;

On Monday I was talking with a student who's doing a project using &lt;a href="http://nodejs.org/" target="_blank"&gt;node.js&lt;/a&gt;, looking at using it as the basis for doing elastic concurrent programs. It's the sort of project that could underpin some kinds of low-end cloud computing, letting designers use JavaScript end to end.

The discussion turned to type systems, and how Javascript's very weak view of types makes things more difficult for his project, as he will have to constantly protect against having the wrong methods called. On the other hand, it makes other things easier by letting him create proxies and other structures freely. The question is then whether strongly-typed languages are preferable to weakly-typed ones.

In a strongly-typed language, every denotable value has a type, the language ensures that all uses of those values are type-correct and generates a type error if not. A strongly statically-typed language does this purely at compile-time, and it's generally recognised by those who advocate these approaches that it's preferable to catch as many errors as possible as early as possible. It's also recognised that this isn't always possible (think Java class loaders), and so some run-time structures are also needed -- but these can be provided so as to catch problems as early as possible (when code is loaded). (See Dobson and Matthews. &lt;a href="http://www.simondobson.org/softcopy/ions-ecoop-2000.ps"&gt;Ionic types&lt;/a&gt;. In ECOOP 2000 – object-oriented programming, pages 296–312. Elisa Bertoni (ed). Volume 1850 of LNCS. Springer-Verlag. 2000.)

For some people, static typing feels too rigid: the compile will often prevent things that the programmer "knows" to be possible. In this case a looser type regime is often preferred. Strong dynamic typing checks at every operation to make sure that the values being manipulated are type-correct; weak dynamic typing does fewer checks, often only detecting problems very late; untyped or monotyped languages do few or no checks and will apply any operation to any piece of data at the programmer's instruction.

I tend to fall into the strong static typing camp -- which is slightly ironic, given that I'm currently working on &lt;a href="http://www.threaded-interpreter.org" target="_blank"&gt;untyped extensible virtual machines&lt;/a&gt;. Programmers' beliefs that they know better than the type-checker are often erroneous, the more so as code gets more complicated.

The fashion for type systems seems to follow a cycle. People are using a language with strong typing when a new kind of programming comes along, often driven by some new technology. The strong types are restrictive for this new domain (having been designed for a different world) so programmers invent or re-discover languages with dynamic typing that allow them to write the code they need to write without the difficulties of fighting a type system. In large-scale systems, programmers also like being able to evolve the data structures gradually, without having to update every user. (Doing so in the presence of strong types often doesn't work, although with care it's possible.) This leads to a widespread belief that type-checking is unnecessary, difficult, for losers, &lt;em&gt;etc&lt;/em&gt;, and that dynamic languages are the only way to go.

Then, as programs get bigger and more complicated, problems start to emerge. Typically these revolve around different parts of the system not agreeing on the exact data representation, so everyone has to check the data they receive because the language offers no guarantees that it'll be correct. (This is the down-side of being able to evolve the representation piecemeal.)  Such checks rapidly become untenable, and so programmers start thinking about whether there are automated mechanisms to improve checking -- and re-discover strong type systems.

Having been discussing this in the context of Javascript, I then stumbled across &lt;a href="http://www.typescriptlang.org" target="_blank"&gt;TypeScript&lt;/a&gt;, a Javascript extension that allows type annotations. These aren't exactly a strong type system -- they're optional, for a start -- but definitely mark a change in the way Javascript would be used, as a system with defined type structure rather than as a type free-for-all. Since Javascript occurs in a lot of systems these days -- on the front-ends, but also increasingly server-side -- this is a welcome departure. I find it hard to believe that large, long-lived component-based systems can be built in a dependable fashion using only a dynamic approach to typing. It relies too much on programmers' willingness and ability to check &lt;em&gt;everything&lt;/em&gt;, &lt;em&gt;every time&lt;/em&gt;.

Actually there &lt;em&gt;are&lt;/em&gt; strong arguments for the need for non-static run-time checks, most notably in distributed systems when you can't be sure the data you receive will be type-correct even if the compiler that generated the code thinks it is, since you generally don't have complete control over all the components and their evolutions. But this isn't an argument against strong typing in general: it still helps, even if there are small holes. Instead one perhaps needs to check types at the component boundaries so that, once admitted, you have confidence in their type-correctness. This in turn places demands on the transport protocols to be self-describing in terms of their payloads' types, and doing so supports other constructs (like type-driven computation) for free without sacrificing the benefits of the strong checks. Having some dynamism (and room for run-time failure) within a generally static seems like a decent compromise.&lt;/p&gt;</description><category>Blog</category><category>javascript</category><category>programming</category><category>type systems</category><guid>https://simondobson.org/2012/10/05/typewheel/</guid><pubDate>Fri, 05 Oct 2012 07:00:23 GMT</pubDate></item><item><title>Forth, 2^5 years ago</title><link>https://simondobson.org/2012/08/08/forth-32-years/</link><dc:creator>Simon Dobson</dc:creator><description>&lt;p&gt;2^5 years ago this month, &lt;em&gt;Byte&lt;/em&gt; magazine devoted an issue to the Forth language.

&lt;!--more--&gt;

Byte ("the small systems journal") volume 5 number 8, August 1980, was largely devoted to Forth. I only discovered this by accident, researching some background for a paper I'm writing on extensible virtual machines. What's even more remarkable is that you can &lt;a href="http://malus.exotica.org.uk/~buzz/byte/pdf/" target="_blank"&gt;download the issue -- along with a lot of others -- as a PDF&lt;/a&gt;.

That the premier hobbyist/hacker magazine of its day would give over most of an entire issue to one language tells you something about the way people thought about programming their machines back then. There was a premium on compactness: one of the first adverts in this issue of Byte is for a Cromenco Z-2H with 64Kb of RAM and 11Mb of hard disc, and proudly claiming that it  "is under $10K".

One article is a history of Forth, one is a tutorial, and two are deeply technical programming pieces aimed at people comfortable with the idea of writing their own software pretty much from scratch  -- and indeed, keen to get on with it. What's more, they could write software as good or better as that which they could buy (to the extent that there &lt;em&gt;was&lt;/em&gt; any hobbyist software to buy). That's not something we've been able to say for at least the last 2^4 years: hobbyist software hasn't competed with commercial offerings in most domains for a long time.

I think there were a number of things going on. The simplicity of the machines was obviously a bonus: one could understand the hardware and software of a personal computer in its entirety, and contemplate re-writing it from the ground up as an individual or small group.

Expectations were lower, but that works both ways: low expectations coupled with low-performance hardware can still lead to some impressive software. But it's certainly the case that one of the main barriers to software development from-the-ground-up these days is the need to interface with so many devices and processes in order to do anything of interest: any new system would need to talk to flash drives and the web, which probably means writing device drivers and a filing system. You can get round this using hardware packages, of course: Zigbee radios have simple programmer interfaces and encapsulate the software stack inside them.

Another factor, though, was a difference in ambition. A hobbyist in the 1980's only had herself and her friends to impress (and be impressed by): the horizon was closer. I'm sure that led to a lot of re-definition and duplication that the internet would allow one to avoid somewhat, but in some senses it provided a better learning environment in which a sequence of problems needed solution from a programmer's own creativity and resources. That's a significantly different skill set than what's required today, where we place a value on compliance, compatibility and re-use at least as high as we place on creativity and innovation.

I'm not advocating a return to the past -- although programming in Forth for sensor networks does give me a tremendous sense of pleasure that I haven't felt in programming for a long time, at least partially derived from the old-school nature of it all. However, I &lt;em&gt;would&lt;/em&gt; say that there's also value in this old-school style even today. The hackers who read Byte wouldn't settle for sub-standard tools: they wouldn't think twice about re-coding their compilers and operating systems (as well as their applications) if they were sub-standard. That power brings on a &lt;em&gt;sense&lt;/em&gt; of power -- the ability to change what's perceived to be wrong in something-- that's to be celebrated and encouraged even today, amongst programmers who sometimes seem to be constrained by their toolchains rather than freed by them.&lt;/p&gt;</description><category>Blog</category><category>forth</category><category>programming</category><guid>https://simondobson.org/2012/08/08/forth-32-years/</guid><pubDate>Wed, 08 Aug 2012 16:39:51 GMT</pubDate></item><item><title>Layered abstractions and Russian dolls</title><link>https://simondobson.org/2012/06/14/russian-dolls/</link><dc:creator>Simon Dobson</dc:creator><description>&lt;p&gt;The layering of abstractions has served us well, but it's now generating the sorts of complexity it was designed to solve. Time for a re-think?

&lt;!--more--&gt;Anyone who's built a large piece of software knows that much of the effort is in managing the complexity of the project: which other software a piece of code relies on, how to keep the various aspects separate, how to manage changes and upgrades, and so on. This isn't something that's got easier over time: it has for a given code size and style, as we've understood build processes and dependency management better; but the code sizes have relentlessly increased to compensate for our improved understanding; and modern practices don't make life any easier. Downloaded code, dynamic modules and classes, client-server and the like all generate their own intrinsic complexity.

One of the biggest sources of complexity is the use of multiple applications, especially in enterprise systems. A typical e-commerce system, for example, will make use of a web server to present pages (which themselves might contain embedded JavaScript for client-side processing), a database to track orders and inventory, a procurement system to fulfil orders, and possibly a supply-chain management system to order new inventory. That's the application. Then there'll be the operating system, a logging facility, a facilities management system, and a load of administrative tools and scripts. And the operating system may itself be virtualised and running as a guest within another, host operating system and hypervisor, which needs its own toolset. The interactions between these tools can be mind-boggling.

Someone once asked: who knows how to work the &lt;a href="http://httpd.apache.org/" target="_blank"&gt;Apache web server&lt;/a&gt;? It sounds like a simple question -- any competent web manager? the main developers? -- but the sting in the tail is that Apache is very configurable: so configurable, in fact, that it's pretty much impossible to work out what a given combination of options will do (or, conversely, what combination of options to use to achieve a given effect). The interactions are just too complicated, and the web abounds with examples where interactions between (for example) the thread pool size, the operating system block size, and the Java virtual machine parameters conspire to crash a system that looks like it should be working fine. If you can't work one server properly -- one component of the system -- what hope is there to get a complete system humming along?

&lt;a href="http://blogs.cs.st-andrews.ac.uk/al/" target="_blank"&gt;Al Dearle&lt;/a&gt; and I have been talking about this for a while. The basic issue seems to be an interaction between decomposition and dependency. In other words, the complexity comes at the "seams" between the various sub-systems, and is magnified the more configurable the components on either side of the seam are. This is important, because systems are becoming more finely decomposed: the move to component software, software-as-a-service and the like all increase the number of seams. Al's image of this is that modern systems are like Russian dolls, where each supposedly independent component contains more components that influence the size and complexity of the component containing them. You can only simplify any individual piece so far, because it depends on so many other pieces.

Actually a lot of the seams are now unnecessary anyway. Going back to the e-commerce example, the operating system goes to great pains to provide a process abstraction to keep the components separate -- to stop faults in the database affecting the web server, for example. Historically this made perfect sense and prevented a single faulty process in a time-sharing system affecting the processes of other users. Nowadays, however, it makes considerably less sense, for a number of reasons. Firstly, all the components are owned by a single nominal user (although there are still good reasons for separating the root user from the application user), so the security concerns are less pronounced. Secondly, all the components depend on each other, so a crash in the database will effectively terminate the web server anyway. (We're simplifying, but you get the idea.) Finally, there's a good chance that the web server, database and so on are each running in their own virtual machine, so there's only one "real" process per machine (plus all the supporting processes). The operating system is offering protection that isn't needed, because it's being provided (again) by the hypervisor running the virtual machines and perhaps (&lt;em&gt;again&lt;/em&gt;) by the host operating system(s) involved.

We also tend to build very flexible components (like Apache), which can deal with multiple simultaneous connections, keep users separate, allow modules to be loaded and unloaded dynamically -- behave like small operating systems, in other words, replicating the OS functionality again at application level. This is despite the fact that, in enterprise configurations, you'll probably know in advance the modules to be loaded and have a single user (or small user population) and fixed set of interactions: the flexibility makes the component more complex for no net gain during operation. Although it might simplify configuration and evolution slightly, there are often other mechanisms for this: in a cloud environment one can spin-up a replacement system in an evolved state and then swap the set of VMs over cleanly.

It's easy to think that this makes no difference for modern machines, but that's probably not the case. All these layers still need to be resourced; more importantly, they still need to be managed, maintained and secured, which take time to do well -- with a result that they typically get done badly (if at all).

Can we do anything about it? One thought is that the decomposition that makes thinking about systems and programming easier makes executing those systems more complex and fragile. In many cases, once the system is configured appropriately, flexibility becomes an enemy: it'll often be too complicated to re-configure or optimise in a live environment anyway. There may be a reason to have Russian dolls when &lt;em&gt;designing&lt;/em&gt; a system, but once designed it's better to make each doll solid to remove the possibility of then opening-up and falling apart.

So it's not decomposition that's the issue, it's &lt;em&gt;decomposition manifested at run-time&lt;/em&gt;. When we add new abstractions to systems, we typically add them in the form of components or libraries that can be called from other components. These components are often general, with lots of parameters and working with multiple clients -- sound familiar? This is all good for the component-writer, as it lets the same code be re-used: but it bloats each system that uses the component, adding complexity and interactions.

So one thought for tackling complexity is to change where decomposition manifests itself. If instead of placing new functions in the run-time system, we placed it into the compiler used to build the run-time, we could use compilation techniques to optimise-out the unnecessary functionality so that what results is optimised for the configuration that it's actually being placed in, rather than being general enough to represent any configuration. There's substantial work on these ideas in the fields of staged compilation and partial evaluation (for example &lt;a href="http://www.metaocaml.org/" target="_blank"&gt;MetaOCaml&lt;/a&gt;, &lt;a href="http://www.haskell.org/haskellwiki/Template_Haskell" target="_blank"&gt;Template Haskell&lt;/a&gt;, Flask and the like): the flexibility is manifested at compile-time as compile-time abstractions, that in the course of compilation are removed and replaced with inflexible -- but more efficient and potentially more dependable -- specialised code. Think taking the source code for Linux, Apache and MySQL, accelerating them together at high speed, and getting out a single program that'd run on a bare machine, had nothing it didn't actually need, and had all the options for the various (conceptual) sub-systems set correctly to work together.

Don't believe it's possible? Neither do I. There's too much code and especially too much legacy code for this to work at enterprise (or even desktop) level. However, for embedded systems and sensor networks it's a different story. For these systems, every extra abstraction that makes the programmer's life easier is a menace if it increases the code size hitting the metal: there just isn't the memory. But there also isn't the legacy code base, and there is a crying need for better abstractions. So an approach to the Russian dolls that moves the abstractions out of the run-time and&lt;a href="https://simondobson.org/2011/05/evolving/" target="_blank"&gt; into the languages and compilers&lt;/a&gt; might work, and might considerably improve the robustness and ease of use for many systems we need to develop. It also works well with modern language technology, and with other trends like &lt;a href="https://simondobson.org/2011/12/middleware-doughnut/" target="_blank"&gt;ever-more-specialised middleware&lt;/a&gt; that remove bloat and overhead at the cost of generality. Keeping the former &lt;em&gt;and&lt;/em&gt; the latter seems like a worthwhile goal.&lt;/p&gt;</description><category>Blog</category><category>compilers</category><category>programming</category><category>software engineering</category><category>virtualisation</category><guid>https://simondobson.org/2012/06/14/russian-dolls/</guid><pubDate>Thu, 14 Jun 2012 07:00:52 GMT</pubDate></item><item><title>The post-modernisation of programming languages</title><link>https://simondobson.org/2012/05/04/postmodernism-programming/</link><dc:creator>Simon Dobson</dc:creator><description>&lt;p&gt;Do we now have some post-modern programming languages?

&lt;!--more--&gt;

Programming languages change all the time. It's never been the case that one could learn just one and then use it: programming is too rich and complicated for that to be a solution for the majority of programmers. However, we seem to be seeing a change in what we think of as programming languages and how they're used to build larger software systems.

At the risk of gross over-simplification (and ridicule), let's divide languages into various "eras". In the beginning, the &lt;em&gt;early&lt;/em&gt; languages were built around some explicit notion of the machine on which they were running. Assemblers were devoted to a single machine code. Higher-level languages like Forth abstracted away from the machine code but still exposed the guts of the machine, and in particular its data bus width and memory access mechanisms.

The next step in abstraction -- to &lt;em&gt;pre-modernism&lt;/em&gt; -- was to provide a machine that was still recognisably a Von Neumann system but that hid the details of the underlying machine code and architecture. C and Pascal fall into this category: their operations and abstractions are still largely those of the physical machine, but at a sufficient remove to achieve portability and a usable programming model. But we still have a view of the machine's underlying limitations. &lt;em&gt;Modern&lt;/em&gt; languages like Perl, Java and Haskell offer a model of computation, and especially a model of memory, that's considerably removed from the underlying machine and enormously simplifies the programmer's task, at least for "suitable" programs.

(Interestingly enough languages don't fall into eras in the sequence of their design or popularity. Lisp considerably pre-dates C but is definitely modern using the terminology above.)

What's perhaps not obvious is that all these languages share a common design philosophy. They are all based on a small set of primitive notions combined with some ways of combining those notions, to get compound abstractions that resemble their primitive underpinnings. In C we get a small number of base types, some compound structs and unions, and pointers, and most interesting compound structures one builds will contain pointers all over the place. In Java we can build classes that create and combine objects; in Haskell we get lists and higher-order functions as the basic building blocks. The point is that the basic concepts are &lt;em&gt;general&lt;/em&gt;, as are the composition operators, and the larger capabilities are in some sense a &lt;em&gt;consequence&lt;/em&gt; of these initial choices and characterise the programs it's easy (or hard) to write.

This philosophy makes perfect sense, of course, which explains why it's so widespread. Programmers have to get to grips with a small set of basic concepts and composition methods. Everything else flows from this, and the extensions to the language -- libraries and the like -- will be the same as code one could write oneself: they're time-savers, but not fundamentally different from your own code. As a consequence the basic concepts have to be well-chosen to support the range of programs people will want to write. Some will be easier to write than others -- that's what makes the choice of language more than simply a matter of fashion -- but there's an acceptance that pretty much anything is writeable.&lt;em&gt;&lt;/em&gt;

However, we're now seeing languages emerge that aren't like this. Instead they're very much targeted at specific domains and problem sets. We've always had domain-specific languages (DSLs), of course, but the current crop seem to me to be rather different. They've abandoned the design philosophy of a small, basic set of abstractions and powerful composition, in favour of large basic abstractions and limited composition and extension. This significantly changes development, and the balance of power between the language designer and the programmer.

XSLT is the language that's brought this most to mind, and especially the transition from XSLT 1.0 to XSLT 2.0.

First we have to defend the notion that XSLT is a programming language. I first thought of it as just a transformation system for documents, and so it is: but it's targeted at such a general set of applications that it takes on the flavour of a DSL. It's essentially a functional language with pattern-matching and recursion, whose main data structure is the XML document. It lets one write programs that are driven by these documents: another way to look at it is that it gives executable semantics to a set of XML tags, allowing them to perform computation.

XSLT's design doesn't start with a small set of primitives. Instead it provides a collection of tags that can be used to perform the common document operations. If you want to number a list, for example, XSLT contains tags that support the common numbering formats: numbers, letters with brackets, and so forth. What?, you wanted something else? Well, you can &lt;em&gt;probably&lt;/em&gt; do it, but it'll be indescribably hard because you'll be writing a program in a language for which full computation is an afterthought. Instead, XSLT 2.0 adds &lt;em&gt;new&lt;/em&gt; numbering formats that you can select using a helpful set of tags, and this is done &lt;em&gt;within&lt;/em&gt; the XSLT processor, not &lt;em&gt;using&lt;/em&gt; the XSLT processor. Meaningful extension requires the intervention of the language designer.

This is a major philosophical change, a &lt;em&gt;post-modern&lt;/em&gt; approach to language design. Don't focus on getting the core abstractions right: instead, build a system -- &lt;em&gt;any&lt;/em&gt; system -- that demonstrates initial functionality, and then extend it with new features to meet new demands regardless of how they fit. There's no overarching conceptual scheme to the system. Instead it's judged purely on the functions it provides, regardless of how they fit together. (&lt;a href="http://www.wall.org/~larry/pm.html" target="_blank"&gt;Larry Wall described Perl as post-modern&lt;/a&gt;, but I don't think he meant quite what I'm getting at here -- although there are similarities.)

Post-modernism  works for three reasons. Firstly, no-one is expecting to write very large amounts of code in these systems. They're intended for &lt;em&gt;sub-&lt;/em&gt;systems, not for systems themselves, and so the need for generality is limited. Secondly, there's a premium on speed of development rather than on maintenance, which in turn puts a premium on getting some result out quickly rather than on ensuring that, in the future, we can get any result out we need. The sub-systems are intended to change on a short timescale, so maintenance and extensibility is perceived to be less of an issue than agility. Thirdly, a lot of these languages target people who aren't programmers -- or at least don't think of themselves as programmers -- who are focused on something other than the code.

Post-modernism isn't wrong, and &lt;a href="https://simondobson.org/2011/12/middleware-doughnut/" target="_blank"&gt;appears in middleware too&lt;/a&gt;. It's also important to realise the benefits: it makes it easier to put together larger systems, and increases &lt;a href="https://simondobson.org/2011/06/computing-experience/" target="_blank"&gt;the ambition of projects one can undertake&lt;/a&gt; compared to having to build so much from scratch, But it may be misguided. The people who spent the 1960's writing all the COBOL code that's still running today never thought it'd still be being maintained -- and the consequences of that lead to code that's now impossible to change. Simple solutions have a tendency to grow into more complex ones -- "just one more feature and we're done" -- which can push the costs of inappropriate choices out along the project lifecycle.

More importantly, for researchers post-modernism is a seductive siren call that lets you step away from tackling some difficult choices. Instead of picking a small set of concepts, choose &lt;em&gt;any set&lt;/em&gt; and then extend them, in &lt;em&gt;any way&lt;/em&gt;, to build up your system. I think this obscures some insights one might otherwise gain from simplifying your set of concepts.

I think it's also worth remembering that increasing the size and number of abstractions isn't without cost: not for the computer or the compiler, but for the programmer. The more programmers have to remember, and in particular the less well the things to be remembered fit together as a conceptual whole, the harder it is to use the system to its fullest advantage. People instead stay with small sub-sets of the language or -- worse -- settle for programs producing results that aren't those they want, just to stay within their language comfort zone. This sort of simplification is a step backwards, and we need to be careful of the consequences.&lt;/p&gt;</description><category>Blog</category><category>programming</category><category>xhtml</category><guid>https://simondobson.org/2012/05/04/postmodernism-programming/</guid><pubDate>Fri, 04 May 2012 12:17:13 GMT</pubDate></item><item><title>The semantic web: good ideas poorly supported?</title><link>https://simondobson.org/2012/02/02/semantic-web/</link><dc:creator>Simon Dobson</dc:creator><description>&lt;p&gt;The semantic web and open linked data open-up the vision of scientific data published in machine-readable form. But their adoption faces some challenges, many self-inflicted.

&lt;!--more--&gt;

Last week I taught &lt;a href="https://simondobson.org/teaching/ovronnaz/" target="_blank"&gt;a short course on programming context-aware systems&lt;/a&gt; as a Swiss doctoral winter school. The idea was to follow the development process from the ideas of context, through modelling and representation, to reasoning and maintenance.

Context has a some properties that make it challenging for software development. The data available tends to be heterogeneous, dynamic and richly linked. All these properties can impede the use of normal approaches like object-oriented design, which tend to favour systems that can be specified in a static object model up-front. Rather than use an approach that's highly structured from its inception, am alternative approach is to use an open, unstructured representation and then add structure later using auxiliary data.This leads naturally into the areas of &lt;a href="http://linkeddata.org" target="_blank"&gt;linked open data&lt;/a&gt; and the semantic web.

The &lt;a href="https://en.wikipedia.org/wiki/Semantic_Web" target="_blank"&gt;semantic web&lt;/a&gt; is a term coined by Tim Berners-Lee as a natural follow-on from his original web design. Web pages are great for browsing but don't typically lend themselves to automated processing. You may be able to extract my phone number from &lt;a href="https://simondobson.org/contact-details/" target="_blank"&gt;my contact details page&lt;/a&gt;, for example, but that's because you understand typography and abbreviation: there's nothing that explicitly marks the number out from the other characters on the page. Just as the web makes information readily accessible to people, the semantic web aims to make information equally accessible to machines. It does this by allowing web pages to be marked-up using a format that's more semantically rich than the usual HTML. This uses two additional technologies: the &lt;a href="http://www.w3.org/TR/2004/REC-rdf-primer-20040210/" target="_blank"&gt;Resource Description  Framework (RDF)&lt;/a&gt; to assert facts about objects, &lt;a href="http://www.w3.org/TR/rdf-sparql-query/" target="_blank"&gt;SPARQL&lt;/a&gt; to access to model using queries, and the &lt;a href="http://www.w3.org/TR/owl-guide/" target="_blank"&gt;Web Ontology  Language (OWL)&lt;/a&gt; to describe the structure of a particular domain of  discourse. Using the example above, RDF would mark-up the phone number, email address &lt;em&gt;etc&lt;/em&gt; explicitly, using terminology described in OWL to let a computer understand the relationships between, for example, a name, an email address, an employing institution and so on. Effectively the page, as well as conveying content for human consumption, can carry content marked-up semantically for machines to use autonomously. And of course you can also create pages that are &lt;em&gt;purely&lt;/em&gt; for machine-to-machine interaction, essentially treating the web as a storage and transfer mechanism with RDF and OWL as semantically enriched transport formats.

So far so good. But RDF, SPARQL and OWL are far from universally accepted "in the trade", for a number of quite good reasons.

The first is verbosity. RDF uses XML as an encoding, which is quite a verbose, textual format. Second is complexity: RDF makes extensive use of XML namespaces, which add structure and prevent misinterpretation but make pages harder to create and parse. Third is the exchange overhead, whereby data has to be converted from in-memory form that programs work with into RDF for exchange and then back again at the other end, each step adding &lt;em&gt;more&lt;/em&gt; complexity and risks of error. Fourth is the unfamiliarity of many of the concepts, such as the dynamic non-orthogonal classification used in OWL rather than the static class hierarchies of common object-oriented approaches. Fifth is the disconnect between program data and model, with SPARQL sitting off to one side like SQL. Finally there is the need for all these technologies &lt;em&gt;en masse&lt;/em&gt; (in addition to understanding HTTP, XML and XML Schemata) to perform even quite simple tasks, leading to a steep learning curve and a high degree of commitment in a project ahead of any obvious returns.

So the decision to use the semantic web isn't without pain, and one needs to place sufficient value on its advantages -- open, standards-based representation, easy exchange and integration -- to make it worthwhile. It's undoubtedly attractive to be able to define a structure for knowledge that exactly matches a chosen sub-domain, to describe the richness of this structure, and to have it compose more or less cleanly with &lt;em&gt;other&lt;/em&gt; such descriptions of complementary sub-domains defined independently -- and to be able to exchange all this knowledge with anyone on the web. But this flexibility comes with a cost and (often) no obvious immediate, high-value benefits.

Having taught this stuff, I think the essential problem is one of tooling and integration, not core behaviour. The semantic web does include some really valuable concepts, but their realisation is currently poor and this poses a hazard to their adoption.

In many ways the use of XML is a red herring: no sane person holds data to be used programmatically as XML. It is -- and was always intended to be -- an exchange format, not a data structure. So the focus needs to be on the data model underlying RDF (subject-predicate-object triples with subjects and predicates represented using URIs) rather than on the use of XML.

While there are standard libraries and tools for use with the semantic web -- in Java these include &lt;a href="http://incubator.apache.org/jena/" target="_blank"&gt;Jena&lt;/a&gt; for representing models, &lt;a href="http://clarkparsia.com/pellet/" target="_blank"&gt;Pellet&lt;/a&gt; and other reasoners providing ontological reasoning, and &lt;a href="http://protege.stanford.edu/" target="_blank"&gt;Protégé&lt;/a&gt; for ontology development -- their level of abstraction and integration with the rest of the language remain quite shallow. It is hard to ensure the validity of an RDF graph against an ontology, for example, and even harder to validate updates. The type systems also don't match, either statically or dynamically: OWL performs classification based on attributes rather than by defining hard classes, and the classification may change unpredictably as attributes are changed. (This isn't just a problem for statically-typed programming languages, incidentally: having the objects you're working with re-classified can invalidate the operations you're performing at a semantic level, regardless of whether the type system complains.) The separation of querying and reasoning from representation is awkward, rather like the use of SQL embedded into programs: the query doesn't fit naturally into the host language, which typically has no syntactic support for constructing queries.

Perhaps the solution is to step back and ask: what problem does the semantic web solve? In essence it addresses the open and scalable mark-up of data across the web according to semantically meaningful schemata. &lt;em&gt;But programming languages don't do this&lt;/em&gt;: they're about nailing-down data structures, performing local operations efficiently, and letting developers share code and functionality. So there's a mis-match between the goals  of the two system components, and their strengths don't complement each other in the way one might like.

This suggests that we re-visit the integration of RDF, OWL and SPARQL into programming languages; or, alternatively, that we look at for what features would provide the best computational capabilities alongside these technologies. A few characteristics spring to mind:
&lt;/p&gt;&lt;ul&gt;
    &lt;li&gt;Use classification throughout, a more dynamic type structure than classical type systems&lt;/li&gt;
    &lt;li&gt;Access RDF data "native", using binding alongside querying&lt;/li&gt;
    &lt;li&gt;Adopt the XML Schemata types "native" as well&lt;/li&gt;
    &lt;li&gt;Make code polymorphic in the manner of OWL ontologies, so that code can be exchanged and re-used. This implies basing typing on reasoning rather than being purely structural&lt;/li&gt;
    &lt;li&gt;Hiding namespaces, URIs and the other elements of RDF and OWL behind more familiar (and less intrusive) syntax (while keeping the semantics)&lt;/li&gt;
    &lt;li&gt;Allow programmatic data structures, suited to local use in a program, to be layered onto the data graph without forcing the graph itself into convoluted structures&lt;/li&gt;
    &lt;li&gt;Thinking about the global, non-local data structuring issues&lt;/li&gt;
    &lt;li&gt;Make access to web data intrinsic, not something that's done outside the normal flow of control and syntax&lt;/li&gt;
&lt;/ul&gt;
The challenges here are quite profound, not least from relatively pedestrian matters like concurrency control, but at least we would then be able to leverage the investment in data mark-up and exchange to obtain some of the benefits the semantic web clearly offers.</description><category>Blog</category><category>context</category><category>owl</category><category>programming</category><category>rdf</category><category>semantics</category><category>sparql</category><guid>https://simondobson.org/2012/02/02/semantic-web/</guid><pubDate>Thu, 02 Feb 2012 08:00:06 GMT</pubDate></item><item><title>The changing student computing experience</title><link>https://simondobson.org/2011/06/27/computing-experience/</link><dc:creator>Simon Dobson</dc:creator><description>&lt;p&gt;I'm jealous of my students in many ways, for the things they'll get to build and experience. But they should be jealous of me, too.

&lt;!--more--&gt;

It was graduation week last week, which is always a great bash. The students enjoy it, obviously -- but it's mainly an event for the parents, many of whom are seeing their first child, or even the first child in their extended family, succeed at university. It's also the time of year when we conduct &lt;em&gt;post mortem&lt;/em&gt; analyses of what and how we've taught throughout the year, and how we can change it for the better in the next session.

One of the modules I teach is for second-year undergraduates on data structures, algorithms, applied complexity and other really basic topics. It's a subject that's in serious danger of being as dry as ditchwater: it's also extremely important, not only because of it's applications across computer science but also because it's one of the first experiences the students have of the subject, so it's important that it conveys the opportunities and excitement of computer science so they don't accidentally nuke their lives by going off to do physics or maths instead.

One of the aspects of making a subject engaging is appealing to the students' backgrounds and future interests -- which of course are rather different to the ones I had when I was in their position 25 years ago. (I initially wrote "quarter of a century ago," which sounds &lt;em&gt;way&lt;/em&gt; longer somehow.) So what are the experiences and aspirations of our current students?

Many get their first experience of programming computers with us, but they're all experienced computer users who've been exposed to computers and the internet their entire lives. They're the first generation for whom this is true, and I don't think we've really assimilated what it means. They're completely at home, for example, in looking up background material on Wikipedia, or surfing for alternative sources of lectures and tutoring on YouTube and other, more specialised sites. They can do this while simultaneously writing email, using Facebook and replying to instant messages in a way that most older people can't. They're used to sharing large parts of themselves with their friends and with the world, and it's a world in which popularity can sometimes equate with expertise in unexpected ways. It's hard to argue that this diversity of experience is a bad thing, and &lt;a href="https://simondobson.org/2010/06/smarter-internet/" target="_blank"&gt;I completely disagree with those whom have done so&lt;/a&gt;: more information on more topics collected from more people can only be positive in terms of exposure to ideas. For an academic, though, this means that we have to change how and what we teach: the facts are readily available, but the &lt;em&gt;interpretation&lt;/em&gt; and &lt;em&gt;criticism&lt;/em&gt; of those facts, and the &lt;em&gt;balancing of issues&lt;/em&gt; in complex systems, are something that still seem to benefit from a lecture or tutorial setting.

Many of the students have also built web sites, of course -- some very complex ones. Put another way, they've built distributed information systems by 17, and in doing so have unknowingly made use of techniques that were at cutting edge of research less than 17 years ago. They &lt;em&gt;expect&lt;/em&gt; sites to be slick, to have decent graphics and navigation, to be linked into social media, and so forth. They've seen the speed at which new ideas can be assimilated and deployed, and the value that information gains when it's linked, tagged and commented upon by a crowd of people. Moreover they &lt;em&gt;expect this to continue&lt;/em&gt;: none of them expects the web to fragment into isolated "gated communities" (which is a fear amongst some commentators), or to become anything other than more and more usable and connected with time.

I'm jealous of my students, for the web that they'll have and the web that many of them will help to build. But before I get too envious, it's as well to point out that they should be jealous of me too: of the experience my peers and I had of computers. It's not been without impact.

One of the things that surprises me among some students is that they find it hard to imagine ever building some of the underpinning software that they use. They can't really imagine building an operating system, for example, even though they know intellectually that Linux was built, and is maintained, by a web-based collaboration. They can't imagine building the compilers, web servers and other base technology -- even though they're happy to use and build upon them in ways that really surprise me.

I suspect the reasons for this are actually embedded into their experience. All the computers, web sites and other services they've used have always had a certain degree of &lt;em&gt;completeness&lt;/em&gt; about them. That's not to say they were any good necessarily, but they were at least functional and usable to some degree, and targeted in general at a consumer population who expected these degrees of functionality and usability (and more). This is radically different to the experience we had of unpacking a &lt;a href="http://oldcomputers.net/zx80.html" target="_blank"&gt;ZX-80&lt;/a&gt;, &lt;a href="http://www.old-computers.com/MUSEUM/computer.asp?c=80" target="_blank"&gt;Acorn Atom&lt;/a&gt; or some other 1980's vintage home computer, which didn't really &lt;em&gt;do&lt;/em&gt; anything -- &lt;em&gt;unless we made it do it ourselves&lt;/em&gt;. These machines were largely blank slates as far as their functions were concerned, and you had to become a
programmer to make them worth buying. Current games consoles criminalise these same activities: you need permission to program them.

It's not just a commercial change. A modern system is immensely complex and involves a whole stack of software just to make it function. It's hard to imagine that you can actually take control all the way down. In fact it's worse than that: it's hard to see why you'd &lt;em&gt;want&lt;/em&gt; to, given that you'd have to re-invent so much to get back to the level of functionality you expect to have in your devices. &lt;a href="https://simondobson.org/2011/05/smalltalk/" target="_blank"&gt;As with programming languages&lt;/a&gt;, the level of completeness in modern systems is a severe deterrent to envisioning them, and re-building them, in ways other than they are.

Innovation, for our students, is something that happens &lt;em&gt;on top of&lt;/em&gt; a large stack of previous innovation that's just accepted and left untouched. And this barrier -- as much mental as technological -- is the key difference between their experience of computing and mine. I grew up with computers that could be -- and indeed had to be -- understood from the bare metal up. One could rebuild &lt;em&gt;all&lt;/em&gt; the software in a way that'd be immensely more challenging now, given the level of function we've come to expect.

This is far more of a difference than simply an additional couple of decades of experience with technology and research: it sits at the heart of where the next generation will see the value of their efforts, and of where they can change the world: in services that sit at the top of the value chain, rather than in the plumbing down at its base. Once we understand that, it becomes clearer what and how we should teach the skills they'll need in order best to apply themselves to the challenges they'll select as worth their time. And I'll look forward to seeing what these result in.

Congratulations to the graduating class of 2011. Have great lives, and build great things.&lt;/p&gt;</description><category>Blog</category><category>programming</category><category>university</category><guid>https://simondobson.org/2011/06/27/computing-experience/</guid><pubDate>Mon, 27 Jun 2011 15:00:55 GMT</pubDate></item><item><title>Mainstreaming Smalltalk</title><link>https://simondobson.org/2011/05/27/smalltalk/</link><dc:creator>Simon Dobson</dc:creator><description>&lt;div id="heystaks_preview" style="width: 100%;height: 100%"&gt;&lt;/div&gt;
Smalltalk's influence has declined of late, at least in part because of the  "all or nothing" architecture of the most influential distribution. We've got to the stage that we could change that.

&lt;!--more--&gt;

Some programming languages achieve epic influence without necessarily achieving universal adoption. Lisp was phenomenally influential, but has remained in an AI niche; &lt;a href="https://secure.wikimedia.org/wikipedia/en/wiki/APL_%28programming_language%29" target="_blank"&gt;APL&lt;/a&gt; gave rise to notions of bulk processing and hidden parallelism without having much uptake outside finance. Smalltalk's influence has been similar: it re-defined what it meant to be an interactive programming environment and laid the ground for many modern interface concepts, as well as giving a boost to object-oriented programming that prepared the way for C++ and Java.

So why does no-one use it any more? Partly it's because of its very interactivity. Smalltalk -- and more especially &lt;a href="http://www.squeak.org" target="_blank"&gt;Squeak&lt;/a&gt;, its most prominent modern implementation -- are unusual in basing software around  complete interactive images rather than collections of source files. This isn't inherently poor -- images start immediately and encourage direct-manipulation design and interfacing -- but it's radically unfamiliar to programmers used to source code and version control. (Squeak provides version-controlled change sets to collect code outside an image, but that's still an unfamiliar structure.)

But a more important issue is the all-or-nothing nature of Squeak in particular, and in fact of novel languages in general. If you want to use Squeak for a project, all the components really need to be in Squeak (although it can also use web services and other component techniques). If you want to integrate web pages, someone needs to  write an HTML rendering component, HTTP back-end and the like; for the semantic web someone needs to write XML parsers, triple stores, reasoners and the like. You can't just re-use the ones that're already out there, at least not without violating the spirit of the system somewhat. That means it plays well at the periphery with other tools, but at its core need most of the services to be written again. This is a huge buy-in. Similarly Squeak provides a great user interface for direct manipulation -- but only within its own Squeak window, rendered differently and separately from other windows on the screen. These aren't issues inherent to Smalltalk -- it's perfectly possible to imagine a Smalltalk system that used "standard" rendering (and indeed they exist) -- but the "feel" of the language is rather more isolated than is common for modern systems used to integrating C libraries and the like. At bottom it's not necessarily all that different to Java's integration with the host operating system, but the style is very much more towards separation in pursuit of uniformity and expressive power. This is a shame, because Smalltalk is in many ways a perfect development environment for novice programmers (and especially for children, who find it captivating) who are a vast source of programming innovation for small, focused services such as we find on the web ad on smartphones.

So can we make Smalltalk more mainstream? Turn it into an attractive development platform for web services and mobile apps? Looking at some recent developments I think the answer is firmly &lt;em&gt;yes&lt;/em&gt; -- and without giving up on the interactivity that gives it its attraction. The key change is (unsurprisingly) the web, or more precisely the current crop of browsers that support Javascript, style sheets, SVG, dynamic HTML and the like. The browser has now arrived at a point at which it can provide a complete GUI -- complete with windows, moving and animated elements and the like -- in a standard, platform-independent and (most importantly) cloud-friendly way.

What I have in mind is essentially implementing a VM for a graphical Smalltalk system, complete with interactive compiler and direct-manipulation editing, in Javascript within a browser. The "image" is then the underlying XML document and its style sheet, which can be downloaded, shared and manipulated directly. The primitives are written in Javascript, together with an engine to run Smalltalk code and objects. Objects are persisted by turning them into document elements and storing them in the DOM tree, which incidentally allows their look and feel to be customised quite simply. Crucially, it can also appeal to any current or emerging features that can appear in style sheets, the semantic web,  Javascript or embedded components: it's mainstream with respect to the other technologies being developed.

Why use Smalltalk, and not Javascript directly? I simply think that the understanding we gained from Smalltalk's simplicity of programming model and embrace of direct manipulation is too valuable to lose. That's not to say that it doesn't need to be re-imagined for the web world, though. In fact, Smalltalk's simplicity and interactivity are ideally suited to the development of front-ends, components and mobile apps -- &lt;em&gt;if&lt;/em&gt; they play well with the &lt;em&gt;other&lt;/em&gt; technologies those front-ends and apps need to use, and with a suitably low barrier to entry. It's undoubtedly attractive to be able to combine local and remote components together as end-user programs, without the hassle of a traditional compile cycle, and then be able to share those mash-ups directly to the web to be shared (and, if desired, modified) by anyone.

One of the things that's always attracted me about Smalltalk (and Forth, and Scheme -- and Javascript to a lesser extent) is that the code lives completely within  the dominant data structure: indeed, the code &lt;em&gt;is&lt;/em&gt; just data in most cases, and can be manipulated using data-structure operations. This is very different from the separation you get between code and data in most other languages, and gives you a huge amount of expressive power. Conversely, one of the thing that always &lt;em&gt;fails&lt;/em&gt; to attract me about these &lt;em&gt; &lt;/em&gt;same languages is their lack of any static typing and consequent dependence on testing. Perhaps these two aspects necessarily go hand in hand, although I can't think of an obvious reason why that should be.

I know purists will scream at this idea, but to me it seems to go along with ideas that Smalltalk's co-inventor, Alan Kay, has expressed, especially with regard to the need to do away with closely-packaged applications and move towards a more fluid style of software:
&lt;blockquote&gt;The "no applications" idea first surfaced for me at PARC, when we realised that you really wanted to freely construct arbitrary combinations (and could do just that with (mostly media) objects). So, instead of going to a place that has specialised tools for doing  just a few things, the idea was to be in an "open studio" and &lt;em&gt;pull&lt;/em&gt; the resources you wanted to combine to you. This doesn't mean to say  that &lt;em&gt;e.g.&lt;/em&gt; a text object shouldn't be pretty capable -- so it's a mini app if you will -- but that it and all the other objects that intermingle with each other should have very similar UIs and have their graphics aspect be essentially totally similar as far as the graphics system is concerned -- and this goes also for user constructed objects. The media presentations I do in Squeak for my talks are good examples of the directions this should be taken for the future.&lt;/blockquote&gt;
(Anyone who has seen one of Kay's talks -- as I did at the ECOOP conference in 2000 -- can attest to how stunningly engaging they are.) To which I would add that it's equally important today that their &lt;em&gt;data&lt;/em&gt; work together seamlessly too, and with the other tools that we'll develop along the way.

The use of the browser as a desktop isn't new, of course: it's central to &lt;a href="http://www.chromium.org/chromium-os" target="_blank"&gt;Google Chromium&lt;/a&gt; and to &lt;a href="https://simondobson.org/2011/03/jolicloud/" target="_blank"&gt;cloud-friendly Linux variants like Jolicloud&lt;/a&gt;. But it hasn't really been used so much as a development environment, or as the host for a language that lives inside the web's main document data structure. I'm not hung-up on it being Smalltalk -- a direct-manipulation front-end to &lt;a href="http://jqueryui.com/" target="_blank"&gt;jQuery UI&lt;/a&gt; might be even better -- but some form of highly interactive programming-in-the-web might be interesting to try.</description><category>Blog</category><category>javascript</category><category>programming</category><category>smalltalk</category><guid>https://simondobson.org/2011/05/27/smalltalk/</guid><pubDate>Fri, 27 May 2011 07:00:29 GMT</pubDate></item><item><title>Why we have code</title><link>https://simondobson.org/2011/05/23/code/</link><dc:creator>Simon Dobson</dc:creator><description>&lt;div id="heystaks_preview" style="width: 100%;height: 100%"&gt;&lt;/div&gt;
Coding is an under-rated skill, even for non-programmers.
&lt;!--more--&gt;

Computer science undergraduates spend a lot of time learning to program. While one can argue convincingly that &lt;a href="https://simondobson.org/2010/05/cs-book-worth-reading-twice/" target="_blank"&gt;computer science is about more than programming&lt;/a&gt;, it's without doubt a central pillar of the craft: one can't reasonably claim to be a computer scientist without demonstrating a strong ability to work with code. (What this says about the many senior computer science academics who can no longer program effectively is another story.) The reason is that it helps one to think about process, and some of the best illustrations of that comes from teaching.

Firstly, why is code important? One can argue that both programming languages and the discipline of code itself are two of the main contributions computer science has made to knowledge. (To this list I would add the fine structuring of big data and the improved understanding of human modes of interaction -- the former is about programming, the latter an area in which the programming structures are still very weak.) They're so important because they force an understanding of a process at its most basic level.

When you write computer software you're effectively explaining a process to a computer in perfect detail. You often get a choice about the level of abstraction you choose. You can exploit the low-level details of the machine using assembler or C, or use the power of the machine to handle the low-level details and write in Haskell, Perl, or some other high-level language. But this doesn't alter the need to express precisely all that the machine needs to know to complete the task at hand.

But that's not all. Most software is intended to be used by someone other than the programmer, and generally will be written or maintained in part by more than one person -- either directly as part of the programming team or indirectly through the use of third-party compilers and libraries. This implies that, as well as explaining a purpose to the computer, the code also has to explain a purpose to other programmers.

So code, and programming languages more generally, are about &lt;em&gt;communication&lt;/em&gt; -- from humans to machines, and to other humans. More importantly, code is the communication of process reduced to its  purest form: there is &lt;em&gt;no clearer way&lt;/em&gt; to describe the way a process works than to read well-written, properly-abstracted code. I sometimes think (rather tenuously, I admit) this is an unexpected consequence of the &lt;a href="https://secure.wikimedia.org/wikipedia/en/wiki/Halting_problem" target="_blank"&gt;halting problem&lt;/a&gt;, which essentially says that the simplest (and generally only) way to decide what a program does is to run it. The simplest way to understand a process is to express it as close to executable form as possible.
&lt;blockquote&gt;You think you know when you learn, are more sure when you can write,
even more when you can teach, but certain only when you can program.

Alan Perlis&lt;/blockquote&gt;
There are caveats here, of course, the most important of which is that the code be well-written and properly abstracted: it needs to separate-out the details so that there's a clear process description that calls into -- but is separate from -- the details of exactly what each stage of the process does. Code that doesn't do this, for whatever reason, obfuscates rather than explains. A good programming education will aim to impart this skill of separation of concerns, and moreover will do so in a way that's independent of the language being used.

Once you adopt this perspective, certain things that are otherwise slightly confusing become clear. Why do programmers always find documentation so awful? Because the code is a clearer explanation of what's going on, because it's a fundamentally &lt;em&gt;better&lt;/em&gt; description of process than natural language.

This comes through clearly when marking student assessments and exams. When faced with a question of the form "explain this algorithm", some students try to explain it in words without reference to code, because they think explanation requires text. As indeed it does, but a better approach is to sketch the algorithm as code or pseudo-code and then explain with reference to that code -- because the code is the clearest description it's possible to have, and any explanation is just clearing up the details.

Some of the other consequences of the discipline of programming are slightly more surprising. Every few years some computer science academic will look at the messy, unstructured, ill-defined rules that govern the processes of a university -- especially those around module choices and student assessment -- and decide that they will be immensely improved by being written in Haskell/Prolog/Perl/whatever. Often they'll actually go to the trouble of writing some or all of the rules in their code of choice, discover inconsistencies and ambiguities, and proclaim that the rules need to be re-written. It never works out, not least because the typical university administrator has not the slightest clue what's being proposed or why, but also because the process always highlights grey areas and boundary cases that can't be codified. This could be seen as a failure, but can also be regarded as a success: coding successfully distinguishes between those parts of an organisation that are structured and those parts that require human judgement, and by doing so makes clear the limits of individual intervention and authority in the processes.

The important point is that, by thinking about a non-programming problem within a programming idiom, you clarify and simplify the problem and deepen your understanding of it.

So programming has an impact not only on computers, but on everything to which one can bring a description of process; or, put another way, once you can precisely describe processes easily and precisely you're free to spend more time on the motivations and cultural factors that surround those processes without them dominating your thinking. Programmers think differently to other people, and often in a good way that should be encouraged and explored.</description><category>Blog</category><category>programming</category><category>university</category><guid>https://simondobson.org/2011/05/23/code/</guid><pubDate>Mon, 23 May 2011 07:00:59 GMT</pubDate></item><item><title>Evolving programming languages</title><link>https://simondobson.org/2011/05/20/evolving/</link><dc:creator>Simon Dobson</dc:creator><description>&lt;div id="heystaks_preview" style="width: 100%;height: 100%"&gt;&lt;/div&gt;
Most programming languages have fixed definitions and hard boundaries. In thinking about building software for domains we don't understand very well, a case can be made for a more relaxed, evolutionary approach to language design.

&lt;!--more--&gt;

I've been thinking a lot about languages this week, for various reasons: mainly about the recurring theme of what are the right programming structures for systems driven by sensors, whether they're pervasive systems or sensor networks. In either case, the structures we've evolved for dealing with desktop and server systems don't feel like they're the right abstractions to effectively take things forward.

A favourite example is the &lt;tt&gt;if&lt;/tt&gt; statement: first decide whether a condition is true or false, and execute one piece of code or another depending on which it is. In a sensor-driven system we often can't make this determination cleanly because of noise and uncertainty -- and if we can, it's often only probably true, and only for a particular period. So are &lt;tt&gt;if&lt;/tt&gt; statements (and &lt;tt&gt;while&lt;/tt&gt; loops and the like) actually appropriate constructs, when we can't make the decisions definitively?

Whatever you think of this example (and plenty of people hate it) there are certainly differences between what we want to do between traditional and highly sensorised systems, and consequently how we program them. The question is, how do we work out what the right structures are?

Actually, the question is broader than this. It should be: how do we improve our ability to develop languages that match the needs of particular computational and conceptual domains?

Domain-specific languages (DSLs) have a tangled history in computer science, pitched between those who like the idea and those who prefer their programming languages general-purpose and re-usable across a &lt;em&gt;range&lt;/em&gt; of domains. There are strong arguments on both sides: general-purpose languages are more productive to learn and are often more mature, but can be less efficient and more cumbersome to apply; DSLs mean learning &lt;em&gt;another&lt;/em&gt; language that may not last long and will probably have far weaker support, but can be enormously more productive and well-targeted in use.

In some ways, though, the similarities between traditional languages and DSLs are very strong. As a general rule both will have syntax and semantics defined up-front: they won't be experimental in the sense of allowing experimentation &lt;em&gt;within the language itself&lt;/em&gt;. If we don't know what we're building, does it make sense to be this definite?

There are alternatives. One that I'm quite keen on is the idea of &lt;a href="https://simondobson.org/2010/05/languages-extensible-vms/" target="_blank"&gt;extensible virtual machines&lt;/a&gt;, where the primitives of a language are left "soft" to be extended as required. This style has several advantages. Firstly, it encourages experimentation by not forcing a strong division of concepts between the language we write (the target language) and the language this is implemented in (the host language): the two co-evolve. Secondly, it allows extensions to be as efficient as "base" elements, assuming we can reduce the cost of building new elements appropriately low. Thirdly, it allows multiple paradigms and approaches to co-exist within the same system, since they can share some elements while having other that differ.

Another related feature is the ability to modify the compiler: that is, don't fix the syntax &lt;em&gt;or&lt;/em&gt; the way in which its handled. So as well as making the low level soft, we also make the high level soft. The advantage here is two-fold. Firstly, we can modify the forms of expression we allow to capture concepts precisely. A good example would be the ability to add concurrency control to a language: the low-level might use semaphores, but programing might demand monitors or some other construct. Modifying the high-level form of the language allows these constructs to be added if required -- and ignored if not.

This actually leads to the  second advantage, that we can &lt;em&gt;avoid&lt;/em&gt; features we don't want to be available, for example not providing general recursion for languages that need to complete all operations in a finite time. This is something that's surprisingly uncommon in language design despite being common in teaching programming: leaving stuff out can have a major simplifying effect.

Some people argue that syntax modification is unnecessary in a language that's sufficiently expressive, for example Haskell. I don't agree. The counter-example is actually in Haskell itself, in the form of the &lt;tt&gt;do&lt;/tt&gt; block syntactic sugar for simplifying monadic computations. This &lt;em&gt;had&lt;/em&gt; to be in the language to make it in any way usable, which implied a change of definition, and the monad designers couldn't add it without the involvement of the language "owners", even though the construct is really just a &lt;a href="https://simondobson.org/2010/06/monads-language-design-perspective/" target="_blank"&gt;re-formulation and generalisation of one common in other languages&lt;/a&gt;. There are certainly other areas in which such sugaring would be desirable to make the forms of expression simpler and more intuitive. The less well we understand a domain, the more likely this is to happen.

Perhaps surprisingly, there are a couple of existing examples of systems that do pretty much what I'm suggesting. Forth is a canonical example (which explains my current work on &lt;a href="http://www.threaded-interpreter.org" target="_blank"&gt;Attila&lt;/a&gt;); Smalltalk is another, where the parser an run-time are almost completely exposed, although abstracted behind several layers of higher-level structure. Both the languages are quite old, have devoted followings, and weakly and dynamically typed -- and may have a lot to teach us about how to develop languages for new domains. They share a design philosophy of allowing a language to &lt;em&gt;evolve&lt;/em&gt; to meet new applications. In Forth, you don't so much write applications as extend the language to meet the problem; in Smalltalk you develop a model of co-operating objects that provide   direct-manipulation interaction through the GUI.

In both cases the whole language, including the definition and control structures, is built in the language itself &lt;em&gt;via&lt;/em&gt; bootstrapping and cross-compilation. Both languages are compiled, but in both cases the separation between run-time and compile-time are weak, in the sense that the compiler is by default available interactively. Interestingly this doesn't stop you building "normal" compiled applications: cross-compile a system without including the compiler itself, a process that can still take advantage of any extensions added into the compiler without cluttering-up the compiled code. You're unlikely to get strictly the best performance or memory footprint as you might with a mature C compiler, but you &lt;em&gt;do&lt;/em&gt; get advantages in terms of expressiveness and experimentation which seem to outweigh these in a domain we don't understand well. In particular, it means you can evolve the language quickly, easily, and within itself, to explore the design space more effectively and find out whether your wackier ideas are actually worth pursuing further.</description><category>Blog</category><category>forth</category><category>haskell</category><category>programming</category><category>sensor networks</category><category>smalltalk</category><guid>https://simondobson.org/2011/05/20/evolving/</guid><pubDate>Fri, 20 May 2011 07:00:11 GMT</pubDate></item></channel></rss>
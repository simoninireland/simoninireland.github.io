<!DOCTYPE html>
<html prefix="
	og: http://ogp.me/ns# article: http://ogp.me/ns/article#
    " vocab="http://ogp.me/ns" lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Posts about programming (old posts, page 1) | Simon Dobson</title>
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.0.13/css/all.css" integrity="sha384-DNOHZ68U8hZfKXOrtjWvjxusGo9WQnrNx2sqG0tfsghAvtVlRW3tvkXWZh58N9jp" crossorigin="anonymous">
<link rel="stylesheet" href="assets/css/normalize.css">
<link rel="stylesheet" href="assets/css/main.css">
<link href="../../assets/css/baguetteBox.min.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/rst_base.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/nikola_rst.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/code.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/fonts.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/theme.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" hreflang="en" href="../../rss.xml">
<link rel="canonical" href="https://simondobson.org/categories/programming/index-1.html">
<link rel="prev" href="index-2.html" type="text/html">
<!--[if lt IE 9]><script src="../../assets/js/html5shiv-printshiv.min.js"></script><![endif]--><!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=UA-10943215-1"></script><script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-10943215-1');
</script><link rel="alternate" type="application/rss+xml" title="RSS for tag programming" hreflang="en" href="../programming.xml">
</head>
<body>
  <a href="#page-content" class="sr-only sr-only-focusable">
    Skip to main content
  </a>

  

  <section id="header"><a href="../../">
      <div class="logo">
	  <div id="nologoimage">
	    Simon Dobson
	  </div>
      </div>
    </a>
  </section><section id="socialNav"><!-- Navigation Menu - on top on small screens, down the left on larger --><div class="navlinks">
<!--
      <a href="/">
	<div id="titleauth">
	    by Simon Dobson
	</div>
      </a>
-->

      <!-- Navigation links (hidden by default) -->
      <div id="navmenuitems">
	      <a href="../../index.html" title="Home">
	<span class="menuitemtext">Home</span>
	<span class="menuitemicon"><i class="fa fa-home"></i></span>
      </a>
      <a href="../../personal/" title="About me">
	<span class="menuitemtext">About me</span>
	<span class="menuitemicon"><i class="fa fa-user"></i></span>
      </a>
      <a href="../../research/" title="Research">
	<span class="menuitemtext">Research</span>
	<span class="menuitemicon"><i class="fa fa-lightbulb"></i></span>
      </a>
      <a href="../../development/projects/" title="Software">
	<span class="menuitemtext">Software</span>
	<span class="menuitemicon"><i class="fa fa-cogs"></i></span>
      </a>
      <a href="../../writing/" title="Writing">
	<span class="menuitemtext">Writing</span>
	<span class="menuitemicon"><i class="fa fa-feather"></i></span>
      </a>
      <a href="../../personal/contact/" title="Contact">
	<span class="menuitemtext">Contact</span>
	<span class="menuitemicon"><i class="fa fa-info-circle"></i></span>
      </a>
      <a href="../../rss.xml" title="RSS">
	<span class="menuitemtext">RSS</span>
	<span class="menuitemicon"><i class="fa fa-rss"></i></span>
      </a>
  <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">
     <img alt="Creative Commons License CC-BY-NC-SA-4.0" style="border-width:0" src="../../images/cc-by-nc-sa-4.0.png"></a>
  
  

      </div>

      <!-- "Hamburger menu" / "Bar icon" to toggle the navigation links -->
      <a href="javascript:void(0);" id="hamburger" class="icon" onclick="toggleNav()">
	<i class="fa fa-bars">
	</i>
      </a>
    </div>
  </section><section class="page-content"><div class="content" rel="main">
    <header><h1>Posts about programming (old posts, page 1)</h1>
        <div class="metadata">
                            <p class="feedlink">
                                    <a href="../programming.xml" hreflang="en" type="application/rss+xml">RSS feed</a>

                </p>

            
        </div>
    </header><div class="postindex">
    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="../../2010/05/28/languages-extensible-vms/" class="u-url">Languages for extensible virtual machines</a></h1>

        
        <div class="metadata">
            <p class="dateline"><a href="../../2010/05/28/languages-extensible-vms/" rel="bookmark"><i class="fa fa-clock"></i> <time class="published dt-published" datetime="2010-05-28T06:00:30+01:00" title="2010-05-28 06:00">2010-05-28 06:00</time></a></p>

                <p class="commentline"><i class="fa fa-comment"></i>         <a href="../../2010/05/28/languages-extensible-vms/#disqus_thread" data-disqus-identifier="cache/posts/2010/05/28/languages-extensible-vms.html">Comments</a>


        </p>
</div>
    </header><div class="p-summary entry-summary">
    <p></p>
<p>Many languages have an underlying virtual machine (VM) to provide a more portable and convenient substrate for compilation or interpretation. For language research it's useful to be able to generate custom VMs and other language tools for different languages. Which raises the question: what's the appropriate  language for writing experimental languages?</p>
<!--more-->

<p>What I have in mind is slightly more than just VMs, and more a platform for experimenting with language design for novel environments such as sensor-driven systems. As well as a runtime, this requires the ability to parse, to represent and evaluate type and semantic rules, and to provide a general framework for computation that can then be exposed into a target language as constructs, types and so forth. What's the right language in which to do all this?</p>
<p>This isn't a simple question. It's well-accepted that the correct choice of language is vital to the success of a coding project.  One could work purely at the language level, exploring constructs and type systems without any real constraint of the real world (such as being runnable on a sensor mote). This has to some extent been traditional in programming language research, justified by the Moore's law increases in performance of the target machines. It isn't justifiable for sensor networks, though, where <a href="../../2010/03/things-that-wont-change/">we won't see the same phenomenon</a>. If we want to prototype realistic language tools in the same framework, we need at least a run-time VM that was appropriate for these target devices; alternatively we could ignore this, focus on the language, and prototype only when we're happy with the structures, using a different framework. My gut ffeeling is that the former is preferable, if it's possible, for reasons of conceptual clarity, impact and simplicity. But even without making this decision we can consider the features of different candidate language-writing languages:
</p>
<h3>C</h3>
The most obvious approach is to use C, which is run-time-efficient and runs on any potential platform. For advanced language research, though, it's less attractive because of its poor symbolic data handling. That makes it harder to write type-checking sub-systems and the like, which are essentially symbolic mathematics.
<h3>Forth</h3>
<p><a href="../../2010/03/forth-for-sensors/">I've wondered about Forth before</a>. At one level it combines the same drawbacks as C -- poor symbolic and dynamic data handling -- with the additional drawback of being unfamiliar to almost everyone.</p>
<p>Forth <em>does</em> have some redeeming features, though. Firstly, threaded interpretation means that additional layers of abstraction are largely cost-free: they run at the same speed as the language itself. Moreover there's a sense in which threaded interpretation blurs the distinction between host language and meta-language: you don't write Forth applications, you extend it towards the problem, so the meta-language <em>becomes</em> the VM and language tool. This is something that needs some further exploration.</p>
<h3>Scheme</h3>
<p>Scheme's advantages are its simplicity, regularity, and pretty much unrivalled flexibility in handling symbolic data. There's <a href="../../2010/05/cs-book-worth-reading-twice/">a long  tradition of Scheme-based language tooling</a>, and so a lot of experience and libraries to make use of. It's also easy to write purely functional code, which can aid re-use.</p>
<p>Scheme is dynamically typed, which can be great when exploring approaches like partial evaluation (specialising an interpreter against a particular piece of code to get a compiled program, for example).</p>
<h3>Haskell</h3>
<p>In some ways, Haskell is the obvious language for a new language project. The strong typing, type classing and modules mean one can generate a typed meta-language. There are lots of libraries and plenty of activity in the research community. Moreover Haskell is in many ways the "mathematician's choice" of language, since one can often map mathematical concepts almost directly into code. Given thaat typing and semantics are just mathematical operations over symbols, this is a significant attraction.</p>
<p>Where Haskell falls over, of course, is its runtime overheads -- mostly these days in terms of memory rather than performance. It essentially mandates a choice of target platform to be fairly meaty, which closes-off some opportunities. There are some "staged" Haskell strategies that might work around this, and one could potentially stage the code to another runtime virtual machine. Or play games like implement a Forth VM inside Haskell for experimentation, and then emit code for a <em>different</em> Forth implementation for runtime.</p>
<h3>Java</h3>
<p>Java remains the language <em>du jour</em> for most new projects. It has decent dynamic data handling, poor symbolic data handling, fairly large run-time overheads and a well-stocked library for re-use. (Actually I used Java for <a href="../../publications/#Vanilla-GCSE99">Vanilla</a>, an earlier project in a similar area.) Despite the attractions, Java feels wrong. It doesn't provide a good solution to <em>any</em> of the constraints, and would be awkward as a platform for manipulating rules-based descriptions.</p>
<h3>Smalltalk</h3>
<p>Smalltalk -- and especially <a href="http://www.squeak.org">Squeak</a> -- isn't a popular choice within language research, but does have a portable virtual machine, VM generation, and other nice features and libraries. The structure is also attractive, being modern and object-oriented. It's also a good platform for building interactive systems, so one could do simulation, visual programming and the like within the same framework -- something that'd be much harder with other choices. There are also some obvious connectionns between Smalltalk and pervasive systems, where one is talking about the interactions of objects in the real world.</p>
<p>Where does that leave us? Nowhere, in a sense, other than with a list of characteristics of different candidate languages for language research. It's unfortunate there isn't a clear winner; alternatively, it's positive that there's a choice depending on the final direction. The worry has to be that a project like this is a moving target that moves away from the areas of strength for any choice made.</p>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="../../2010/05/21/impressions-pervasive-2010/" class="u-url">Impressions of Pervasive 2010</a></h1>

        
        <div class="metadata">
            <p class="dateline"><a href="../../2010/05/21/impressions-pervasive-2010/" rel="bookmark"><i class="fa fa-clock"></i> <time class="published dt-published" datetime="2010-05-21T06:00:02+01:00" title="2010-05-21 06:00">2010-05-21 06:00</time></a></p>

                <p class="commentline"><i class="fa fa-comment"></i>         <a href="../../2010/05/21/impressions-pervasive-2010/#disqus_thread" data-disqus-identifier="cache/posts/2010/05/21/impressions-pervasive-2010.html">Comments</a>


        </p>
</div>
    </header><div class="p-summary entry-summary">
    <p></p>
<p>I've spent this week at the <a href="http://www.pervasive2010.org">Pervasive 2010 conference</a> on pervasive computing, along with the <a href="http://www.pmmps.org">Programming Methods for Mobile and Pervasive Systems</a> workshop I co-arranged with <a href="http://www.dominicduggan.org/">Dominic Duggan</a>. Both events have been fascinating.</p>
<!--more-->
<p>The PMMPS workshop is something we've wanted to run for a while, bringing together the programming language and pervasive/mobile communities to see where languages  ought to go. We received a diverse set of submissions: keynotes from Roy Campbell and Aaron Quigley, talks covering topics including debugging, software processes, temporal aspects (me), context collectionvisual programming ang a lot more. Some threads emerge quite strongly, but I think they'll have to wait for a later post after I've collected my thoughts a bit more.</p>
<p>The main conference included many  papers so good that it seems a shame to single any out. The following are simply those that spoke most strongly to me:</p>
<p><strong>Panorama and Cascadia.</strong> The University of Washington presented work on a "complex" events system, combining lower-level raw events. Simple sensor events are noisy and often limited in their coverage. Cascadiais an event service that allows complex events to be defined over the raw event stream, using Bayesian particle filters to interpolate missing events or those from uncovered areas: so it's possible in principle to inferentially  "sense" someone's location even in places without explicit sensor coverage, using a model of the space being observed. This is something that could be generalised to other model-based sensor streams. The Panorama tool allows end-users to specify complex events by manipulating thresholds, which seems little unsatisfactory: there's no principled way to determine the thresholds, and it still begs the question of how to program with the uncertain event stream. Still, I have to say this is the first complex event system I've seen that I actually believe could work.</p>
<p><strong>Eyecatcher.</strong> How do you stop people hiding from a camera, or playing-up to it? Work from Ochanomizu University in Japan places a small display on top of the camera, which can be used to present images to catch the subject's attention and to suggest poses or actions. (Another version barks like a dog, to attract your pet's attention.)I have to say this research is very Japanese, a very unusual but perceptive view of the world and the problems appropriate for research.</p>
<p><strong>Emotion modeling</strong>. Jennifer Healey from Intel described how to monitor and infer emotions from physiological data. The main problem is that there is no common language for describing emotions -- "anxious" is good for some and bad for others -- so getting ground truth is hard even given extensive logging.</p>
<p><strong>Indoor location tracking for non-experts</strong>. More University of Washington work, this time looking at an indoor location system simple enough to be used by non-experts such as rehabilitation therapists. They used powerline positioning, injecting different frequencies into a home's power network and detecting the radiated signal using what are essentially AM radios. Interestingly one of the most important factors was the aesthetics of the sensors: people don't want ugly boxes in their home.</p>
<p><strong>Transfer learning</strong>. Tim van Kasteren of the University of Amsterdam has generated one of the most useful smart-home data sets, used across the community (including by several of my students). He reported experiences with transfering machine-learned classifiers from one sensor network to another, by mapping the data into a new, synthetic feature space. He also used the known distribution of features from the first network to condition the learning algorithhm in the second, to improve convergence.</p>
<p><strong>Common Sense</strong>. Work from UC Berkeley on a platform for participative sensing: <a href="http://www.communitysensing.org/">CommonSense</a>. The idea is to place environmental sensors onto commodity mobile devices, and give them to street cleaners and others "out and about" in a community. The great thing about this is that is gives information on pollution and the like to the communities themselves, directly, rather than mediated through a (possibly indifferent or otherwise) State agency.</p>
<p><strong>Energy-aware data traffic management</strong>. I should add the disclaimer that is work by my colleague, Mirco Musolesi of the University of  St Andrews. Sensor nodes need to be careful about the energy they use to transmit data back to their base station. This work compares a range of strategies that trade-off the accuracy of returned data with the amount of traffic exchanged and so the impact on the nodoe's battery. This is //really// important for environmental sensing, and makes me think about further modifying the models to account for what's being sensed to trade-off information content as well.</p>
<p><strong>Tutorials</strong> AJ Brush did a wonderful tutorial on how to do user surveys. This is something we've done ourselves, and it was great to see the issues nailed-down -- along with war stories of how to plan and conduct a survey for greatest validity and impact. Equally, John Krumm did a fantastic overview of signal processing, particle filters, hidden Markov models and the like that make the maths <em>far</em> more accessible than it normally is. Adrian Friday heroically took the graveyard slot with experiences and ideas about system support for pervasive systems.</p>
<p>This is the first large conference I've attended for a while, for various reasons, and it's been a great week both scientifically and socially. The organisers at the University of Helsinki deserve an enormous vote of thanks for their efforts. Pervasive next year will be in San Francisco, an I'll definitely be there -- hopefully with a paper to present :-)</p>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="../../2010/05/14/cs-book-worth-reading-twice/" class="u-url">The only computer science book worth reading twice?</a></h1>

        
        <div class="metadata">
            <p class="dateline"><a href="../../2010/05/14/cs-book-worth-reading-twice/" rel="bookmark"><i class="fa fa-clock"></i> <time class="published dt-published" datetime="2010-05-14T06:00:35+01:00" title="2010-05-14 06:00">2010-05-14 06:00</time></a></p>

                <p class="commentline"><i class="fa fa-comment"></i>         <a href="../../2010/05/14/cs-book-worth-reading-twice/#disqus_thread" data-disqus-identifier="cache/posts/2010/05/14/cs-book-worth-reading-twice.html">Comments</a>


        </p>
</div>
    </header><div class="p-summary entry-summary">
    <p></p>
<p>I was talking to one of my students earlier, and lent him a book to read over summer. It was only after he'd left that I realised  that -- for me at any rate -- the book I'd given him is probably the most seminal work in the whole of computer science, and certainly the book that's most influenced my career and research interests.</p>
<!--more-->
<p>So what's the book? <em>Structure and interpretation of computer programs</em> by Hal Abelson and Jerry Sussman <span> (MIT Press. 1984. ISBN 0-262-01077-1), also known as <em>SICP</em>. </span>The book's still in print, but -- even better -- is <a href="http://mitpress.mit.edu/sicp/">available online in its entirety</a>.</p>
<p><span>OK, everyone has their favourite book: why's this one so special to me? The first reason is the time I first encountered it: in </span><a href="http://www.cs.newcastle.ac.uk/">Newcastle upon Tyne</a> in <span>the second year of my first degree. I was still finding my way in computer science, and this book was a recommended text after you'd finished the first programming courses. It's the book that introduced me to programming <em>as it could be</em> (rather than programming <em>as it was</em>, in Pascal at the time). What I mean by that is that SICP starts out by introducing the elements of programming -- values, names, binding, control and so on -- and then runs with them to explore a quite dazzling breadth of issues including:</span>
</p>
<ul>
<li><span>lambda-abstraction and higher-order computation</span></li>
    <li><span>complex data structures, including structures with embedded computational content</span></li>
    <li><span>modularity and mutability</span></li>
    <li><span>streams</span></li>
    <li><span>lazy evaluation</span></li>
    <li><span>interpreter and compiler construction</span></li>
    <li><span>storage management, garbage collection and virtual memory
</span></li>
    <li><span>machine code
</span></li>
    <li><span>domain-specific languages</span></li>
</ul>
<span>...and so forth. The list of concepts is bewildering, and only stays coherent because the authors are skilled writers devoted to their craft. But it's also a remarkable achievement to handle all these concepts within a single language framework -- Scheme -- in such a way that each builds on what's gone before.
</span>
<p><span>The second reason is the way in which Hal and Jerry view everything as an exercise in language design:</span></p>
<blockquote>We have also obtained a glimpse of another crucial idea about languages and program design. This is the approach of stratified design, the notion that a complex system should be structured as a sequence of levels that are described using a sequence of languages. Each level is constructed by combining parts that are regarded as primitive at that level, and the parts constructed at each level are used as primitives at the next level. The language used at each level of a stratified design has primitives, means of combination, and means of abstraction appropriate to that level of detail.</blockquote>
<p>Layered abstraction of course is second nature to all computer scientists. What's novel in this view is that each level should be <em>programmable</em>: that the layers are all about computation and transformation, and not simply about hiding information. We don't see that in the mainstream of programming languages, because layering doesn't extend the language at all: Java is Java from top to bottom, with class and libraries but no new control structures. If a particular domain has concepts that would benefit from dedicated language constructs, that's just tough. Conversely (and this is something that very much interests me) if there are constructs it'd be desirable <em>not</em> to have in some domain, they can't be removed. (Within the language, anyway: Java-ME dumps some capabilities in the interests of running on small devices, but that's not something you can do without re-writing the compiler.)</p>
<p>The third influential feature is the clear-sighted view of what computer science is actually about:</p>
<blockquote>The computer revolution is a revolution in the way we think and in the way we express what we think.  The essence of this change is the emergence of what might best be called <em>procedural epistemology</em> -- the study of the structure of knowledge from an imperative point of view, as opposed to the more declarative point of view taken by classical mathematical subjects. Mathematics provides a framework for dealing precisely with notions of "what is."  Computation provides a framework for dealing precisely with notions of "how to."</blockquote>
<p>I've taken a view before about <a href="../../2010/04/computer-microscope/">computers being the new microscopes</a>, opening-up new science on their own as well as facilitating existing approaches. The "how to" aspect of computer science re-appears everywhere in this: in describing the behaviours of sensor networks that can adapt while continuing the reflect the phenomena they've been deployed to sense; in the interpretation of large-scale data mined and mashed-up across the web; in capturing scientific methods and processes for automation; and so forth. The richness of these domains mitigates against packaged software and encourages integration through programming languages like <a href="http://www.r-project.org">R</a>, so that the interfaces and structures remain "soft" and open to experimentation.</p>
<p><span>When I looked at my copy, the date I'd written on the inside was September 1988. So a book I bought nearly 22 years ago is still relevant. <em> </em>In fact, I'd go further and say that it's the only computer science book of that age that I'd happily and usefully read again without it being just for historical interest: the content has barely aged at all. That's not all that unusual for mathematics books, but it's almost unheard of in computer science, where the ideas move so quickly and where much of what's written about is ephemeral rather than foundational. It goes to show how well SICP nailed the core concepts. In this sense, it's certainly one of the very few  books on computer science that it's worth reading twice (or more). SICP is to computer science what Feynman's <em>Lectures on Physics</em> are to physics: an accessible distillation of the essence of the subject that's stood the test of time.
</span></p>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="../../2010/03/12/forth-for-sensors/" class="u-url">Forth for sensors?</a></h1>

        
        <div class="metadata">
            <p class="dateline"><a href="../../2010/03/12/forth-for-sensors/" rel="bookmark"><i class="fa fa-clock"></i> <time class="published dt-published" datetime="2010-03-12T06:00:10Z" title="2010-03-12 06:00">2010-03-12 06:00</time></a></p>

                <p class="commentline"><i class="fa fa-comment"></i>         <a href="../../2010/03/12/forth-for-sensors/#disqus_thread" data-disqus-identifier="cache/posts/2010/03/12/forth-for-sensors.html">Comments</a>


        </p>
</div>
    </header><div class="p-summary entry-summary">
    <p></p>
<p>Most sensor systems are programmed using C: compact and well-known, but low-level and tricky to get right when things get compact and complex. There have been several proposals for alternative languages from across the programming language research spectrum. I haven't heard anyone mention Forth, though, and it's worth considering -- even if only as a target for other languages.</p>
<!--more-->
<p>Many people will never have encountered <a href="http://en.wikipedia.org/wiki/Forth_%28programming_language%29">Forth</a>, a language that's enjoyed up-and-down popularity for over four decades without ever hitting the mainstream. Sometimes touted as an alternative to Basic, it even had an <a href="http://en.wikipedia.org/wiki/Jupiter_Ace">early-1980's home computer</a> that used it as the introductory language.</p>
<p>Forth has a number of characteristics that are completely different to the vast majority of modern languages:
</p>
<ul>
<li>It uses and explicit data stack and <a href="http://en.wikipedia.org/wiki/Reverse_Polish_notation">Reverse-Polish notation</a> uniformly throughout the language</li>
    <li>There's no type system. Everything is represented pretty much using addresses and integers. The programmer is on her own when building complex structures</li>
    <li>It is a threaded interpreter where every construct in the language is a "word". Composing words together generates new words, but (unlike in an interpreter) the definitions are compiled efficiently, so there's an immediacy to things without crippling performance overheads</li>
    <li>A standard system usually mixes its "shell" and "compiler" together, so one can define new words interactively which get compiled immediately</li>
    <li>There's a small kernel of machine-code (or C) routines, but...</li>
    <li>The compiler itself -- and indeed the vast majority of the system -- can be written in Forth, so you can extend the compiler (and hence the language) with new constructs that have first-class status alongside the built-in words. There's typically almost no overhead of programmer- <em>versus</em> system-defined words, since they're all written in the same (fast) language</li>
    <li>If you're careful, you can build a cross-compiler that will generate code for a different target system: just port the kernel and the compiler <em>should</em> be re-usable to generate code that'll run on it. (It's not that simple, of course, as I'm finding myself...)</li>
</ul>
So Forth programs don't look like other languages. There's no real phase distinction between compilation and run-time, since everything's mixed-in together, but that has the advantage that you can write new "compiler" words to make it easier to write your "application" words, all within the same framework and set of capabilities. You don't  write applications so much as extend the language itself towards your problem. That in turn means you can view Forth either as low-level -- a glorified assembler -- or very high-level in terms of its ability to define new syntax and semantics.
<p>That probably sounds like a nightmare, but suspend judgment for a while.....</p>
<p>One of the problems that concerns a lot of sensor networks people is the programming level at which we have to deal with systems. Typically we're forced to write C on a per-node basis: program the individual nodes, and try to set it up so that the network behaves, as a whole, in an intended way. This is clearly possible in many cases, and clearly gets way more difficult as things get bigger and more complex, and especially when we want the network to adapt to the phenomena it's sensing, which often requires decisions to be made on a non-local basis.</p>
<p>Writing a new language is a big undertaking, but a substantially smaller undertaking with Forth. It's possible to conceive of new language structures (for example a construct that generates <a href="http://www.simondobson.org/2010/02/216/">moving averages</a>) and implement it quickly and simply. This might just be syntactic sugar, or might be something rather more ambitious in terms of control flow. Essentially you can extend the syntax <em>and</em> the semantics of Forth so that it "understands", at the same level as the rest of the compiler, the new construct you want to use.</p>
<p>Which is interesting enough, but what makes it more interesting for sensors is the structure of these compiler words. Typically they're what is known as <tt>IMMEDIATE</tt> words, which means they <em>execute</em> when encountered <em>compiling</em> a word. That may sound odd, but what it means is that the compiler word executes and generates code, rather than being compiled itself. And <em>that</em> means that, when used with a cross-compiler, the new language constructs don't add to the target system's footprint, because their action all happens at compile-time to generate code that's expressed in terms of lower-level words. In core Forth, constructs like <tt>IF</tt> and <tt>LOOP</tt> (conditional and counted loops respectively) do exactly this: they compile low-level jumps (the word <tt>(BRANCH)</tt> and <tt>(?BRANCH)</tt>, which do non-conditional and conditional branches respectively) implementing the higher-level structured-programming abstraction.</p>
<p>A lot of modern languages use virtual machines as targets for their compilers, and a lot of those VMs are stack machines -- Forth, in other words. If we actually <em>use</em> Forth as the VM for a compiler, we have an <em>extensible</em> VM in which we can define new constructs, so we can evolve the VM better to fit the language that targets it. (There are also some very interesting, close parallels between Forth code and the abstract syntax trees used to represent code within compilers, but that's something I need to think about a bit more before I write about it.)</p>
<p>All this is rather speculative, and doesn't really address the core problem of programming a network rather than a device, but it does provide a device-level platform that might be more amenable to language research. I've been experimenting with Forth for this purpose, and have a prototype system -- Attila, an abstract, re-targetable threaded interpreter that's fully cross-compilable -- in the works, but not quite ready to see the light of day. It's taught me a lot about the practicalities of threaded interpreters and cross-compilers. This is a strand of language design that's been almost forgotten, and I think it deserves more of a look.</p>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="../../2010/02/26/216/" class="u-url">Programming with limited certainty</a></h1>

        
        <div class="metadata">
            <p class="dateline"><a href="../../2010/02/26/216/" rel="bookmark"><i class="fa fa-clock"></i> <time class="published dt-published" datetime="2010-02-26T09:00:18Z" title="2010-02-26 09:00">2010-02-26 09:00</time></a></p>

                <p class="commentline"><i class="fa fa-comment"></i>         <a href="../../2010/02/26/216/#disqus_thread" data-disqus-identifier="cache/posts/2010/02/26/216.html">Comments</a>


        </p>
</div>
    </header><div class="p-summary entry-summary">
    <p></p>
<p>Sensor networks are all about uncertainty: if the sensor says it's 20°C  out there, then it might be 20°C plus-or-minus half a degree or so (limited precision); or it might be some different temperature, and the sensor's just reported a duff value for some reason (limited accuracy). By contrast, computers most definitely <em>aren't</em> about uncertainty, a fact enshrined in the famous maxim "garbage in, garbage out". What does this mean for our ability to build really large, robust and flexible sensor networks?</p>
<!--more-->
<p>All the really foundational models of computing -- λ calculus, Turing machines and the like -- pretty much reflect this notion that input is correct in some sense, and if it's wrong then that's an error to be corrected outside the computational system. That seems to mean that the computational system can't itself either tolerate or express the notions of limited certainty -- precision and accuracy -- that lie at the core of sensor networks (and a lot of other modern systems, or course). That suggests to me that there might be a problem at the heart of computer science as we currently formulate it: it isn't a realistic model of the world we're trying to compute over.</p>
<p>In some ways this is nether surprising nor threatening. Any mathematical or computational model is only a simplified abstraction of the real world, for which we have to make often staggeringly bold simplifications if we're to get anywhere. We should however always be prepared to challenge the <em>validity</em> and <em>necessity</em> of these simplifications, and that's what I'd like to do here.</p>
<p>As far as validity is concerned, the simplification is quite patently <em>invalid</em> when it comes to any system that operates with real-world data: some of it is <em>bound</em> to be "wrong" in some sense. This isn't the same as being tolerant of mistakes, such as when someone presses the delete key by mistake: that's a  action that certainly happened and to which the system responded correctly, albeit "wrongly" from the user's perspective. Interesting problem, but different: we're talking here about responding to inherently erroneous input -- the delete key seeming to press itself, if you like.</p>
<p>Necessity, then: is it necessary to handle computation in this way? Clearly not: we can easily conjecture a computational model that's more tolerant of input with limited certainty.</p>
<p>Consider precision first. If the input is only known to a limited precision, then we don't want that error margin to cause enormous errors. If we have a function $latex f$, then we want $latex f$ to exhibit a tolerance of imprecision such that $latex \delta x &lt; tol_x \Rightarrow \left | f(x + \delta x) - f(x) \right | &lt; s \left | \delta x \right|$ for some scaling factor $latex s &lt; 1$. $latex f$ doesn't cause errors to blow-up in unpredictable ways. A lot of functions behave in exactly this way: for example, in a sliding-window average function $latex f_w(\overline{x}, x) = \frac{x + \overline{x}(w - 1)}{w}$ for an average $latex \overline{x}$ computed from $latex w$ recent observations, we have that $latex s = \frac{1}{w}$. Small errors therefore perturb the result significantly less than the input is perturbed. If the errors are uniformly distributed, the function should converge on the "true" value.</p>
<p>Conversely, a large, accurate new observation will perturb the average only slowly, so large step-changes will be detected only slowly. It's hard to distinguish such a change when it first happens from an inaccurate reading. There are various ways of dealing with this, such as using a weighted sliding window with non-linear weighting.</p>
<p>This is a rather topological idea. Rather than simply mapping points in an input space (such as temperature) to an output space (average temperature over the past few minutes), we're also requiring that the mapping take elements close in the input space to elements close in the result space: we require that it be a <em>contraction mapping</em>. Building systems from contraction mappings, perhaps combined with contraction-preserving operators, yields systems that are robust to small errors in precision from their sensors.</p>
<p>Of course not all systems <em>are</em> actually like this, and in many cases we <em>want</em> rapid changes to be detected quickly and/or propagated. The point, perhaps, is that this is a <em>choice we should make</em> rather than a <em>consequence</em> of choosing a particular model of computation. There might actually be a model of computation lurking about here, in which we define functions coupled with a model of how their input and output errors should behave. At the very least, this yields systems in which we can predict the consequences of errors and imprecisions, which is a major challenge to deal with at present.</p>
    </div>
    </article>
</div>
        <nav class="postindexpager"><ul class="pager">
<li class="next">
                <a href="index-2.html" rel="next">Newer posts</a>
            </li>
        </ul></nav><script>var disqus_shortname="simoninireland";(function(){var a=document.createElement("script");a.async=true;a.src="https://"+disqus_shortname+".disqus.com/count.js";(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(a)}());</script>
</div>
  </section><script src="../../assets/js/baguetteBox.min.js"></script><script src="../../assets/js/moment-with-locales.min.js"></script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(2, "YYYY-MM-DD HH:mm");
  </script><!-- end fancy dates --><script>
    baguetteBox.run('div#content', {
    ignoreClass: 'islink',
    captions: function(element) {
    return element.getElementsByTagName('img')[0].alt;
    }});
  </script><script src="../../assets/js/ToggleNav.js"></script>
</body>
</html>

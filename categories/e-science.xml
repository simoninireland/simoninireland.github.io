<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Simon Dobson (Posts about e-science)</title><link>https://simondobson.org/</link><description></description><atom:link href="https://simondobson.org/categories/e-science.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2021 &lt;a href="mailto:simon.dobson@computer.org"&gt;Simon Dobson&lt;/a&gt; </copyright><lastBuildDate>Mon, 15 Mar 2021 16:30:14 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Call for papers: D3Science</title><link>https://simondobson.org/blog/2011/06/06/cfp-d3science/</link><dc:creator>Simon Dobson</dc:creator><description>&lt;div&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Papers are solicited for a workshop on data-intensive, distributed and dynamic e-science problems.&lt;/p&gt;
&lt;!--more--&gt;
&lt;p&gt;&lt;/p&gt;&lt;h2&gt;&lt;a href="http://www.ci.uchicago.edu/D3Science/" target="_blank"&gt;Workshop on D3Science&lt;/a&gt; - Call for Papers&lt;/h2&gt;
&lt;div&gt;
&lt;h3&gt;(To be held with &lt;a href="http://www.escience2011.org/" target="_blank"&gt;IEEE e-Science 2011&lt;/a&gt;)
Monday, 5 December 2011
Stockholm, Sweden&lt;/h3&gt;
&lt;/div&gt;
&lt;div&gt;

This workshop is interested in data-intensive, distributed, and dynamic (D3) science. It will also focus on innovative approaches for scalability in the end-to-end real-time processing of scientific data. We refer to D3 applications as those are data-intensive, are fundamentally, or need to be, distributed, and need to support and respond to data that may be non-persistent and is dynamically generated. We are also looking to bring researchers together to look at holistic, rather than piecewise, approaches to the end-to-end processing and managing of scientific data.

&lt;/div&gt;
&lt;div&gt;

There has been a lot of effort in managing and distributing tasks where computation is dominant. Such applications have after all, been historically the drivers of "grid" computing. There has been, however, relatively less effort on tasks where the computationalload is matched by the data load, or even dominated by the data load. For such tasks to be able to operate at scale, there are conceptually simple run-time tradeoffs that need to be made, such as determining whether to move data to compute versus moving compute to data, or possibly regenerating data on-the-fly. Due to fluctuating resource availability and capabilities, as well as insufficient prior information about application requirements, such decisions must be made at run-time. Furthermore, resource, connectivity and/or storage constraints may require the data to be manipulated in-transit so that it is "made-right" for the consumer. Currently it is very difficult to implement these dynamic decisions or the underlying mechanisms in a general-purpose and scalable fashion. Although the increasing volumes and complexity of data will make many problems data-dominated, the computational requirements will still be high. In practice, data-intensive applications will encompass data-driven applications. For example, many data-driven applications will involve computational activities triggered as a consequence of independently created data; thus it is imperative for an application to be able to respond to unplanned changes in data load or content. Therefore, understanding how to support dynamic computations is a fundamental, but currently missing element in data-intensive computing.

&lt;/div&gt;
&lt;div&gt;

The D3Science workshop builds upon a 3 year research theme on Distributed Programming Abstractions (DPA, &lt;a href="http://wiki.esi.ac.uk/Distributed_Programming_Abstractions"&gt;http://wiki.esi.ac.uk/Distributed_Programming_Abstractions&lt;/a&gt;), which has held a series of related workshops including but not limited to e-Science 2008, EuroPar 2008 and the CLADE series, and the ongoing 3DPAS (&lt;a href="http://wiki.esi.ac.uk/3DPAS"&gt;http://wiki.esi.ac.uk/3DPAS&lt;/a&gt;) research theme funded by the NSF and UK EPSRC, which is holding one workshop in June 2011: the 3DAPAS workshop (&lt;a href="https://sites.google.com/site/3dapas/"&gt;https://sites.google.com/site/3dapas/&lt;/a&gt;). The workshop is intended to lead to a funding proposal for transcontinental collaboration, with contributors as potential members of the collaboration, and as such, we are particularly interested is discussing both existing and future projects that are suitable for transcontinental collaboration.

&lt;/div&gt;
&lt;div&gt;

Topics of interest include but are not limited to:

&lt;/div&gt;
&lt;ul&gt;
    &lt;li&gt;Case studies of development, deployment and execution of representative&lt;/li&gt;
    &lt;li&gt;D3 applications, particularly projects suitable for transcontinental collaboration&lt;/li&gt;
    &lt;li&gt;Programming systems, abstractions, and models for D3 applications&lt;/li&gt;
    &lt;li&gt;Discussion of the common, minimally complete, characteristics of D3 application&lt;/li&gt;
    &lt;li&gt;Major barriers to the development, deployment, and execution of D3 applications, and primary challenges of D3 applications at scale&lt;/li&gt;
    &lt;li&gt;Patterns that exist within D3 applications, and commonalities in the way such patterns are used&lt;/li&gt;
    &lt;li&gt;How programming models, abstraction and systems for data-intensive applications can be extended to support dynamic data applications&lt;/li&gt;
    &lt;li&gt;Tools, environments and programming support that exist to enable emerging distributed infrastructure to support the requirements of dynamic applications (including but not limited to streaming data and in-transit data analysis)&lt;/li&gt;
    &lt;li&gt;Data-intensive dynamic workflow and in-transit data manipulation&lt;/li&gt;
    &lt;li&gt;Adaptive/pervasive computing applications and systems&lt;/li&gt;
    &lt;li&gt;Abstractions and mechanisms for dynamic code deployment and "moving code to data"&lt;/li&gt;
    &lt;li&gt;Application drivers for end-to-end scientific data management&lt;/li&gt;
    &lt;li&gt;Runtime support for in-situ analysis&lt;/li&gt;
    &lt;li&gt;System support for high end workflows&lt;/li&gt;
    &lt;li&gt;Hybrid computing solutions for in-situ analysis&lt;/li&gt;
    &lt;li&gt;Technologies to enable multi-platform workflows&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Submission instructions&lt;/h3&gt;
&lt;p&gt;Authors are invited to submit papers containing unpublished, original work (not under review elesewhere) of up to 8 pages of double column text using single spaced 10 point size on 8.5 x 11 inch pages, as per IEEE 8.5 x 11 manuscript guidelines. Templates are available:&lt;a href="http://www.ieee.org/web/publications/pubservices/confpub/AuthorTools/conferenceTemplates.html"&gt; http://www.ieee.org/web/publications/pubservices/confpub/AuthorTools/conferenceTemplates.html&lt;/a&gt; Authors should submit a PDF or PostScript (level 2) file that will print on a PostScript printer. Papers conforming to the above guidelines can be submitted through the workshop's paper submission system: &lt;a href="http://www.easychair.org/conferences/?conf=d3science"&gt;http://www.easychair.org/conferences/?conf=d3science&lt;/a&gt; It is a requirement that at least one author of each accepted paper register and attend the conference.&lt;/p&gt;
&lt;h3&gt;Important dates&lt;/h3&gt;
&lt;ul&gt;
    &lt;li&gt;17 July 2011 - submission date&lt;/li&gt;
    &lt;li&gt;23 August 2011 - decisions announced&lt;/li&gt;
    &lt;li&gt;23 September 2011 - final versions of papers due to IEEE for proceedings&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Organizers&lt;/h3&gt;
&lt;ul&gt;
    &lt;li&gt;Daniel S. Katz, University of Chicago &amp;amp; Argonne National Laboratory, USA&lt;/li&gt;
    &lt;li&gt;Neil Chue Hong, University of Edinburgh, UK&lt;/li&gt;
    &lt;li&gt;Shantenu Jha, Rutgers University &amp;amp; Louisiana State University, USA&lt;/li&gt;
    &lt;li&gt;Omer Rana, Cardiff University, UK&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;PC members&lt;/h3&gt;
&lt;ul&gt;
    &lt;li&gt;Gagan Aggarwal, Ohio State University, USA&lt;/li&gt;
    &lt;li&gt;Deb Agarwal, Lawrence Berkeley National Lab, USA&lt;/li&gt;
    &lt;li&gt;Gabrielle Allen, Lousiana State University, USA&lt;/li&gt;
    &lt;li&gt;Malcolm Atkinson, University of Edinburgh, UK&lt;/li&gt;
    &lt;li&gt;Adam Barker, University of St Andrews, UK&lt;/li&gt;
    &lt;li&gt;Paolo Besana, University of Edinburgh, UK&lt;/li&gt;
    &lt;li&gt;Jon Blower, University of Reading, UK&lt;/li&gt;
    &lt;li&gt;Yun-He Chen-Burger, University of Edinburgh, UK&lt;/li&gt;
    &lt;li&gt;Simon Dobson, University of St Andrews, UK&lt;/li&gt;
    &lt;li&gt;Gilles Fedak, INRIA, France&lt;/li&gt;
    &lt;li&gt;Cécile Germain, University Paris Sud, France&lt;/li&gt;
    &lt;li&gt;Keith R. Jackson, Lawrence Berkeley National Lab, USA&lt;/li&gt;
    &lt;li&gt;Manish Parashar, Rutgers, USA&lt;/li&gt;
    &lt;li&gt;Abani Patra, University of Buffalo, USA&lt;/li&gt;
    &lt;li&gt;Yacine Rezgui, Cardiff University, UK&lt;/li&gt;
    &lt;li&gt;Yogesh Simmhan, University of Southern California, USA&lt;/li&gt;
    &lt;li&gt;Domenico Talia, University of Calabria, Italy&lt;/li&gt;
    &lt;li&gt;Paul Watson, Newcastle University, UK&lt;/li&gt;
    &lt;li&gt;Jon Weissman, University of Minnesota, USA&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;/p&gt;&lt;/div&gt;</description><category>Blog</category><category>cfp</category><category>conference</category><category>e-science</category><guid>https://simondobson.org/blog/2011/06/06/cfp-d3science/</guid><pubDate>Mon, 06 Jun 2011 09:20:44 GMT</pubDate></item><item><title>Computer science and the financial crisis</title><link>https://simondobson.org/blog/2010/04/09/computer-science-financial-crisis/</link><dc:creator>Simon Dobson</dc:creator><description>&lt;div&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Many people expected a financial crisis; many also expected it to be caused by automated trading strategies driving share prices, As it turned out, that aspect of computing in finance didn't behave as badly as expected, and the problems arose in the relatively computing-free mortgage sector. Is there any way computing could have helped, or could help avoid future crises? &lt;!--more--&gt;&lt;/p&gt;
&lt;!-- more --&gt;

&lt;p&gt;Over brunch earlier this week my wife &lt;a href="http://www.facebook.com/profile.php?id=670070295"&gt;Linda&lt;/a&gt; was worrying about the credit crunch. It's especially pressing since we're in Ireland at the moment, and the whole country is convulsed with the government's setting-up of the &lt;a href="http://www.nama.ie"&gt;National Asset Management Agency (NAMA)&lt;/a&gt; to take control of a huge tranche of bad debts from the Irish banks. She asked me, "what can you do, as a computer scientist, to fix this?" Which kind of puts me on the spot thinking about a topic about which I know very little, but it got me thinking that there may be areas where programming languages and data-intensive computing might be relevant for the future, at least. So here goes....&lt;/p&gt;
&lt;p&gt;The whole financial crisis has been horrendously complex, of course, so let's start by focusing on the area I know best: the Irish "toxic tiger" crash. This has essentially been caused by banks making loans to developers to finance housing and commercial property construction, both of which were encouraged through the tax system. Notably the crash was &lt;em&gt;not&lt;/em&gt; caused by loans to sub-prime consumers and subsequent securitisation, and so is substantively different to the problems in the US (although triggered by them through tightening credit markets). The loans were collateralised on the present (or sometimes future, developed) value of the land -- and sometimes on "licenses to build" on land rather than the land itself --  often cross-collateralised with several institutions and developments, and financed by borrowings from the capital markets rather than from deposits (of which Ireland, as a country of 4M people, has rather little). As capital availability tightened across 2008-9 the property market stalled, the value of the land dropped (by 80% in some cases, and by 100% for the licenses to build), and the banks have been left with bad loans and capital shortcomings in the range of at least EUR60B which now the government, for reasons strongly suspected to be political and ideological rather than being necessarily in the best interests of the taxpayer, are taking onto the public balance sheet rather than allowing them to be carried by the banks' owners and bondholders. The crash has also, as might be expected, revealed enormous shortcomings in banks' risk management, their understanding of their own holdings and exposures, some unbelievably lax supervision by the authorities, and a lot of suspected corruption and corporate fraud.&lt;/p&gt;
&lt;p&gt;(The above is a gross simplification, of course. The &lt;a href="http://www.irisheconomy.ie"&gt;Irish Economy blog&lt;/a&gt; is the best source of details, being run by a large fraction of Ireland's leading academic economists who have been depressingly accurate as to how the crisis would unfold.)&lt;/p&gt;
&lt;p&gt;So what can computer science say about this mess? To start with, there are several key points we can pull out of the above scenario:
&lt;/p&gt;&lt;ol&gt;
    &lt;li&gt;&lt;em&gt;Modelling.&lt;/em&gt; Its hard to know what the effect of a given stressor would be on the system: what &lt;em&gt;exactly&lt;/em&gt; happens if there's sector-wide unemployment, or a localised fall in purchasing?. Most banks seem to have conducted modelling only at the grossed statistical level.&lt;/li&gt;
    &lt;li&gt;&lt;em&gt;Instrument complexity.&lt;/em&gt; Different financial instruments respond to stimuli in different ways. A classic example is where unpaid interest payments are rolled-up into the principal of a mortgage, changing its behaviour completely. Parcelling-up these instruments makes their analysis next to impossible.&lt;/li&gt;
    &lt;li&gt;&lt;em&gt;Traceability.&lt;/em&gt; The same assets appear in different places without being detected up-front, which makes all the instruments involved significantly more risky and less valuable.&lt;/li&gt;
    &lt;li&gt;&lt;em&gt;Public trust.&lt;/em&gt; The "stress tests" conducted by regulators were conducted in secret without independent scrutiny, as have been the valuations applied to the bad loans. The public is therefore being asked to sign-off on a debt of which it knows nothing.&lt;/li&gt;
&lt;/ol&gt;
Clearly this is very complicated, and statistical modelling is only going to provide us with an overview. The simplifications needed to get the mathematics to work will be heroic, despite the power of the underlying analytic techniques.
&lt;p&gt;So let's treat the problem as one of simulation rather than analysis.A mortgage is generally treated as &lt;em&gt;data&lt;/em&gt; with a particular principal, interest rate, default rate and so on. It can however also be viewed as &lt;em&gt;process&lt;/em&gt;, a computational object: at each accounting period (month) it takes in money (mortage payment) and consequently has a value going forward. There is a risk that the payment won't come in, which changes the risk profile and risk-weighted value of the mortgage. It will respond in a particular way, perhaps by re-scheduling payments (increasing the term of the mortgage), or trying to turn itself into a more liquid object like cash (foreclosing on the loan and selling the house), Foreclosure involves interacting with other objects that model the potential behaviour of buyers, to indicate how much cash the foreclosure brings in: in a downturn, the liklihood of payment default increases and  the cash value of mortgages forclosed-upon similarly reduces.&lt;/p&gt;
&lt;p&gt;The point is that there's relatively little human, &lt;em&gt;banking&lt;/em&gt; intervention involved here: it's mostly computational. One could envisage a programming language for expressing the behaviours of mortgages, which defines their responses to different stimuli and defines an expected present value for the mortgage discounted by the risks of default, the amount recoverable by foreclosure, and so on.&lt;/p&gt;
&lt;p&gt;So the first thing computer science can tell us about the financial crash is that the underlying instruments are essentially computational, and can be represented as code. This provides a reference model for the behaviour we should expect from a particular instrument exposed to particular stimuli, expressed clearly as code.&lt;/p&gt;
&lt;p&gt;We can go a stage further. If loans are securitised -- packaged-up into another instrument whose value is derived from that of the underlying assets, like a futures contract -- then the value of the derivative can be computed from the values of the assets underlying it. Derivatives are often horrifically complicated, and their might be significant advantages to be had in expressing their behaviours as code.&lt;/p&gt;
&lt;p&gt;How do we get the various risk factors? Typically this is done at a gross level across an entire population, but it need not be. We now live in an exabyte era. We can treat the details of the underlying asset as metadata on the code: the location of a house, its size and price history, the owners' jobs and so forth. We currently have this data held privately, and as statistical aggregates, but there's no reason why we can't have associate the &lt;em&gt;actual&lt;/em&gt; details to &lt;em&gt;each&lt;/em&gt; loan or instrument, and therefore to each derivative constructed from them. This after all is what linked data is all about. means that each financial instrument is inherently computational, and carries with it all the associated metadata. This little package &lt;em&gt;is&lt;/em&gt; the loan, to all intents and purposes.&lt;/p&gt;
&lt;p&gt;So the second thing computer science can tell us is that we can link instruments, assets and data together, and track between them, using the standard tools and standards of the semantic web. This means we can query them at a high semantic level, and us these queries to extract partitions of the data we're interested in examining further. There's no scientific reason that this can't be done across an entire market, not simply within a single institution.&lt;/p&gt;
&lt;p&gt;The net result of the above is that, given this coding and linking, &lt;em&gt;the financial system can be run in simulation&lt;/em&gt;. We can conduct stress tests at a far finer resolution by for example running semantic queries to extract a population of interest, making them all redundant (in simulation), and seeing what happens not only to their loans, but to the securitised products based on them -- since everythings just a computation. Multiple simulations can be run to explore different future scenarios, based on different risk weightings an so forth.&lt;/p&gt;
&lt;p&gt;(This sounds like a lot of data, so let's treat the UK housing market as a &lt;a href="http://en.wikipedia.org/wiki/Fermi_problem"&gt;Fermi problem&lt;/a&gt; and see if it's feasible. There are 60M people in the UK. Assume 2-person families on average, yielding 30M domestic houses to be mortgaged. Some fraction of these are mortgage-free, say one third, leaving 20M mortgages. The workforce is around 40M working in firms with an average of say 20 employees, each needing premises, yielding a further 2M commercial mortages. If each mortgage needs 10Kb of data to describe it, we have 22M objects requiring about 250Tb of data: a large but not excessive data set, especially when most objects execute the same operations and share a lot of common data: certainly well within the simulation capabilities of cloud computing. So we're not in computationally infeasible territory here.)&lt;/p&gt;
&lt;p&gt;We can actually go a few stages further. Since we have a track-back from instrument to metadata, we can &lt;em&gt;learn&lt;/em&gt; the risks over time by observing what happens to specific cohorts of borrowers exposed to different stimuli and stresses. Again, linked data lets us identify the patterns of behaviour happening in other data sets, such as regional unemployment (now &lt;a href="http://data.gov.uk"&gt;directly available online in the UK&lt;/a&gt; and elsewhere). The more data goes online the easier it is to spot trends, and the easier and finer one can learn the behaviors the system is exhibiting. As well as running the simulation &lt;em&gt;forwards&lt;/em&gt; to try to see what's coming, we can run it &lt;em&gt;backwards&lt;/em&gt; to learn and refine parameters that can then be used to improve prediction.&lt;/p&gt;
&lt;p&gt;Therefore the third think computer science can tell us is that the financial markets as a whole are potentially a rich source of machine learning and statistical inference, which can be conducted using standard techniques from the semantic web.&lt;/p&gt;
&lt;p&gt;Furthermore, we can conduct simulations in the open. If banks have to represent their holdings as code, and have to link to (some of) the metadata associated with a loan, then regulators can run simulations and publish their results. There's a problem of commercial confidentiality, of course, but one can lock-down the fine detail of metadata if required (identifying owners by postcode and without names, for example). If each person, asset and loan has a unique identifier, it's easier to spot cross-collateralisations and other factors that weaken the values of instruments, without needing to be able to look inside the asset entirely. This exposes a bank's holdings in metadata terms -- residential loans in particular areas -- but that's probably no bad thing, given that the lack of information about securitise contributed to the fall.&lt;/p&gt;
&lt;p&gt;This is the last thing computer science can tell us. Open-source development suggests that having more eyes on a problem reduces the number, scope and severity of bugs, and allows for re-use and re-purposing far beyond what might be expected &lt;em&gt;a priori&lt;/em&gt;. For a market, more eyes means a better regulatory and investor understanding of banks' positions, and so (in principle) a more efficient market.&lt;/p&gt;
&lt;p&gt;For all I know, banks already do a lot of this internally, but making it an open process could go a long way to restore confidence in both taxpayers and future investors. There's no time like a crash to try out new ideas.&lt;/p&gt;&lt;/div&gt;</description><category>Blog</category><category>e-science</category><category>finance</category><category>ireland</category><category>nama</category><guid>https://simondobson.org/blog/2010/04/09/computer-science-financial-crisis/</guid><pubDate>Fri, 09 Apr 2010 18:55:11 GMT</pubDate></item><item><title>The computer is the new microscope</title><link>https://simondobson.org/blog/2010/04/01/computer-microscope/</link><dc:creator>Simon Dobson</dc:creator><description>&lt;div&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;What contributions can computer scientists uniquely make to the latest scientific challenges? The answer may require us to step back and look at how instruments affect science, because the computer is the key instrument for the scientific future.&lt;/p&gt;
&lt;!--more--&gt;

&lt;p&gt;In the late seventeenth century, science was advancing at an extraordinary rate -- perhaps the greatest rate until modern times. The scientists of this era were attempting to re-write the conceptual landscape through which they viewed the universe, and so in many ways were attempting something far harder than we typically address today, when we have a more settled set of concepts that are re-visited only periodically. This also took place in a world with far less of a support infrastructure, in which the scientists were also forced to be tool-makers manufacturing the instruments they needed for their research. It's revealing to look at a list of scientists of this era who were also gifted instrument-makers: Newton, Galileo, Hooke, Huygens and so on.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://en.wikipedia.org/wiki/Antonie_van_Leeuwenhoek"&gt;Antonie van Leeuwenhoek&lt;/a&gt; is a classic example. He revolutionised our understanding of the world by discovering single-celled micro-organisms, and by documenting his findings with detailed drawings in letters to the Royal Society. The key instrument in his research was, of course, the microscope, of which he manufactured an enormous number. Whilst microscopes were already known, van Leeuwenhoek developed (and kept secret) new techniques for the manufacture of lenses which allowed him significantly to advance both the practice of optics and the science of what we would now term microbiology.&lt;/p&gt;
&lt;p&gt;The important here is not that early scientists were polymaths, although that's also a fascinating topic. What's far more important is the effect that tooling has on science. New instruments not only provide tools with which to conduct science; they also open-up new avenues &lt;em&gt;for&lt;/em&gt; science by revealing phenomena that haven't been glimpsed before, or by providing measurements and observations of details that conflict with the existing norms. The point is that &lt;em&gt;tools and science progress together&lt;/em&gt;, and therefore that &lt;em&gt;advances in instrument-making are valuable both in their own right and in the wider science they facilitate&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Not all experimental scientists see things this way. It's fairly common for those conducting the science to look down on the instrument-makers as mere technicians, whose efforts are of a different order to those involved in "doing" the science. It's fair to say that the scientists of the seventeenth century wouldn't share (or indeed understand) this view, since they were in a sense much closer to the instrument as a contributor to their research. Looked at another way, new experiments then &lt;em&gt;typically&lt;/em&gt; required new instruments, rather than as now generally being conducted with a standard set of tools the researcher has to hand.&lt;/p&gt;
&lt;p&gt;What are the instruments today whose advance will affect the wider scientific world? "Traditional" instrument-making is still vitally important, of course, and we can even regard the &lt;a href="http://www.lhc.ac.uk"&gt;LHC&lt;/a&gt; as a big instrument to used in support of particular experiments. But beyond this we have "non-traditional" instruments, of which computers are by far the most commonplace and potentially influential.&lt;/p&gt;
&lt;p&gt;I've talked &lt;a href="https://simondobson.org/2010/03/both-ends-of-the-data-intensive-spectrum/"&gt;previously&lt;/a&gt; about exabyte-scale science and the ways in which new computing techniques will affect it. Some experimenters overlook the significance of computational techniques -- or, if they &lt;em&gt;do&lt;/em&gt; see them, regard them as making technician-level rather than science-level contributions to knowledge. Even more experimenters overlook the impact that more rarefied computer science concerns such as programming languages, meta-data and search have on the advancement of knowledge. These views are restricted, restricting, and (in the worst case) stifling. They are also short-sighted and incorrect.&lt;/p&gt;
&lt;p&gt;At the large scale, computational techniques often offer the &lt;em&gt;only way&lt;/em&gt; of "experimenting" with large-scale data. They can be used to confirm hypotheses in the normal sense, but there are also examples where they have served to help derive new hypotheses by illuminating factors and phenomena in the data that were previously unsuspected, and furthermore could not have been discovered by any other means. The science is advanced by the application of large-scale computing to large-scale data, possibly collected for completely different purposes.&lt;/p&gt;
&lt;p&gt;In that sense the computer is behaving as an instrument that opens-up new opportunities in science: &lt;em&gt;as the new microscope&lt;/em&gt;, in fact. This is &lt;em&gt;not&lt;/em&gt; simply a technical contribution to improving the way in which traditional science is done: coupled with simulation, it changes both &lt;em&gt;what&lt;/em&gt; science is done and &lt;em&gt;how&lt;/em&gt; it is done, and also opens-up new avenues for both traditional and non-traditional experiments and data collection. A good example is in climate change, where large-scale simulations of the atmosphere can confirm hypotheses, suggest new ones, and direct the search for real-world mechanisms that can confirm or refute them.&lt;/p&gt;
&lt;p&gt;At the other end of the scale, we have sensor networks. Sensor networks will allow experimental scientists directly to collect data "in the wild", at high resolution and over long periods -- things that're difficult or impossible with other approaches. This is &lt;em&gt;the computer as the new microscope&lt;/em&gt; again: providing a view of things that were previously hidden. This sort of data collection will become much more important as we try to address (for example) climate change, for which high-resolution long-term data collected on land and in water nicely complement larger-scale space-based sensing. Making such networks function correctly and appropriately is a significant challenge that can't be handled as an after-thought.&lt;/p&gt;
&lt;p&gt;At both scales, much of the richness in the data comes from the ways it's linked and marked-up so as to be searched, traversed and reasoned-with. While some experimental scientists develop strong computational techniques, &lt;em&gt;very&lt;/em&gt; few are expert in metadata, the semantic web, machine learning and automated reasoning -- although these computer science techniques are all key to the long-term value of large-scale data.&lt;/p&gt;
&lt;p&gt;As with the earliest microscopes, the instrument-maker may also be the scientist, but that causes problems perhaps more severe today than in past centuries. Like it or not, we live in an era of specialisation, and in an era where it's impossible to be really expert in &lt;em&gt;one&lt;/em&gt; field let alone the several one might need in order to make proper contributions. But the development of new instruments -- computational techniques, sensing, reasoning, matadata cataloguing  -- is nevertheless key to the development of science. In the years after van Leeuwenhoek, several microbiologists formed close collaborations with opticians who helped refine and develop the tools and techniques available -- allowing the microbiologists to focus on their science while the opticians focused on their instruments. (Isn't it interesting how "focused" really is the appropriate metaphor here?) Looked at broadly, it's hard to say which group's contribution was more influential, and in some senses that's the wrong question: both focused on what they were interested in, solving hard conceptual, experimental and technological problems along the way, and influencing and encouraging each other to perfect their crafts.&lt;/p&gt;
&lt;p&gt;It's good to see this level of co-operation between computer scientists and biologists, engineers, sustainable-development researchers and the rest beginning to flower again, at both ends of the scale (and at all points in between). It's easy to think of instruments as technical devices devoid of scientific content, but it's better to think of them both as having a core contribution to make to the concepts and practice of science, and as having a fascination in their own right that gives rise to a collection of challenges that, if met, will feed-back and open-up new scientific possibilities. The microscope is a key example of this co-evolution and increased joint value from the past and present: the computer is the new microscope for the present and future.&lt;/p&gt;&lt;/div&gt;</description><category>Blog</category><category>e-science</category><category>strategy</category><guid>https://simondobson.org/blog/2010/04/01/computer-microscope/</guid><pubDate>Thu, 01 Apr 2010 15:49:46 GMT</pubDate></item><item><title>Both ends of the data-intensive spectrum</title><link>https://simondobson.org/blog/2010/03/22/both-ends-of-the-data-intensive-spectrum/</link><dc:creator>Simon Dobson</dc:creator><description>&lt;div&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Data-intensive science (or "web science" as it is sometimes known) has received a major boost from the efforts of Googlle and others, with the availability of enormous data sets against which we can learn. It's easy to see that such large-scale data affects experimental science, but there are lessons further down the scale too.&lt;/p&gt;
&lt;!--more--&gt;
&lt;p&gt;I spent part of last week at a workshop on data-intensive computing hosted by the &lt;a href="http://www.nesc.ac.uk"&gt;National e-Science Centre&lt;/a&gt; in Edinburgh. It was a fscinating meeting, and I very grateful for having been invited. Most of the sessions focussed on the challenges of petabyte and exabyte data, but it struck me that many of the points were actually relative rather than absolute: problems that are data-intensive because they have large amounts of data relative to the processing power you can deploy against them. This got me thinking to what extent the characteristics of data-intensive systes can be applied to sensor systems too.&lt;/p&gt;
&lt;p&gt;One of the most interesting points was made early on, by &lt;a href="http://www.sdss.jhu.edu/~szalay/"&gt;Alex Szalay&lt;/a&gt; of the &lt;a href="http://www.sdss.org/"&gt;Sloan Digital Sky Survey&lt;/a&gt;, who set out some desiderata for data intensive computing first made by the late &lt;a href="http://en.wikipedia.org/wiki/Jim_Gray_%28computer_scientist%29"&gt;Jim Gray&lt;/a&gt; -- "Gray's laws":
&lt;/p&gt;&lt;ol&gt;
    &lt;li&gt; Scientific computing revolves around data — &lt;em&gt;not&lt;/em&gt; computing&lt;/li&gt;
    &lt;li&gt; Build solutions that intrisically can scale-out  as required&lt;/li&gt;
    &lt;li&gt; Take the analysis to  the data — because the interesting data's almost certainly too big to move, even with  fast backbones&lt;/li&gt;
    &lt;li&gt; Start with asking for"20 queries" — the most important questions— and recognise that the first 5 will be  by far the most important&lt;/li&gt;
    &lt;li&gt; Go from "working to working" — assume that  the infrastructure will change every 2 — 5 years, and  design hardware and software accordingly&lt;/li&gt;
&lt;/ol&gt;
This is advice hard-won from practice, and it's easy to see how it affects the largest-scale systems. I think Gray's laws also work at the micro-scale of sensor networks, and at points in between.
&lt;p&gt;Data-intensive science is perhaps better envisioned as data-&lt;em&gt;driven&lt;/em&gt; science, in which the data drives the design and evolution computation. This view unifies large- and small-scales: a sensor network needs to respond to the observations it makes of the phenomane it's sensing, even though the scale of the data (for an individual node) is so small.&lt;/p&gt;
&lt;p&gt;By focusing on networking we can scale-out solutions, but we also need to consider that several different networks may be needed to take in the different aspects of systems being observed. It's a &lt;a href="https://simondobson.org/2010/03/things-that-wont-change/"&gt;mistake&lt;/a&gt; to think that we can grow the capabilities of individual nodes too much, since that starts to eat into power budgets. At a data centre level, scale-out tends to mean virtualisation and replication: proven parallel-processing design patterns. At a network level, though, I suspect it means composition and co-design, which we understand substantially less well.&lt;/p&gt;
&lt;p&gt;Taking the analysis to the data means pushing processing down into the network and reducing the amount of raw data that needs to be returned to a base station. This is a slightly contentious point: do I &lt;em&gt;want&lt;/em&gt; to limit the raw data I collect, or should I grab it all in case something emerges later that I need to check against a raw stream? In other words, can I define the information I want to extract from the data stream sufficiently clearly to avoid storing it all? This same point was made by &lt;a href="http://www.roe.ac.uk/~afh/"&gt;Alan Heavens&lt;/a&gt; at the meeting, pointing out that one can do radical data discarding if one has a strong enough model of the pheonmenon against which to evaluate the raw data. Again, the point may be the scale if the data &lt;em&gt;relative to the platform on which it's being processed&lt;/em&gt; rather than in any absolute sense: astonomical data is too, well, "astronomical" to retain even in a large data centre, while sensor data is large relative to node capabilities. It's an open question whether many systems have strong enough data models to support aggressive discarding, though.&lt;/p&gt;
&lt;p&gt;The "20 queries" goal is I think key to many things: identify the large questions and address them first. (Richard Hamming &lt;a href="http://www.cs.virginia.edu/~robins/YouAndYourResearch.html"&gt;made a similar point&lt;/a&gt; with regard to research as a whole.) Coupling sensing research to the science (and public policy formation) that needs it is the only way to do this, and strikes me as at least as important as theoretical advances in network sensing science. The engineering challenges of (for example) putting a sensor network into a field are at least as valuable -- and worthy of support -- as the basic underpinnings.&lt;/p&gt;
&lt;p&gt;The coupling of computer and physical science also speaks the the need for designing systems for upgrade. The techniques for doinjg this -- component design and so forth -- are well-explored by computer scientists and under-understood by many practitioners from other disciplines. Designing sensor &lt;em&gt;and&lt;/em&gt; data systems for expansion and re-tasking should form a backbone of any research effort.&lt;/p&gt;
&lt;p&gt;So I think Jim Gray's pioneering insights into large-scale data may actually be broader than we think, and might also characterise the individually small-scale -- but collectively widespread -- instrumentation of the physical world. It also suggests that there are end-to-end issues that can usefully form part of the research agenda.&lt;/p&gt;&lt;/div&gt;</description><category>Blog</category><category>e-science</category><category>gray's laws</category><guid>https://simondobson.org/blog/2010/03/22/both-ends-of-the-data-intensive-spectrum/</guid><pubDate>Mon, 22 Mar 2010 11:54:52 GMT</pubDate></item></channel></rss>
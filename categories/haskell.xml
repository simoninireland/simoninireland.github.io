<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Simon Dobson (Posts about haskell)</title><link>https://simondobson.org/</link><description></description><atom:link href="https://simondobson.org/categories/haskell.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2022 &lt;a href="mailto:simon.dobson@computer.org"&gt;Simon Dobson&lt;/a&gt; </copyright><lastBuildDate>Tue, 12 Apr 2022 08:05:43 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Evolving programming languages</title><link>https://simondobson.org/blog/2011/05/20/evolving/</link><dc:creator>Simon Dobson</dc:creator><description>&lt;p&gt;&lt;/p&gt;&lt;div id="heystaks_preview" style="width: 100%;height: 100%"&gt;&lt;/div&gt;
Most programming languages have fixed definitions and hard boundaries. In thinking about building software for domains we don't understand very well, a case can be made for a more relaxed, evolutionary approach to language design.
&lt;!--more--&gt;

&lt;p&gt;I've been thinking a lot about languages this week, for various reasons: mainly about the recurring theme of what are the right programming structures for systems driven by sensors, whether they're pervasive systems or sensor networks. In either case, the structures we've evolved for dealing with desktop and server systems don't feel like they're the right abstractions to effectively take things forward.&lt;/p&gt;
&lt;p&gt;A favourite example is the &lt;tt&gt;if&lt;/tt&gt; statement: first decide whether a condition is true or false, and execute one piece of code or another depending on which it is. In a sensor-driven system we often can't make this determination cleanly because of noise and uncertainty -- and if we can, it's often only probably true, and only for a particular period. So are &lt;tt&gt;if&lt;/tt&gt; statements (and &lt;tt&gt;while&lt;/tt&gt; loops and the like) actually appropriate constructs, when we can't make the decisions definitively?&lt;/p&gt;
&lt;p&gt;Whatever you think of this example (and plenty of people hate it) there are certainly differences between what we want to do between traditional and highly sensorised systems, and consequently how we program them. The question is, how do we work out what the right structures are?&lt;/p&gt;
&lt;p&gt;Actually, the question is broader than this. It should be: how do we improve our ability to develop languages that match the needs of particular computational and conceptual domains?&lt;/p&gt;
&lt;p&gt;Domain-specific languages (DSLs) have a tangled history in computer science, pitched between those who like the idea and those who prefer their programming languages general-purpose and re-usable across a &lt;em&gt;range&lt;/em&gt; of domains. There are strong arguments on both sides: general-purpose languages are more productive to learn and are often more mature, but can be less efficient and more cumbersome to apply; DSLs mean learning &lt;em&gt;another&lt;/em&gt; language that may not last long and will probably have far weaker support, but can be enormously more productive and well-targeted in use.&lt;/p&gt;
&lt;p&gt;In some ways, though, the similarities between traditional languages and DSLs are very strong. As a general rule both will have syntax and semantics defined up-front: they won't be experimental in the sense of allowing experimentation &lt;em&gt;within the language itself&lt;/em&gt;. If we don't know what we're building, does it make sense to be this definite?&lt;/p&gt;
&lt;p&gt;There are alternatives. One that I'm quite keen on is the idea of &lt;a href="https://simondobson.org/2010/05/languages-extensible-vms/" target="_blank"&gt;extensible virtual machines&lt;/a&gt;, where the primitives of a language are left "soft" to be extended as required. This style has several advantages. Firstly, it encourages experimentation by not forcing a strong division of concepts between the language we write (the target language) and the language this is implemented in (the host language): the two co-evolve. Secondly, it allows extensions to be as efficient as "base" elements, assuming we can reduce the cost of building new elements appropriately low. Thirdly, it allows multiple paradigms and approaches to co-exist within the same system, since they can share some elements while having other that differ.&lt;/p&gt;
&lt;p&gt;Another related feature is the ability to modify the compiler: that is, don't fix the syntax &lt;em&gt;or&lt;/em&gt; the way in which its handled. So as well as making the low level soft, we also make the high level soft. The advantage here is two-fold. Firstly, we can modify the forms of expression we allow to capture concepts precisely. A good example would be the ability to add concurrency control to a language: the low-level might use semaphores, but programing might demand monitors or some other construct. Modifying the high-level form of the language allows these constructs to be added if required -- and ignored if not.&lt;/p&gt;
&lt;p&gt;This actually leads to the  second advantage, that we can &lt;em&gt;avoid&lt;/em&gt; features we don't want to be available, for example not providing general recursion for languages that need to complete all operations in a finite time. This is something that's surprisingly uncommon in language design despite being common in teaching programming: leaving stuff out can have a major simplifying effect.&lt;/p&gt;
&lt;p&gt;Some people argue that syntax modification is unnecessary in a language that's sufficiently expressive, for example Haskell. I don't agree. The counter-example is actually in Haskell itself, in the form of the &lt;tt&gt;do&lt;/tt&gt; block syntactic sugar for simplifying monadic computations. This &lt;em&gt;had&lt;/em&gt; to be in the language to make it in any way usable, which implied a change of definition, and the monad designers couldn't add it without the involvement of the language "owners", even though the construct is really just a &lt;a href="https://simondobson.org/2010/06/monads-language-design-perspective/" target="_blank"&gt;re-formulation and generalisation of one common in other languages&lt;/a&gt;. There are certainly other areas in which such sugaring would be desirable to make the forms of expression simpler and more intuitive. The less well we understand a domain, the more likely this is to happen.&lt;/p&gt;
&lt;p&gt;Perhaps surprisingly, there are a couple of existing examples of systems that do pretty much what I'm suggesting. Forth is a canonical example (which explains my current work on &lt;a href="http://www.threaded-interpreter.org" target="_blank"&gt;Attila&lt;/a&gt;); Smalltalk is another, where the parser an run-time are almost completely exposed, although abstracted behind several layers of higher-level structure. Both the languages are quite old, have devoted followings, and weakly and dynamically typed -- and may have a lot to teach us about how to develop languages for new domains. They share a design philosophy of allowing a language to &lt;em&gt;evolve&lt;/em&gt; to meet new applications. In Forth, you don't so much write applications as extend the language to meet the problem; in Smalltalk you develop a model of co-operating objects that provide   direct-manipulation interaction through the GUI.&lt;/p&gt;
&lt;p&gt;In both cases the whole language, including the definition and control structures, is built in the language itself &lt;em&gt;via&lt;/em&gt; bootstrapping and cross-compilation. Both languages are compiled, but in both cases the separation between run-time and compile-time are weak, in the sense that the compiler is by default available interactively. Interestingly this doesn't stop you building "normal" compiled applications: cross-compile a system without including the compiler itself, a process that can still take advantage of any extensions added into the compiler without cluttering-up the compiled code. You're unlikely to get strictly the best performance or memory footprint as you might with a mature C compiler, but you &lt;em&gt;do&lt;/em&gt; get advantages in terms of expressiveness and experimentation which seem to outweigh these in a domain we don't understand well. In particular, it means you can evolve the language quickly, easily, and within itself, to explore the design space more effectively and find out whether your wackier ideas are actually worth pursuing further.&lt;/p&gt;</description><category>Blog</category><category>forth</category><category>haskell</category><category>programming</category><category>sensor networks</category><category>smalltalk</category><guid>https://simondobson.org/blog/2011/05/20/evolving/</guid><pubDate>Fri, 20 May 2011 07:00:11 GMT</pubDate></item><item><title>Monads: a language designer's perspective</title><link>https://simondobson.org/blog/2010/06/22/monads-language-design-perspective/</link><dc:creator>Simon Dobson</dc:creator><description>&lt;p&gt;&lt;/p&gt;&lt;p&gt;Monads are one of the hottest topics in functional programming, and arguably simplify the construction of a whole class of systems. Which makes it surprising that they're so opaque and hard to understand to people who's main experience is in imperative or object-oriented languages.&lt;/p&gt;
&lt;!--more--&gt;

&lt;p&gt;There are a lot of explanations of, and tutorials on, monads, but most of them seem to take one of two perspectives: either start with a concrete example, usually in I/O handling, and work back, or start from the abstract mathematical formulation and work forwards. This sounds reasonable, but apparently neither works well in practice -- at least, judging from the comments one receives from  intelligent and able programmers who happen not to have an extensive functional programming or abstract mathematical background. Such a core concept shouldn't be hard to explain, though, so I thought I'd try a different tack: monads from the perspective of language design.&lt;/p&gt;
&lt;p&gt;In Pascal, C or Java, statements are separated (or terminated) by semicolons. This is usually regarded as a piece of syntax, but let's look at it slightly differently. Think of the semicolon as an operator that takes two program fragments and combines them together to form a bigger fragment. For example:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;
int x = 4;
int y = x * 3;
printf("%d", y);
&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;We have three program fragments. The semicolon operator at the end of the first line takes the fragment on its left-hand side and combines it with the fragment on its right-hand side. Essentially the semicolon defines how the RHS is affected by the code on the LHS: in this case the RHS code is evaluated in an environment that includes a binding of variable x, effectively resolving what is otherwise a free variable. Similarly, the semicolon at the end of the second line causes the third line to be evaluated in an environment that include y. The meaning of the semicolon is hard-wired into the language (C, in this case) and defines how code fragments are sequenced and their effects propagated.&lt;/p&gt;
&lt;p&gt;Now from this perspective, &lt;strong&gt;a monad is a programmable semicolon&lt;/strong&gt;. A monad allows the application programmer, rather than the language designer, to determine how a sequence of code is put together, and how one fragment affects those that come later.&lt;/p&gt;
&lt;p&gt;Let's revert to Haskell. In a slightly simplified form, a monad is a type class with the  following signature:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;
class Monad m where
   return :: a -&amp;gt; m a
   (&amp;gt;&amp;gt;=)  :: m a -&amp;gt; (a -&amp;gt; m b) -&amp;gt; m b
&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;So a monad is a constructed type wrapping-up some underlying element  type that defines two functions, &lt;tt&gt;return&lt;/tt&gt; and &lt;tt&gt;(&amp;gt;&amp;gt;=)&lt;/tt&gt;. The first  function injects an element of the element type into the monadic type.  The second takes an element of the monadic type and a function that maps  an element that monad's element type to some other monadic type, and  returns an element of this second monadic type.&lt;/p&gt;
&lt;p&gt;The simplest example of a monad is Haskell's &lt;tt&gt;Maybe&lt;/tt&gt; type, which represents either a value of some underlying element type  or the absence of a value:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;
data Maybe a = Just a
            | Nothing
&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;tt&gt;Maybe&lt;/tt&gt; is an instance of &lt;tt&gt;Monad&lt;/tt&gt;, simply by virtue of defining the two  functions that the type class needs:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;
instance Monad Maybe where
   return a = Just a
   Just a  &amp;gt;&amp;gt;= f = f a
   Nothing &amp;gt;&amp;gt;= _ = Nothing
&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;tt&gt;return&lt;/tt&gt; injects an element of &lt;tt&gt;a&lt;/tt&gt; into an element of &lt;tt&gt;Maybe a&lt;/tt&gt;.  &lt;tt&gt;(&amp;gt;&amp;gt;=)&lt;/tt&gt; takes an element of &lt;tt&gt;Maybe a&lt;/tt&gt; and a function from &lt;tt&gt;a&lt;/tt&gt; to &lt;tt&gt;Maybe  b&lt;/tt&gt;. If the element of &lt;tt&gt;Maybe a&lt;/tt&gt; it's passed is of the form &lt;tt&gt;Just a&lt;/tt&gt;, it  applies the function to the element value &lt;tt&gt;a&lt;/tt&gt;. If, however, the element is &lt;tt&gt;Nothing&lt;/tt&gt;, it returns &lt;tt&gt;Nothing&lt;/tt&gt; without evaluating the function.&lt;/p&gt;
&lt;p&gt;It's hard to see what this type has to do with sequencing, but bear with me. Haskell provides a &lt;tt&gt;do&lt;/tt&gt; construction which gives rise to code like the following:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;
do v &amp;lt;- if b == 0 then Nothing
                  else Just (a / b)
   return 26 / v
&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Intuitively this looks like a sequence of code fragments, so we might infer that the conditional executes first and binds a value to &lt;tt&gt;v&lt;/tt&gt;, and then the next line computes with that value -- which is in fact what happens, but with a twist. The &lt;em&gt;way&lt;/em&gt; in which the fragments relate is &lt;em&gt;not&lt;/em&gt; pre-defined by Haskell. Instead, the relationship between the fragments is determined by &lt;em&gt;the values of which monad the fragments manipulate&lt;/em&gt; (usually expressed as &lt;em&gt;which monad the code executes in&lt;/em&gt;). The &lt;tt&gt;do&lt;/tt&gt; block is just syntactic sugar for a stylised use of the two monad functions. The example above expands to:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;
(if b == 0 then Nothing else Just (a / b)) &amp;gt;&amp;gt;= (\v -&amp;gt; return (26 / v))
&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;So the &lt;tt&gt;do&lt;/tt&gt; block is syntax that expands into user-defined code, depending on the monad that the expressions within it use. In this case, we execute the first expression and then compose it with the function on the right-hand side of the &lt;tt&gt;(&amp;gt;&amp;gt;=)&lt;/tt&gt; operator. The definition says that, if the left-hand side value is &lt;tt&gt;Just a&lt;/tt&gt;, the result is that we call the RHS passing the element value; if the LHS is &lt;tt&gt;Nothing&lt;/tt&gt;, we return &lt;tt&gt;Nothing&lt;/tt&gt; immediately. The result is that, if &lt;em&gt;any&lt;/em&gt; code fragment in the computation returns &lt;tt&gt;Nothing&lt;/tt&gt;, then the &lt;em&gt;entire computation&lt;/em&gt; returns &lt;tt&gt;Nothing&lt;/tt&gt;, since all subsequent compositions will immediately short-circuit: the &lt;tt&gt;Maybe&lt;/tt&gt; type acts like a simple exception that escapes from the computation immediately &lt;tt&gt;Nothing&lt;/tt&gt; is encountered. So the monad structure introduces what's normally regarded as a control construct, &lt;em&gt;entirely within the language&lt;/em&gt;. It's fairly easy to see that we could provide "real" exceptions by hanging an error code off the failure value. It's also fairly easy to see that the monad sequences the code fragments and aborts when one of the "fails". In C we can think of the same function being provided by the semicolon "operator", but with the crucial difference that it is the &lt;em&gt;language&lt;/em&gt;, and not the &lt;em&gt;programmer&lt;/em&gt;,&lt;em&gt; &lt;/em&gt;that decides what happens, one and for all. Monads reify the control of sequencing into the language.&lt;/p&gt;
&lt;p&gt;To see how this can be made more general, let's think about another monad: the list type constructor. Again, to make lists into monads we need to define &lt;tt&gt;return&lt;/tt&gt; and &lt;tt&gt;(&amp;gt;&amp;gt;=)&lt;/tt&gt; with appropriate types. The obvious injection is to turn a singleton into a list:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;
instance Monad [] where
   return a = [a]
&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The definition of &lt;tt&gt;(&amp;gt;&amp;gt;=)&lt;/tt&gt; is slightly more interesting: which function of type &lt;tt&gt;[a] -&amp;gt; (a -&amp;gt; [b]) -&amp;gt; [b]&lt;/tt&gt; is appropriate? One could choose to select an element of the &lt;tt&gt;[a]&lt;/tt&gt; list at random and apply the function to it, giving a list &lt;tt&gt;[b]&lt;/tt&gt; -- a sort of non-deterministic application of a function to a set of possible arguments. (Actually this might be interesting in the context of &lt;a href="https://simondobson.org/2010/03/five-big-questions/"&gt;programming with uncertainty&lt;/a&gt;, but that's another topic.) Another definition -- and the one that Haskell actually chooses -- is to apply the function to all the elements of &lt;tt&gt;[a]&lt;/tt&gt;, taking each a to a list &lt;tt&gt;[b]&lt;/tt&gt;, and then concatenating the resulting lists together to form one big list:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;
   l &amp;gt;&amp;gt;= f = concat (map f l)
&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;What happens to the code fragments in a do block now? The monad threads them together using the two basic functions. So if we have code such as:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;
do x &amp;lt;- [1..10]
   y &amp;lt;- [20..30]
   return (x, y)
&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;What happens? The first and second fragments clearly define lists, but what about the third, which seems to define a pair? To see what happens, we need to consider &lt;em&gt;all&lt;/em&gt; the fragments together. Remember, each fragment is combined with the next by applying &lt;tt&gt;concat (map f l)&lt;/tt&gt;. If we expand this out, we get:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;
concat (map (\x -&amp;gt; concat (map (\y -&amp;gt; return (x, y)) [20..30])) [1..10])
&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;So to summarise, Haskell provides a do block syntax that expands to a nested sequence of monadic function calls. The actual functions used depend on the monadic type in the do block, so the programmer can define how the code fragments relate to one another. Common monads include some simple types, but also I/O operations and state bindings, allowing Haskell to perform operations that are typically regarded as imperative without losing its laziness. The Haskell tutorial &lt;a href="http://www.haskell.org/tutorial/io.html"&gt;explains the I/O syntax&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;What can we say about monads from the perspective of language design? Essentially they reify sequencing, in a functional style. They only work as seamlessly as they do because of Haskell's flexible type system (allowing the definition of new monads), and also because of the &lt;tt&gt;do&lt;/tt&gt; syntax: without the syntactic sugar, most monadic code is incomprehensible. The real power is that they allow some very complex functionality to be abstracted into functions and re-used. Consider the &lt;tt&gt;Maybe&lt;/tt&gt; code we used earlier: without the "escape" provided by the &lt;tt&gt;Maybe&lt;/tt&gt; monad, we'd have to guard each statement with a conditional to make sure there wasn't a &lt;tt&gt;Nothing&lt;/tt&gt; returned at any point. This quickly gets tiresome and error-prone: the monad encapsulates and enforces the desired behaviour. When you realise that one can also compose monads using monad transformers, layering monadic behaviours on top of each other (albeit with some contortions to keep the type system happy), it becomes clear that this is a very powerful capability.&lt;/p&gt;
&lt;p&gt;I think one can also easily identify a few drawbacks, though. One that immediately springs to mind is that monads reify &lt;em&gt;one&lt;/em&gt; construction, of the many that one might choose. A more general meta-language, like the use of meta-objects protocols or aspects, or structured language and compiler extensions, would allow even more flexibility. A second -- perhaps with wider impact -- is that one has to be intimately familiar with the monad being used before one has the &lt;em&gt;slightest idea&lt;/em&gt; what a piece of code will do. The list example above is not obviously a list comprehension, until one recognises the "idiom" of the list monad. Thirdly, the choice of monadic function definitions isn't often canonical, so there can be a certain arbitrariness to the behaviour. It'd be interesting to consider generalisations of monads and language constructs to address these issues, but for the meantime one can use them to abstract a whole range of functionality in an interesting way. Good luck!&lt;/p&gt;</description><category>Blog</category><category>haskell</category><category>monads</category><category>programming</category><guid>https://simondobson.org/blog/2010/06/22/monads-language-design-perspective/</guid><pubDate>Tue, 22 Jun 2010 12:20:08 GMT</pubDate></item><item><title>Languages for extensible virtual machines</title><link>https://simondobson.org/blog/2010/05/28/languages-extensible-vms/</link><dc:creator>Simon Dobson</dc:creator><description>&lt;p&gt;&lt;/p&gt;&lt;p&gt;Many languages have an underlying virtual machine (VM) to provide a more portable and convenient substrate for compilation or interpretation. For language research it's useful to be able to generate custom VMs and other language tools for different languages. Which raises the question: what's the appropriate  language for writing experimental languages?&lt;/p&gt;
&lt;!--more--&gt;

&lt;p&gt;What I have in mind is slightly more than just VMs, and more a platform for experimenting with language design for novel environments such as sensor-driven systems. As well as a runtime, this requires the ability to parse, to represent and evaluate type and semantic rules, and to provide a general framework for computation that can then be exposed into a target language as constructs, types and so forth. What's the right language in which to do all this?&lt;/p&gt;
&lt;p&gt;This isn't a simple question. It's well-accepted that the correct choice of language is vital to the success of a coding project.  One could work purely at the language level, exploring constructs and type systems without any real constraint of the real world (such as being runnable on a sensor mote). This has to some extent been traditional in programming language research, justified by the Moore's law increases in performance of the target machines. It isn't justifiable for sensor networks, though, where &lt;a href="https://simondobson.org/2010/03/things-that-wont-change/"&gt;we won't see the same phenomenon&lt;/a&gt;. If we want to prototype realistic language tools in the same framework, we need at least a run-time VM that was appropriate for these target devices; alternatively we could ignore this, focus on the language, and prototype only when we're happy with the structures, using a different framework. My gut ffeeling is that the former is preferable, if it's possible, for reasons of conceptual clarity, impact and simplicity. But even without making this decision we can consider the features of different candidate language-writing languages:
&lt;/p&gt;&lt;h3&gt;C&lt;/h3&gt;
The most obvious approach is to use C, which is run-time-efficient and runs on any potential platform. For advanced language research, though, it's less attractive because of its poor symbolic data handling. That makes it harder to write type-checking sub-systems and the like, which are essentially symbolic mathematics.
&lt;h3&gt;Forth&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://simondobson.org/2010/03/forth-for-sensors/"&gt;I've wondered about Forth before&lt;/a&gt;. At one level it combines the same drawbacks as C -- poor symbolic and dynamic data handling -- with the additional drawback of being unfamiliar to almost everyone.&lt;/p&gt;
&lt;p&gt;Forth &lt;em&gt;does&lt;/em&gt; have some redeeming features, though. Firstly, threaded interpretation means that additional layers of abstraction are largely cost-free: they run at the same speed as the language itself. Moreover there's a sense in which threaded interpretation blurs the distinction between host language and meta-language: you don't write Forth applications, you extend it towards the problem, so the meta-language &lt;em&gt;becomes&lt;/em&gt; the VM and language tool. This is something that needs some further exploration.&lt;/p&gt;
&lt;h3&gt;Scheme&lt;/h3&gt;
&lt;p&gt;Scheme's advantages are its simplicity, regularity, and pretty much unrivalled flexibility in handling symbolic data. There's &lt;a href="https://simondobson.org/2010/05/cs-book-worth-reading-twice/"&gt;a long  tradition of Scheme-based language tooling&lt;/a&gt;, and so a lot of experience and libraries to make use of. It's also easy to write purely functional code, which can aid re-use.&lt;/p&gt;
&lt;p&gt;Scheme is dynamically typed, which can be great when exploring approaches like partial evaluation (specialising an interpreter against a particular piece of code to get a compiled program, for example).&lt;/p&gt;
&lt;h3&gt;Haskell&lt;/h3&gt;
&lt;p&gt;In some ways, Haskell is the obvious language for a new language project. The strong typing, type classing and modules mean one can generate a typed meta-language. There are lots of libraries and plenty of activity in the research community. Moreover Haskell is in many ways the "mathematician's choice" of language, since one can often map mathematical concepts almost directly into code. Given thaat typing and semantics are just mathematical operations over symbols, this is a significant attraction.&lt;/p&gt;
&lt;p&gt;Where Haskell falls over, of course, is its runtime overheads -- mostly these days in terms of memory rather than performance. It essentially mandates a choice of target platform to be fairly meaty, which closes-off some opportunities. There are some "staged" Haskell strategies that might work around this, and one could potentially stage the code to another runtime virtual machine. Or play games like implement a Forth VM inside Haskell for experimentation, and then emit code for a &lt;em&gt;different&lt;/em&gt; Forth implementation for runtime.&lt;/p&gt;
&lt;h3&gt;Java&lt;/h3&gt;
&lt;p&gt;Java remains the language &lt;em&gt;du jour&lt;/em&gt; for most new projects. It has decent dynamic data handling, poor symbolic data handling, fairly large run-time overheads and a well-stocked library for re-use. (Actually I used Java for &lt;a href="https://simondobson.org/publications/#Vanilla-GCSE99"&gt;Vanilla&lt;/a&gt;, an earlier project in a similar area.) Despite the attractions, Java feels wrong. It doesn't provide a good solution to &lt;em&gt;any&lt;/em&gt; of the constraints, and would be awkward as a platform for manipulating rules-based descriptions.&lt;/p&gt;
&lt;h3&gt;Smalltalk&lt;/h3&gt;
&lt;p&gt;Smalltalk -- and especially &lt;a href="http://www.squeak.org"&gt;Squeak&lt;/a&gt; -- isn't a popular choice within language research, but does have a portable virtual machine, VM generation, and other nice features and libraries. The structure is also attractive, being modern and object-oriented. It's also a good platform for building interactive systems, so one could do simulation, visual programming and the like within the same framework -- something that'd be much harder with other choices. There are also some obvious connectionns between Smalltalk and pervasive systems, where one is talking about the interactions of objects in the real world.&lt;/p&gt;
&lt;p&gt;Where does that leave us? Nowhere, in a sense, other than with a list of characteristics of different candidate languages for language research. It's unfortunate there isn't a clear winner; alternatively, it's positive that there's a choice depending on the final direction. The worry has to be that a project like this is a moving target that moves away from the areas of strength for any choice made.&lt;/p&gt;</description><category>Blog</category><category>forth</category><category>haskell</category><category>moore's law</category><category>programming</category><category>smalltalk</category><category>virtual machine</category><guid>https://simondobson.org/blog/2010/05/28/languages-extensible-vms/</guid><pubDate>Fri, 28 May 2010 05:00:30 GMT</pubDate></item></channel></rss>
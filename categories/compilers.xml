<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Simon Dobson (Posts about compilers)</title><link>https://simondobson.org/</link><description></description><atom:link href="https://simondobson.org/categories/compilers.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2025 &lt;a href="mailto:simoninireland@gmail.com"&gt;Simon Dobson&lt;/a&gt; </copyright><lastBuildDate>Thu, 23 Jan 2025 17:16:05 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Parser-centric grammar complexity</title><link>https://simondobson.org/2012/10/09/parsers/</link><dc:creator>Simon Dobson</dc:creator><description>&lt;p&gt;We usually think about formal language grammars in terms of the complexity of the &lt;em&gt;language&lt;/em&gt;, but it's also possible -- and potentially more useful, for a compiler-writer or other user -- to think about them from the perspective of the &lt;em&gt;parser&lt;/em&gt;, which is slightly different.

&lt;!--more--&gt;

I'm teaching grammars and parsing to a second-year class at the moment, which involves getting to grips with the ideas of formal languages, their representation, using them to recognise ("parse") texts that conform to their rules. Simultaneously I'm also looking at programming language grammars and how they can be simplified. I've realised that these two activities don't quite line up the way I expected.

Formal languages are important for computer scientists, as they let us describe the texts allowed for valid programs, data files and the like. The understanding of formal languages derives from linguistics, chiefly the work of Noam Chomsky in recognising the &lt;a href="https://en.wikipedia.org/wiki/Chomsky_hierarchy" target="_blank"&gt;hierarchy of languages&lt;/a&gt; induced by very small changes in the capabilities of the underlying grammar: regular languages, context-free and context-sensitive, and so forth. Regular and context-free languages have proven to be the most useful in practice, with everything else being a bit too complicated for most uses.

The typical use of grammars in (for example) compilers is that we define a syntax for the language we're trying to recognise (for example Java) using a (typically context-free) grammar. The grammar states precisely what constitutes a syntactically correct Java program and, conversely, rejects syntactically incorrect programs. This parsing process is typically purely syntactic, and says nothing about whether the program makes sense semantically. The result of parsing is usually a parse tree, a more computer-friendly form of the program that's then used to generate code after some further analysis and optimisation.

In this process we start from the grammar, which is generally quite complex: Java is a complicated system to learn. It's important to realise that many computer languages aren't like this at all: they're a lot simpler, and consequently can have parsers that are extremely simple, compact, and don't need much in the way of a formal grammar: you can build them from scratch, which would be pretty much impossible for a Java parser.

This leads to a different view of the complexity of languages, based on the difficulty of writing a parser for them -- which doesn't quite track the Chomsky hierarchy. Instead you end up with something like this:

&lt;strong&gt;Isolated.&lt;/strong&gt; In languages like Forth, programs are composed of symbols separated by whitespace and the language says nothing about the ways in which they can be put together. Put another way, each symbol is recognised and responded to &lt;em&gt;on its own&lt;/em&gt; rather than as part of a larger construction. In Java terms, that's like giving meaning to the &lt;tt&gt;{&lt;/tt&gt; symbol independent of whether it's part of a &lt;tt&gt;class&lt;/tt&gt; definition or a &lt;tt&gt;for&lt;/tt&gt; loop.

It's easy to see how simple this is for the compiler-writer: we simply look-up each symbol extracted from the input and execute or compile it, without reference to anything else. Clearly this only works to some extent: you only expect to see an else branch somewhere after a corresponding if statement, for example. In Forth this isn't a matter of grammar, but rather of compilation: there's an extra-grammatical mechanism for testing the sanity of the control constructs as part of their execution. While this may sound strange (and is, really) it means that it's easy to extend the syntax of the language -- because it doesn't really have one. Adding new control constructs means adding symbols to be parsed &lt;em&gt;and&lt;/em&gt; defining the appropriate control structure to sanity-check their usage. The point is that we can simplify the parser well below the level that might be considered normal and still get a usable system.

&lt;strong&gt;Incremental.&lt;/strong&gt; The next step up is to allow the execution of a symbol to take control of the input stream itself, and define what it consumes &lt;em&gt;itself&lt;/em&gt;. A good example of this is from Forth again, for parsing strings. The Forth word introducing a literal string is &lt;tt&gt;S"&lt;/tt&gt;, as in &lt;tt&gt;S" hello world"&lt;/tt&gt;. What happens with the &lt;tt&gt;hello world"&lt;/tt&gt; part of the text isn't defined by a grammar, though: it's defined by &lt;tt&gt;S"&lt;/tt&gt;, which takes control of the parsing process and consumes the text it wants before returning control to the parser. (In this case it consumes all characters until the closing quotes.)  Again this means we can define new language constructs, simply by having words that do their own parsing.

Interestingly enough, these parser words could themselves use a grammar that's different from that of the surrounding language -- possibly using a standard parser generator. The main parser takes a back seat while the parser word does its job, allowing for arbitrary extension of the syntax of the language. The disadvantage of course is that there's no central definition of what constitutes "a program" in the language -- but that's an advantage too, certainly for experimentation and extension. It's the &lt;a href="https://simondobson.org/2012/10/typewheel/" target="_blank"&gt;ideas of dynamic languages&lt;/a&gt; extended to syntax, in a way.

&lt;strong&gt;Structured.&lt;/strong&gt; Part of the subtlety of defining grammars is avoid &lt;em&gt;ambiguity&lt;/em&gt;, making sure that every program can be parsed in a well-defined and unique way. The simplest way to avoid ambiguity is to make everything structured and standard. Lisp and Scheme are the best illustrations of this. Every expression in the language takes the same form: an atom, or a list whose elements may be atoms or other lists. Lists are enclosed in brackets, so it's always possible to find the scope of any particular construction. It's extremely easy to write a parser for such a language, and the "grammar" fits onto about three lines of description.

Interestingly, this sort of language is also highly extensible, as all constructs look the same. Adding a new control construct is straightforward &lt;em&gt;as long as it follows the model&lt;/em&gt;, and again can be done extra-grammatically without defining new elements to the parser. One is more constrained than with the isolated or incremental models, but this constraint means that there's also more scope for controlled expansion. Scheme, for example, provides a macro facility that allows new syntactic forms, within the scope and style of the overall parser, that nevertheless behave differently to "normal" constructs: they can capture and manipulate fragments of program text and re-combine them into new structures using &lt;a href="https://en.wikipedia.org/wiki/Quasi-quotation" target="_blank"&gt;quasi-quoting&lt;/a&gt; and other mechanisms. One can provide these facilities quite uniformly without difficulty, and without explicit syntax. (This is even true for quasi-quoting, although there's usually some syntactic sugar to make it more usable.) The results will always "look like Lisp", even though they might behave rather differently: again, we limit the scope of what is dealt with grammatically and what is dealt with programmatically, and get results that are somewhat different to those we would get with the standard route to compiler construction.

This isn't to say that we should give up on grammars and go back to more primitive definitions of languages, but just to observe that grammars evolved to suit a particular purpose that needs to be continually checked for relevance. Most programmers find Java (for example) easier to read than Scheme, despite the latter's more straightforward and regular syntax: simplicity is in the eye of the beholder, not the parser. But a formal grammar may not be the best solution in situations where we &lt;em&gt;want&lt;/em&gt; variable, loose and extensible syntax for whatever reason, so it's as well to be aware of the alternative that make the problem one of programming rather than of parsing.&lt;/p&gt;</description><category>compilers</category><category>forth</category><category>parsing</category><guid>https://simondobson.org/2012/10/09/parsers/</guid><pubDate>Tue, 09 Oct 2012 09:00:13 GMT</pubDate></item><item><title>Layered abstractions and Russian dolls</title><link>https://simondobson.org/2012/06/14/russian-dolls/</link><dc:creator>Simon Dobson</dc:creator><description>&lt;p&gt;The layering of abstractions has served us well, but it's now generating the sorts of complexity it was designed to solve. Time for a re-think?

&lt;!--more--&gt;Anyone who's built a large piece of software knows that much of the effort is in managing the complexity of the project: which other software a piece of code relies on, how to keep the various aspects separate, how to manage changes and upgrades, and so on. This isn't something that's got easier over time: it has for a given code size and style, as we've understood build processes and dependency management better; but the code sizes have relentlessly increased to compensate for our improved understanding; and modern practices don't make life any easier. Downloaded code, dynamic modules and classes, client-server and the like all generate their own intrinsic complexity.

One of the biggest sources of complexity is the use of multiple applications, especially in enterprise systems. A typical e-commerce system, for example, will make use of a web server to present pages (which themselves might contain embedded JavaScript for client-side processing), a database to track orders and inventory, a procurement system to fulfil orders, and possibly a supply-chain management system to order new inventory. That's the application. Then there'll be the operating system, a logging facility, a facilities management system, and a load of administrative tools and scripts. And the operating system may itself be virtualised and running as a guest within another, host operating system and hypervisor, which needs its own toolset. The interactions between these tools can be mind-boggling.

Someone once asked: who knows how to work the &lt;a href="http://httpd.apache.org/" target="_blank"&gt;Apache web server&lt;/a&gt;? It sounds like a simple question -- any competent web manager? the main developers? -- but the sting in the tail is that Apache is very configurable: so configurable, in fact, that it's pretty much impossible to work out what a given combination of options will do (or, conversely, what combination of options to use to achieve a given effect). The interactions are just too complicated, and the web abounds with examples where interactions between (for example) the thread pool size, the operating system block size, and the Java virtual machine parameters conspire to crash a system that looks like it should be working fine. If you can't work one server properly -- one component of the system -- what hope is there to get a complete system humming along?

&lt;a href="http://blogs.cs.st-andrews.ac.uk/al/" target="_blank"&gt;Al Dearle&lt;/a&gt; and I have been talking about this for a while. The basic issue seems to be an interaction between decomposition and dependency. In other words, the complexity comes at the "seams" between the various sub-systems, and is magnified the more configurable the components on either side of the seam are. This is important, because systems are becoming more finely decomposed: the move to component software, software-as-a-service and the like all increase the number of seams. Al's image of this is that modern systems are like Russian dolls, where each supposedly independent component contains more components that influence the size and complexity of the component containing them. You can only simplify any individual piece so far, because it depends on so many other pieces.

Actually a lot of the seams are now unnecessary anyway. Going back to the e-commerce example, the operating system goes to great pains to provide a process abstraction to keep the components separate -- to stop faults in the database affecting the web server, for example. Historically this made perfect sense and prevented a single faulty process in a time-sharing system affecting the processes of other users. Nowadays, however, it makes considerably less sense, for a number of reasons. Firstly, all the components are owned by a single nominal user (although there are still good reasons for separating the root user from the application user), so the security concerns are less pronounced. Secondly, all the components depend on each other, so a crash in the database will effectively terminate the web server anyway. (We're simplifying, but you get the idea.) Finally, there's a good chance that the web server, database and so on are each running in their own virtual machine, so there's only one "real" process per machine (plus all the supporting processes). The operating system is offering protection that isn't needed, because it's being provided (again) by the hypervisor running the virtual machines and perhaps (&lt;em&gt;again&lt;/em&gt;) by the host operating system(s) involved.

We also tend to build very flexible components (like Apache), which can deal with multiple simultaneous connections, keep users separate, allow modules to be loaded and unloaded dynamically -- behave like small operating systems, in other words, replicating the OS functionality again at application level. This is despite the fact that, in enterprise configurations, you'll probably know in advance the modules to be loaded and have a single user (or small user population) and fixed set of interactions: the flexibility makes the component more complex for no net gain during operation. Although it might simplify configuration and evolution slightly, there are often other mechanisms for this: in a cloud environment one can spin-up a replacement system in an evolved state and then swap the set of VMs over cleanly.

It's easy to think that this makes no difference for modern machines, but that's probably not the case. All these layers still need to be resourced; more importantly, they still need to be managed, maintained and secured, which take time to do well -- with a result that they typically get done badly (if at all).

Can we do anything about it? One thought is that the decomposition that makes thinking about systems and programming easier makes executing those systems more complex and fragile. In many cases, once the system is configured appropriately, flexibility becomes an enemy: it'll often be too complicated to re-configure or optimise in a live environment anyway. There may be a reason to have Russian dolls when &lt;em&gt;designing&lt;/em&gt; a system, but once designed it's better to make each doll solid to remove the possibility of then opening-up and falling apart.

So it's not decomposition that's the issue, it's &lt;em&gt;decomposition manifested at run-time&lt;/em&gt;. When we add new abstractions to systems, we typically add them in the form of components or libraries that can be called from other components. These components are often general, with lots of parameters and working with multiple clients -- sound familiar? This is all good for the component-writer, as it lets the same code be re-used: but it bloats each system that uses the component, adding complexity and interactions.

So one thought for tackling complexity is to change where decomposition manifests itself. If instead of placing new functions in the run-time system, we placed it into the compiler used to build the run-time, we could use compilation techniques to optimise-out the unnecessary functionality so that what results is optimised for the configuration that it's actually being placed in, rather than being general enough to represent any configuration. There's substantial work on these ideas in the fields of staged compilation and partial evaluation (for example &lt;a href="http://www.metaocaml.org/" target="_blank"&gt;MetaOCaml&lt;/a&gt;, &lt;a href="http://www.haskell.org/haskellwiki/Template_Haskell" target="_blank"&gt;Template Haskell&lt;/a&gt;, Flask and the like): the flexibility is manifested at compile-time as compile-time abstractions, that in the course of compilation are removed and replaced with inflexible -- but more efficient and potentially more dependable -- specialised code. Think taking the source code for Linux, Apache and MySQL, accelerating them together at high speed, and getting out a single program that'd run on a bare machine, had nothing it didn't actually need, and had all the options for the various (conceptual) sub-systems set correctly to work together.

Don't believe it's possible? Neither do I. There's too much code and especially too much legacy code for this to work at enterprise (or even desktop) level. However, for embedded systems and sensor networks it's a different story. For these systems, every extra abstraction that makes the programmer's life easier is a menace if it increases the code size hitting the metal: there just isn't the memory. But there also isn't the legacy code base, and there is a crying need for better abstractions. So an approach to the Russian dolls that moves the abstractions out of the run-time and&lt;a href="https://simondobson.org/2011/05/evolving/" target="_blank"&gt; into the languages and compilers&lt;/a&gt; might work, and might considerably improve the robustness and ease of use for many systems we need to develop. It also works well with modern language technology, and with other trends like &lt;a href="https://simondobson.org/2011/12/middleware-doughnut/" target="_blank"&gt;ever-more-specialised middleware&lt;/a&gt; that remove bloat and overhead at the cost of generality. Keeping the former &lt;em&gt;and&lt;/em&gt; the latter seems like a worthwhile goal.&lt;/p&gt;</description><category>compilers</category><category>programming</category><category>software engineering</category><category>virtualisation</category><guid>https://simondobson.org/2012/06/14/russian-dolls/</guid><pubDate>Thu, 14 Jun 2012 07:00:52 GMT</pubDate></item></channel></rss>
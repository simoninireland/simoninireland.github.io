<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Simon Dobson (Posts about moore's law)</title><link>https://simondobson.org/</link><description></description><atom:link href="https://simondobson.org/categories/moores-law.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2024 &lt;a href="mailto:simoninireland@gmail.com"&gt;Simon Dobson&lt;/a&gt; </copyright><lastBuildDate>Wed, 06 Mar 2024 19:17:43 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Languages for extensible virtual machines</title><link>https://simondobson.org/2010/05/28/languages-extensible-vms/</link><dc:creator>Simon Dobson</dc:creator><description>&lt;p&gt;Many languages have an underlying virtual machine (VM) to provide a more portable and convenient substrate for compilation or interpretation. For language research it's useful to be able to generate custom VMs and other language tools for different languages. Which raises the question: what's the appropriate  language for writing experimental languages?

&lt;!--more--&gt;

What I have in mind is slightly more than just VMs, and more a platform for experimenting with language design for novel environments such as sensor-driven systems. As well as a runtime, this requires the ability to parse, to represent and evaluate type and semantic rules, and to provide a general framework for computation that can then be exposed into a target language as constructs, types and so forth. What's the right language in which to do all this?

This isn't a simple question. It's well-accepted that the correct choice of language is vital to the success of a coding project.  One could work purely at the language level, exploring constructs and type systems without any real constraint of the real world (such as being runnable on a sensor mote). This has to some extent been traditional in programming language research, justified by the Moore's law increases in performance of the target machines. It isn't justifiable for sensor networks, though, where &lt;a href="https://simondobson.org/2010/03/things-that-wont-change/"&gt;we won't see the same phenomenon&lt;/a&gt;. If we want to prototype realistic language tools in the same framework, we need at least a run-time VM that was appropriate for these target devices; alternatively we could ignore this, focus on the language, and prototype only when we're happy with the structures, using a different framework. My gut ffeeling is that the former is preferable, if it's possible, for reasons of conceptual clarity, impact and simplicity. But even without making this decision we can consider the features of different candidate language-writing languages:
&lt;/p&gt;&lt;h3&gt;C&lt;/h3&gt;
The most obvious approach is to use C, which is run-time-efficient and runs on any potential platform. For advanced language research, though, it's less attractive because of its poor symbolic data handling. That makes it harder to write type-checking sub-systems and the like, which are essentially symbolic mathematics.
&lt;h3&gt;Forth&lt;/h3&gt;
&lt;a href="https://simondobson.org/2010/03/forth-for-sensors/"&gt;I've wondered about Forth before&lt;/a&gt;. At one level it combines the same drawbacks as C -- poor symbolic and dynamic data handling -- with the additional drawback of being unfamiliar to almost everyone.

Forth &lt;em&gt;does&lt;/em&gt; have some redeeming features, though. Firstly, threaded interpretation means that additional layers of abstraction are largely cost-free: they run at the same speed as the language itself. Moreover there's a sense in which threaded interpretation blurs the distinction between host language and meta-language: you don't write Forth applications, you extend it towards the problem, so the meta-language &lt;em&gt;becomes&lt;/em&gt; the VM and language tool. This is something that needs some further exploration.
&lt;h3&gt;Scheme&lt;/h3&gt;
Scheme's advantages are its simplicity, regularity, and pretty much unrivalled flexibility in handling symbolic data. There's &lt;a href="https://simondobson.org/2010/05/cs-book-worth-reading-twice/"&gt;a long  tradition of Scheme-based language tooling&lt;/a&gt;, and so a lot of experience and libraries to make use of. It's also easy to write purely functional code, which can aid re-use.

Scheme is dynamically typed, which can be great when exploring approaches like partial evaluation (specialising an interpreter against a particular piece of code to get a compiled program, for example).
&lt;h3&gt;Haskell&lt;/h3&gt;
In some ways, Haskell is the obvious language for a new language project. The strong typing, type classing and modules mean one can generate a typed meta-language. There are lots of libraries and plenty of activity in the research community. Moreover Haskell is in many ways the "mathematician's choice" of language, since one can often map mathematical concepts almost directly into code. Given thaat typing and semantics are just mathematical operations over symbols, this is a significant attraction.

Where Haskell falls over, of course, is its runtime overheads -- mostly these days in terms of memory rather than performance. It essentially mandates a choice of target platform to be fairly meaty, which closes-off some opportunities. There are some "staged" Haskell strategies that might work around this, and one could potentially stage the code to another runtime virtual machine. Or play games like implement a Forth VM inside Haskell for experimentation, and then emit code for a &lt;em&gt;different&lt;/em&gt; Forth implementation for runtime.
&lt;h3&gt;Java&lt;/h3&gt;
Java remains the language &lt;em&gt;du jour&lt;/em&gt; for most new projects. It has decent dynamic data handling, poor symbolic data handling, fairly large run-time overheads and a well-stocked library for re-use. (Actually I used Java for &lt;a href="https://simondobson.org/publications/#Vanilla-GCSE99"&gt;Vanilla&lt;/a&gt;, an earlier project in a similar area.) Despite the attractions, Java feels wrong. It doesn't provide a good solution to &lt;em&gt;any&lt;/em&gt; of the constraints, and would be awkward as a platform for manipulating rules-based descriptions.
&lt;h3&gt;Smalltalk&lt;/h3&gt;
Smalltalk -- and especially &lt;a href="http://www.squeak.org"&gt;Squeak&lt;/a&gt; -- isn't a popular choice within language research, but does have a portable virtual machine, VM generation, and other nice features and libraries. The structure is also attractive, being modern and object-oriented. It's also a good platform for building interactive systems, so one could do simulation, visual programming and the like within the same framework -- something that'd be much harder with other choices. There are also some obvious connectionns between Smalltalk and pervasive systems, where one is talking about the interactions of objects in the real world.

Where does that leave us? Nowhere, in a sense, other than with a list of characteristics of different candidate languages for language research. It's unfortunate there isn't a clear winner; alternatively, it's positive that there's a choice depending on the final direction. The worry has to be that a project like this is a moving target that moves away from the areas of strength for any choice made.</description><category>forth</category><category>haskell</category><category>moore's law</category><category>programming</category><category>smalltalk</category><category>virtual machine</category><guid>https://simondobson.org/2010/05/28/languages-extensible-vms/</guid><pubDate>Fri, 28 May 2010 05:00:30 GMT</pubDate></item><item><title>Things that won't change</title><link>https://simondobson.org/2010/03/05/things-that-wont-change/</link><dc:creator>Simon Dobson</dc:creator><description>&lt;p&gt;Technology always advances, and in most areas the rate of change is also increasing all the time. But there are some areas where technological changes either happen only slowly, or even go into reverse. Not something we're used to in computer science, but it's a feature of sensor network programming: what are the challenges that technology &lt;em&gt;won't&lt;/em&gt; solve for us?

&lt;!--more--&gt;Moore's law has been a defining feature of computers for the past decades. By and large computing power has doubled every 18 months at constant price; alternatively, the cost of a unit of computing power has halved in the same period. The effects of this on user experience have been plain to see.

Within computer science, Moore's law has had an effect on research directions too. In starting on a PhD a student can work on a problem that'd at the edge of the performance envelope of whatever class of machine she is targeting -- cellphone, laptop, desktop or server -- secure in the knowledge that, but the time she's coming to an end, the power available to that machine class will have quadrupled. This doesn't open-up &lt;em&gt;every&lt;/em&gt; problem, of course -- a four-times speed-up on an NP-hard search problem might still leave it largely intractable -- but in fields such as middleware, software tools, language design and the like, it's enough to overcome many issues.

It's therefore something of a shock to come to sensor networks and similar systems, because I suspect these systems aren't subject to Moore's law in the usual way.

In some ways, the situation on such small systems is actually &lt;em&gt;better&lt;/em&gt; than in desktops and enterprise computing. At the higher end, we're already hitting at least the suspicion that the performance increases in individual cores will soon start to flatten out. Multicore processors allow us to keep increasing performance, but at the cost of vastly complicating the sorts of programming needed in order to keep all those cores occupied. Sensor motes are still single-core and typically aren't using state-of-the-art processes at that, so there's still plenty of room for manoeuvre.

But it's easy to forget that while the cash cost of a unit of processing power has decreased, the &lt;em&gt;power&lt;/em&gt; cost of that unit hasn't decreased by nearly as much (and may actually have increased). Those twice-as-powerful chips eighteen months on typically burn significantly more power than their predecessors. You only have to look at the size of heatsinks on chips to realise that there's a &lt;em&gt;lot&lt;/em&gt; of heat being dissipated.

So for a sensor network, which is using a battery or scavenging for power,  increasing the processor power will almost certainly decrease lifetime, and that's not a trade-off designers will accept. Battery, scavenging and renewable power sources like solar cells aren't subject to Moore's law: their growth curves are those of physics and traditional engineering, not those of IT systems. Ten years ago my cellphone went for three days without a charge; my new HTC Hero lasts no more than two days, even if I turn off the data services and wifi. The extra compute cost has a severe power cost.

In many sensor applications, the trade-off will actually be in reverse. Given the choice, a designer might opt for two older, less capable but less power-hungry processors over one more powerful but more hungry. Two motes can provide more coverage, or more robustness, or both.

But this exposes a real programming challenge, since it implies that we're going to have to get used to building modern, open, adaptive software on machines whose capabilities are similar to those of a mid-1980's vintage home computer -- and which might in fact even &lt;em&gt;decrease&lt;/em&gt; over time, since the driving forces are pushing for coverage, lifetime and redundant replication. The performance of a network in aggregate might still increase, of course, but that still means that we have to extract extra performance from co-ordinating distributed processors rather than from improving individual nodes. The history of distributed parallel processing should warn us not to be sanguine about &lt;em&gt;that&lt;/em&gt; prospect.

Actually, though, the challenge will do us good. Modern machines encourage sloppy over-engineering and over-generalisation -- building frameworks for situations that we anticipate but which might never occur. Targeting small machines will change this, and instead encourage us to build software that's fit for immediate purpose, &lt;em&gt;and&lt;/em&gt; that's build to be evolved and extended over time alongside changing requirements and constraints. This building evolution into the core of the system will make for better engineering in the long run.&lt;/p&gt;</description><category>moore's law</category><category>performance</category><category>sensor networks</category><guid>https://simondobson.org/2010/03/05/things-that-wont-change/</guid><pubDate>Fri, 05 Mar 2010 06:30:44 GMT</pubDate></item></channel></rss>
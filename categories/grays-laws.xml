<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Simon Dobson (Posts about gray's laws)</title><link>https://simondobson.org/</link><description></description><atom:link href="https://simondobson.org/categories/grays-laws.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2023 &lt;a href="mailto:simoninireland@gmail.com"&gt;Simon Dobson&lt;/a&gt; </copyright><lastBuildDate>Thu, 18 May 2023 07:29:31 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Five big questions</title><link>https://simondobson.org/2010/03/26/five-big-questions/</link><dc:creator>Simon Dobson</dc:creator><description>&lt;p&gt;If you try to do everything, you always end up doing nothing. Which is why &lt;a href="http://www.simondobson.org/2010/03/both-ends-of-the-data-intensive-spectrum/"&gt;Gray's laws&lt;/a&gt; suggest searching for the twenty "big questions" in a field and then focusing-in the first five as the ones that'll generate the biggest return on the effort invested. So what are the five biggest open issues in programming for sensorised systems?

&lt;!--more--&gt;Of course we should start with a big fat disclaimer: these are &lt;em&gt;my&lt;/em&gt; five biggest open issues, which probably don't relate well to anyone else's -- but that's what blogs are for, right? :-) So here goes: five questions, with an associated suggestion for a research programme.

&lt;strong&gt;1. Programming with uncertainty.&lt;/strong&gt; This is definitely the one I feel is most important. I've &lt;a href="http://www.simondobson.org/2010/02/216/"&gt;mentioned before&lt;/a&gt; that there's a mismatch between traditional computer science and what we have to deal with for sensor systems: the input is uncertain and often of very poor quality, but the output  behaviour has to be a "best attempt" based on what's available and has to be robust against small perturbations due to noise and the like. But uncertainty is something that computers (and computer scientists) are quite bad at handling, so there's a major change that has to happen.

To deal with this we need to re-think the programming models we use, and the ways in which we express behaviour. For example we could look at how programs respond to perturbation, or design languages in which perturbations have a small impact by design. A calculus of &lt;em&gt;stable functions&lt;/em&gt; might be a good starting-point, where perturbation tends to die-out over time and space, but long-term changes are propagated. We might also look at how to program more effectively with Bayesian statistics, or how to program with machine leaning: turn things that are currently either libraries or applications into core constructs from which to build programs.

&lt;strong&gt;2. Modeling adaptive systems as a whole.&lt;/strong&gt; We've had a huge problem getting systems to behave according to specification: now we propose that they adapt in response to changing circumstances. Clearly the space of possible stimuli and responses are too large for exhaustive testing, or for formal model-checking, so correctness becomes a major issue. What we're really interested in, of course, isn't so much specifying &lt;em&gt;what happens&lt;/em&gt; as much as how what happens &lt;em&gt;changes over time and with context&lt;/em&gt;.

Holistic models are common in physics but uncommon in computer science, where more discrete approaches (like model checking) have been more popular. It's easy to see why this is the case, but a small-scale, pointwise formal method doesn't feel appropriate to the scale of the problem. Reasoning about a system as a whole means re-thinking how we express both specifications and programs. But the difference is target is important too: we don't need to capture all the detail of a program's behaviour, just those aspects that relate to properties like stability, response time, accuracy and the like -- a macro method for reasoning about macro properties, not something that gets lost in the details. Dynamical systems might be a good model, at least at a conceptual level, with adaptation being seen as a "trajectory" through the "space" of acceptable parameter values. At the very least this makes adaptation an object of study in its own right, rather than being something that happens within another, less well-targeted model.

&lt;strong&gt;3. Modeling complex space- and time-dependent behaviours.&lt;/strong&gt; Reasoning systems and classifiers generally only deal with instants: things that are decided by the state of the system now, or as what immediately follows from now. In many cases what happens is far richer than this, and one can make predictions (or at least calculate probabilities) about the future based on classifying a person or entity as being engaged in a particular process. In pervasive computing this manifests itself as the ways in which people move around a space, the services they access preferentially in some locations rather than others, and so forth. These behaviours are closely tied-up with the way people move and the way their days progress, as it were: complex spatio-temporal processes giving rise to complex behaviours. The complexities come from how we divide-up people's actions, and how the possibilities branch to give a huge combinatorial range of possibilities -- not all of which are equally likely, and so can be leveraged.

A first step at addressing this would be to look at how we represent real-world spatio-temporal processes with computers. Of course we represent such processes all the time as programs, but (linking back to point 1 above) the uncertainties involved are such that we need to think about these things in new ways. We have a probabilistic definition of the potential future evolutions, against which we need to be able to express behaviours synthesising the "best guesses" we can make and simultaneously use the data we actually observe to validate or refute our predictions and refine our models. The link between programming and the modelingthat underlies it looks surprisingly intimate.

&lt;strong&gt;4. Rich representations of linked data.&lt;/strong&gt; Sensors generate a &lt;em&gt;lot&lt;/em&gt; of data. Much of it has long-term value, if only for verification and later re-study. Keeping track of all this data is going to become a major challenge. It's not something that the scientists for whom it's collected are generally very good at -- and why should they be, given that their interests are in the science and not in data management? But the data has to be kept, has to be retrievable, and has to be associated with enough metadata to make its properly and validly interpretable in the future.

Sensor mark-up languages like &lt;a href="http://www.opengeospatial.org/standards/sensorml"&gt;SensorML&lt;/a&gt; are a first step, but &lt;em&gt;only&lt;/em&gt; a first step. There's also the issue of the methodology by which the data was collected, and especially (returning to point 2) were the behaviours of the sensors consistent with gaining a valid view of the phenomena of interest? That means linking data to process descriptions, or to code, so that we can track-back through the provenance to ensure integrity. Then we can start applying reasoners to classify and derive information automatically from the data, secure in the knowledge that we have an audit trail for the science.

&lt;strong&gt;5. Making it easier to build domain-specific languages for real.&lt;/strong&gt; A &lt;a href="http://en.wikipedia.org/wiki/Domain-specific_language"&gt;lot has been said about DSLs&lt;/a&gt;, much of it negative: if someone's learned C (or Java, or Fortran, or Matlab, or Perl, or...) they won't want to then learn something else just to work in a particular domain. This argument holds that it's therefore more appropriate to provide advanced functions as libraries accessed from a common host language (or a range of languages). The counter-argument is that libraries only work around the edges of a language and can't provide the strength of optimisation, type-checking and new constructs needed. I suspect that there's truth on both sides, and I also suspect that many power users would gladly learn a new language if it &lt;em&gt;really&lt;/em&gt; matched their domain and &lt;em&gt;really&lt;/em&gt; gave them leverage.

Building DSLs is too complicated, though, especially for real-world systems that need to run with reasonable performance on low-grade hardware. A good starting-point might be a system that would allow libraries to be wrapped-up with language-like features -- like &lt;a href="http://en.wikipedia.org/wiki/Tcl"&gt;Tcl&lt;/a&gt; was intended for, but with more generality in terms of language constructs and types. A simpler language-design framework would facilitate work on new languages (as &lt;em&gt;per&lt;/em&gt; point 1 above), and would allow us to search for modes of expression closer to the semantic constructs we think are needed (&lt;em&gt;per&lt;/em&gt; points 2 and 3): starting from semantics and deriving a language rather than &lt;em&gt;vice versa&lt;/em&gt;.&lt;/p&gt;</description><category>Blog</category><category>gray's laws</category><category>strategy</category><guid>https://simondobson.org/2010/03/26/five-big-questions/</guid><pubDate>Fri, 26 Mar 2010 06:00:40 GMT</pubDate></item><item><title>Both ends of the data-intensive spectrum</title><link>https://simondobson.org/2010/03/22/both-ends-of-the-data-intensive-spectrum/</link><dc:creator>Simon Dobson</dc:creator><description>&lt;p&gt;Data-intensive science (or "web science" as it is sometimes known) has received a major boost from the efforts of Googlle and others, with the availability of enormous data sets against which we can learn. It's easy to see that such large-scale data affects experimental science, but there are lessons further down the scale too.

&lt;!--more--&gt;I spent part of last week at a workshop on data-intensive computing hosted by the &lt;a href="http://www.nesc.ac.uk"&gt;National e-Science Centre&lt;/a&gt; in Edinburgh. It was a fscinating meeting, and I very grateful for having been invited. Most of the sessions focussed on the challenges of petabyte and exabyte data, but it struck me that many of the points were actually relative rather than absolute: problems that are data-intensive because they have large amounts of data relative to the processing power you can deploy against them. This got me thinking to what extent the characteristics of data-intensive systes can be applied to sensor systems too.

One of the most interesting points was made early on, by &lt;a href="http://www.sdss.jhu.edu/~szalay/"&gt;Alex Szalay&lt;/a&gt; of the &lt;a href="http://www.sdss.org/"&gt;Sloan Digital Sky Survey&lt;/a&gt;, who set out some desiderata for data intensive computing first made by the late &lt;a href="http://en.wikipedia.org/wiki/Jim_Gray_%28computer_scientist%29"&gt;Jim Gray&lt;/a&gt; -- "Gray's laws":
&lt;/p&gt;&lt;ol&gt;
    &lt;li&gt; Scientific computing revolves around data — &lt;em&gt;not&lt;/em&gt; computing&lt;/li&gt;
    &lt;li&gt; Build solutions that intrisically can scale-out  as required&lt;/li&gt;
    &lt;li&gt; Take the analysis to  the data — because the interesting data's almost certainly too big to move, even with  fast backbones&lt;/li&gt;
    &lt;li&gt; Start with asking for"20 queries" — the most important questions— and recognise that the first 5 will be  by far the most important&lt;/li&gt;
    &lt;li&gt; Go from "working to working" — assume that  the infrastructure will change every 2 — 5 years, and  design hardware and software accordingly&lt;/li&gt;
&lt;/ol&gt;
This is advice hard-won from practice, and it's easy to see how it affects the largest-scale systems. I think Gray's laws also work at the micro-scale of sensor networks, and at points in between.

Data-intensive science is perhaps better envisioned as data-&lt;em&gt;driven&lt;/em&gt; science, in which the data drives the design and evolution computation. This view unifies large- and small-scales: a sensor network needs to respond to the observations it makes of the phenomane it's sensing, even though the scale of the data (for an individual node) is so small.

By focusing on networking we can scale-out solutions, but we also need to consider that several different networks may be needed to take in the different aspects of systems being observed. It's a &lt;a href="https://simondobson.org/2010/03/things-that-wont-change/"&gt;mistake&lt;/a&gt; to think that we can grow the capabilities of individual nodes too much, since that starts to eat into power budgets. At a data centre level, scale-out tends to mean virtualisation and replication: proven parallel-processing design patterns. At a network level, though, I suspect it means composition and co-design, which we understand substantially less well.

Taking the analysis to the data means pushing processing down into the network and reducing the amount of raw data that needs to be returned to a base station. This is a slightly contentious point: do I &lt;em&gt;want&lt;/em&gt; to limit the raw data I collect, or should I grab it all in case something emerges later that I need to check against a raw stream? In other words, can I define the information I want to extract from the data stream sufficiently clearly to avoid storing it all? This same point was made by &lt;a href="http://www.roe.ac.uk/~afh/"&gt;Alan Heavens&lt;/a&gt; at the meeting, pointing out that one can do radical data discarding if one has a strong enough model of the pheonmenon against which to evaluate the raw data. Again, the point may be the scale if the data &lt;em&gt;relative to the platform on which it's being processed&lt;/em&gt; rather than in any absolute sense: astonomical data is too, well, "astronomical" to retain even in a large data centre, while sensor data is large relative to node capabilities. It's an open question whether many systems have strong enough data models to support aggressive discarding, though.

The "20 queries" goal is I think key to many things: identify the large questions and address them first. (Richard Hamming &lt;a href="http://www.cs.virginia.edu/~robins/YouAndYourResearch.html"&gt;made a similar point&lt;/a&gt; with regard to research as a whole.) Coupling sensing research to the science (and public policy formation) that needs it is the only way to do this, and strikes me as at least as important as theoretical advances in network sensing science. The engineering challenges of (for example) putting a sensor network into a field are at least as valuable -- and worthy of support -- as the basic underpinnings.

The coupling of computer and physical science also speaks the the need for designing systems for upgrade. The techniques for doinjg this -- component design and so forth -- are well-explored by computer scientists and under-understood by many practitioners from other disciplines. Designing sensor &lt;em&gt;and&lt;/em&gt; data systems for expansion and re-tasking should form a backbone of any research effort.

So I think Jim Gray's pioneering insights into large-scale data may actually be broader than we think, and might also characterise the individually small-scale -- but collectively widespread -- instrumentation of the physical world. It also suggests that there are end-to-end issues that can usefully form part of the research agenda.</description><category>Blog</category><category>e-science</category><category>gray's laws</category><guid>https://simondobson.org/2010/03/22/both-ends-of-the-data-intensive-spectrum/</guid><pubDate>Mon, 22 Mar 2010 11:54:52 GMT</pubDate></item></channel></rss>
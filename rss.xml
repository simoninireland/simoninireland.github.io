<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Simon Dobson</title><link>https://simondobson.org/</link><description>Aut tace aut loquere meliora silentio</description><atom:link href="https://simondobson.org/rss.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2024 &lt;a href="mailto:simoninireland@gmail.com"&gt;Simon Dobson&lt;/a&gt; </copyright><lastBuildDate>Mon, 10 Jun 2024 11:59:51 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Processing MicroMoth recordings offline</title><link>https://simondobson.org/2024/06/10/processing-the-data-offline/</link><dc:creator>Simon Dobson</dc:creator><description>&lt;div id="outline-container-orgfccf4b6" class="outline-2"&gt;
&lt;h2 id="orgfccf4b6"&gt;Processing MicroMoth recordings offline&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgfccf4b6"&gt;
&lt;p&gt;
The uMoth generates &lt;code&gt;.wav&lt;/code&gt; files, uncompressed waveforms of what it
records. These need to be processed to identify any bird calls
within them.
&lt;/p&gt;

&lt;p&gt;
This function is integrated in BirdNET-Pi, which does recording and
classification, and provides a web GUI. With the uMoths we need to
provide classification as part of a data processing pipeline. We
can however make direct use of the classifier "brain" within
BirdNET-PI, which is unsurprisingly called &lt;a href="https://github.com/kahst/BirdNET-Analyzer"&gt;BirdNET-Analyzer&lt;/a&gt;.
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-org1ca5243" class="outline-3"&gt;
&lt;h3 id="org1ca5243"&gt;Installation&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org1ca5243"&gt;
&lt;p&gt;
I'm working on a 16-core Intel Core i7@3.8GHz running Arch Linux.
&lt;/p&gt;

&lt;p&gt;
First we clone the BirdNET-Analyzer repo. This takes a long time
as it includes the ML models, some of which are 40MB or more.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    git clone https://github.com/kahst/BirdNET-Analyzer.git
    cd BirdNET-Analyzer
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
The repo includes a Docker file that we can use to build the
analyser in a container.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    docker build .
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
The container setup is quite basic and is probably intended for
testing rather than production, but it gives a usable system that
could then be embedded into something more usable. The core of the
system is the &lt;code&gt;analyze.py&lt;/code&gt; script.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orge6466b1" class="outline-3"&gt;
&lt;h3 id="orge6466b1"&gt;Analysing some data (AKA identifying some birds!)&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orge6466b1"&gt;
&lt;p&gt;
The container as defined looks into its &lt;code&gt;/example&lt;/code&gt; directory for
waveforms and analyses them, generating text file for each sample.
The easiest way to get it to analyse captured data is to mount a
data directory of files onto this mount point (thereby shadowing
the example waveform provided).
&lt;/p&gt;

&lt;p&gt;
There are various parameters that configure the classifier. I
copied the &lt;a href="/2024/05/19/first-installation/"&gt;defaults I was using with BirdNET-Pi&lt;/a&gt;, only accepting classifications
at or above 0.7 confidence.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    docker run -v /var/run/media/sd80/DATA:/example birdnet-analyzer analyze.py --rtype=csv --min_conf=0.7 --sensitivity=1.25
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
This crunches through all the files (982 of them from my first
run) and generates a CSV file for each. An example is:
&lt;/p&gt;

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-left"&gt;

&lt;col class="org-left"&gt;

&lt;col class="org-right"&gt;
&lt;/colgroup&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-right"&gt;Start (s)&lt;/td&gt;
&lt;td class="org-right"&gt;End (s)&lt;/td&gt;
&lt;td class="org-left"&gt;Scientific name&lt;/td&gt;
&lt;td class="org-left"&gt;Common name&lt;/td&gt;
&lt;td class="org-right"&gt;Confidence&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-right"&gt;6.0&lt;/td&gt;
&lt;td class="org-right"&gt;9.0&lt;/td&gt;
&lt;td class="org-left"&gt;Corvus monedula&lt;/td&gt;
&lt;td class="org-left"&gt;Eurasian Jackdaw&lt;/td&gt;
&lt;td class="org-right"&gt;0.9360&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-right"&gt;9.0&lt;/td&gt;
&lt;td class="org-right"&gt;12.0&lt;/td&gt;
&lt;td class="org-left"&gt;Corvus monedula&lt;/td&gt;
&lt;td class="org-left"&gt;Eurasian Jackdaw&lt;/td&gt;
&lt;td class="org-right"&gt;0.8472&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-right"&gt;12.0&lt;/td&gt;
&lt;td class="org-right"&gt;15.0&lt;/td&gt;
&lt;td class="org-left"&gt;Corvus monedula&lt;/td&gt;
&lt;td class="org-left"&gt;Eurasian Jackdaw&lt;/td&gt;
&lt;td class="org-right"&gt;0.8681&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-right"&gt;15.0&lt;/td&gt;
&lt;td class="org-right"&gt;18.0&lt;/td&gt;
&lt;td class="org-left"&gt;Corvus monedula&lt;/td&gt;
&lt;td class="org-left"&gt;Eurasian Jackdaw&lt;/td&gt;
&lt;td class="org-right"&gt;0.8677&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-right"&gt;24.0&lt;/td&gt;
&lt;td class="org-right"&gt;27.0&lt;/td&gt;
&lt;td class="org-left"&gt;Columba palumbus&lt;/td&gt;
&lt;td class="org-left"&gt;Common Wood-Pigeon&lt;/td&gt;
&lt;td class="org-right"&gt;0.9198&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-right"&gt;27.0&lt;/td&gt;
&lt;td class="org-right"&gt;30.0&lt;/td&gt;
&lt;td class="org-left"&gt;Columba palumbus&lt;/td&gt;
&lt;td class="org-left"&gt;Common Wood-Pigeon&lt;/td&gt;
&lt;td class="org-right"&gt;0.7716&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-right"&gt;45.0&lt;/td&gt;
&lt;td class="org-right"&gt;48.0&lt;/td&gt;
&lt;td class="org-left"&gt;Corvus monedula&lt;/td&gt;
&lt;td class="org-left"&gt;Eurasian Jackdaw&lt;/td&gt;
&lt;td class="org-right"&gt;0.8023&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-right"&gt;48.0&lt;/td&gt;
&lt;td class="org-right"&gt;51.0&lt;/td&gt;
&lt;td class="org-left"&gt;Corvus monedula&lt;/td&gt;
&lt;td class="org-left"&gt;Eurasian Jackdaw&lt;/td&gt;
&lt;td class="org-right"&gt;0.7696&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;
Those are entirely credible identifications. The start- and
end-point offsets allow rough location within the recording.
(BirdNET segments the recordings into 3s chunks for analysis.)
&lt;/p&gt;

&lt;p&gt;
This is clearly not as straightforward as BirdNET-Pi, nor as
immediately satisfying. But it does scale to analysing lots of
data (and could be made to do so even better, with a better
front-end to the container), which is important for any
large-scale deployment.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>acoustic</category><category>project:acoustic-birds</category><category>sensing</category><guid>https://simondobson.org/2024/06/10/processing-the-data-offline/</guid><pubDate>Mon, 10 Jun 2024 11:57:22 GMT</pubDate></item><item><title>Deploying a MicroMoth</title><link>https://simondobson.org/2024/06/10/configuring-the-board/</link><dc:creator>Simon Dobson</dc:creator><description>&lt;div id="outline-container-orgf2790be" class="outline-2"&gt;
&lt;h2 id="orgf2790be"&gt;Deploying a MicroMoth&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgf2790be"&gt;
&lt;p&gt;
The MicroMoth (or uMoth) from &lt;a href="https://simondobson.org/2024/06/10/configuring-the-board/personal/notebook/open_acoustic_devices-contact.html#ID-c0a38f8e-970a-499d-a30e-56ec235b6ea9"&gt;Open Acoustic Devices&lt;/a&gt; is the same as
their better-known AudioMoth recorder but with a significantly
smaller footprint. It's just a traditional recorder or data-logger,
with now on-board analysis and no wireless connectivity. I got hold
of some to use in a larger project we're thinking about running, and
they're not kidding about the "micro" part.
&lt;/p&gt;

&lt;p&gt;
&lt;img src="https://simondobson.org/attachments/4b/0ad2d7-5cb3-4b93-a0ec-4963e2868155/IMG_20240528_144150.jpg" alt="nil"&gt;
&lt;/p&gt;

&lt;p&gt;
The uMoth uses the same software as the AudioMoth, and therefore
the same configuration app available from the &lt;a href="https://www.openacousticdevices.info/applications"&gt;apps page&lt;/a&gt; – for
64-bit Linux in my case. It downloads as a &lt;code&gt;.appimage&lt;/code&gt; file, which
is simply a self-contained archive. It needed to be marked as
executable, and then ran directly from a double-click. (The page
suggests that there may be some extra steps for some Linux
distros: there weren't for Arch.)
&lt;/p&gt;

&lt;p&gt;
I then followed the &lt;a href="https://www.openacousticdevices.info/config-app-guide"&gt;configuration guide&lt;/a&gt;. The time is set
automatically from the computer's clock when you configure the
device.
&lt;/p&gt;

&lt;p&gt;
For testing I chose two recording periods, 0400–0800 and
1400–1600.
&lt;/p&gt;

&lt;p&gt;
&lt;img src="https://simondobson.org/attachments/4b/0ad2d7-5cb3-4b93-a0ec-4963e2868155/config.png" alt="nil"&gt;
&lt;/p&gt;

&lt;p&gt;
As shown this will, with the default 48KHz sampling, generate
about 2GB of data per day and use about 70mAh of energy. For my
tests I just hung the device out of the window on a USB tether for
power: it works fine drawing power from the USB rather than from
the battery connector.
&lt;/p&gt;

&lt;p&gt;
&lt;img src="https://simondobson.org/attachments/4b/0ad2d7-5cb3-4b93-a0ec-4963e2868155/IMG_20240608_141527.jpg" alt="nil"&gt;
&lt;/p&gt;

&lt;p&gt;
This turned out not to record anything, because the time is lost
if the power is disconnected, even though the configuration is
retained. (The manual does actually say this, with a suitably
close reading. It could be clearer.) There's a smartphone app that
can reset the time once the device is in the field and powered-up,
though, by making an audio chime that encodes the current time and
location in a way the board can understand. Flashing the device
with the "Always require acoustic chime on switching to CUSTOM"
makes it wait after power is applied until its time is set.
&lt;/p&gt;

&lt;p&gt;
The red LED flashes when the device is recording. The green LED
flashes when the device is waiting for a recording period to
start. The red LED stays lit while the time is unset.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>acoustic</category><category>project:acoustic-birds</category><category>sensing</category><guid>https://simondobson.org/2024/06/10/configuring-the-board/</guid><pubDate>Mon, 10 Jun 2024 10:11:02 GMT</pubDate></item><item><title>Pascal Costanza's highly opinionated guide to Lisp</title><link>https://simondobson.org/2024/05/27/pascal-costanza1s-highly-opinionated-guide-to-lisp/</link><dc:creator>Simon Dobson</dc:creator><description>&lt;div id="outline-container-org9a991ba" class="outline-2"&gt;
&lt;h2 id="org9a991ba"&gt;Pascal Costanza's highly opinionated guide to Lisp&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org9a991ba"&gt;
&lt;p&gt;
&lt;a href="https://www.p-cos.net/lisp/guide.html"&gt;Pascal Costanza's Highly Opinionated Guide to Lisp&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;
Part introduction, part paean to the language's power, part study
guide, while dipping into an eclectically-chosen subset of Lisp
features that really illustrate what makes it different.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>experience</category><category>lisp</category><category>project:lisp-bibliography</category><guid>https://simondobson.org/2024/05/27/pascal-costanza1s-highly-opinionated-guide-to-lisp/</guid><pubDate>Mon, 27 May 2024 16:04:50 GMT</pubDate></item><item><title>A road to Common Lisp</title><link>https://simondobson.org/2024/05/27/a-road-to-common-lisp/</link><dc:creator>Simon Dobson</dc:creator><description>&lt;div id="outline-container-orga7343e8" class="outline-2"&gt;
&lt;h2 id="orga7343e8"&gt;A road to Common Lisp&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orga7343e8"&gt;
&lt;p&gt;
&lt;a href="https://stevelosh.com/blog/2018/08/a-road-to-common-lisp/"&gt;A Road to Common Lisp&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;
This a really brief, yet really interesting, approach to
introducing Lisp to someone. Interesting because it covers all the
usual ground, but also has copious pointers to other material
slightly-beyond-introductory ("Where to go from here"). It also
links to material that's essential to modern practice, such as
Lisp packages and systems, and the essential "standard libraries"
such as Alexandria, Bordeaux, CL-PPCRE, usocket, and the like: the
things that are needed in practice and which in other languages
would probably be built-in and included directly in an
introduction.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>lisp</category><category>project:lisp-bibliography</category><category>tutorial</category><guid>https://simondobson.org/2024/05/27/a-road-to-common-lisp/</guid><pubDate>Mon, 27 May 2024 16:00:55 GMT</pubDate></item><item><title>First installation of BirdNET-Pi</title><link>https://simondobson.org/2024/05/19/first-installation/</link><dc:creator>Simon Dobson</dc:creator><description>&lt;div id="outline-container-orgbcc6fc2" class="outline-2"&gt;
&lt;h2 id="orgbcc6fc2"&gt;First installation of BirdNET-Pi&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgbcc6fc2"&gt;
&lt;p&gt;
The BirdNET-Pi system aims to provide out-of-the-box bird
identification. It's slightly more awkward than that, but still
pretty straightforward to get up and running.
&lt;/p&gt;

&lt;p&gt;
My first hardware plan was to use a Raspberry Pi Zero as the
compute host with a &lt;a href="https://www.waveshare.com/wm8960-audio-hat.htm"&gt;Waveshare WM8960 HAT&lt;/a&gt; for the sound capture. It
turns out that BirdNET needs a 64-bit platform – why I'm not sure
– and the Pi Zero only runs 32-bit Linux. I therefore moved to a
Raspberry Pi B that I had lying around, and put a 64-bit "lite"
install on it to run headless.
&lt;/p&gt;

&lt;p&gt;
I then basically just followed the &lt;a href="https://github.com/mcguirepr89/BirdNET-Pi/wiki/Installation-Guide"&gt;installation guide&lt;/a&gt;. There was an
issue with the installation script when cloning the GitHub repo: I
suspect this was because of limited memory on the Pi. I downloaded
manually, and manually ran the rest of the install script, which did
a &lt;i&gt;lot&lt;/i&gt; of setup of services and a PHP web server.
&lt;/p&gt;

&lt;p&gt;
I compiled the &lt;a href="https://www.waveshare.com/wiki/WM8960_Audio_HAT"&gt;drivers&lt;/a&gt; for the HAT, which worked fine. The new
sound card is recognised but is not the system default.
&lt;/p&gt;

&lt;p&gt;
The installed components seem to include:
&lt;/p&gt;

&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;code&gt;icecast2&lt;/code&gt;, a streaming server, used to replay recordings&lt;/li&gt;
&lt;li&gt;&lt;code&gt;caddy&lt;/code&gt; web server&lt;/li&gt;
&lt;li&gt;PHP for serving the web pages&lt;/li&gt;
&lt;li&gt;&lt;code&gt;arecord&lt;/code&gt; to actually record audio&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ffmpeg&lt;/code&gt; to extract waveforms&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sqlite for the database&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;the actual machine learning model used for recognition&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;
The recognition models are built with TensorFlow. This is a great
example of how the standard Linux tools and services can be combined
to get a scientific-grade sensor platform. (Caddy doesn't seem to be
running over TLS by default, which would be an issue outside a
firewall.)
&lt;/p&gt;

&lt;p&gt;
Since the sound card isn't the default, the easiest way to get the
system listening to the right mics is to change the device in the
"advanced settings" panel: in my case I changed from "default" to
"hw:2,0", reflecting the output of &lt;code&gt;arecord -l&lt;/code&gt; that shows the sound
card devices.
&lt;/p&gt;

&lt;p&gt;
I then deployed the Pi out of the kitchen window.
&lt;/p&gt;

&lt;p&gt;
&lt;img src="https://simondobson.org/attachments/18/408cf7-5f1f-4b07-81ef-e8f94d1c38d3/screenshot.png" alt="nil"&gt;
&lt;/p&gt;

&lt;p&gt;
To start with it wasn't hearing anything, which I think may be
because of the waterfall in the courtyard: turning this off made
things much more effective:
&lt;/p&gt;

&lt;p&gt;
&lt;img src="https://simondobson.org/attachments/18/408cf7-5f1f-4b07-81ef-e8f94d1c38d3/screenshot-1.png" alt="nil"&gt;
&lt;/p&gt;

&lt;p&gt;
That's an appropriate set of birds being seen – and we hardly ever
&lt;i&gt;see&lt;/i&gt; magpies, but know they're around. There's actually quite a lot
of background noise even in such a quiet village, but the bird
calls do stand out.
&lt;/p&gt;

&lt;p&gt;
I can't see any reason for the manual installation on bare metal:
as far as I can see everything could be containerised, which would
make deployment and management a &lt;i&gt;lot&lt;/i&gt; easier.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>acoustic</category><category>project:acoustic-birds</category><category>raspberry pi</category><category>sensing</category><guid>https://simondobson.org/2024/05/19/first-installation/</guid><pubDate>Sun, 19 May 2024 14:19:45 GMT</pubDate></item><item><title>epydemic hits 100,000 downloads</title><link>https://simondobson.org/2024/05/08/epydemic-100000/</link><dc:creator>Simon Dobson</dc:creator><description>&lt;div id="outline-container-org74f003b" class="outline-2"&gt;
&lt;h2 id="org74f003b"&gt;epydemic hits 100,000 downloads&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org74f003b"&gt;
&lt;p&gt;
Downloads of &lt;code&gt;epydemic&lt;/code&gt;, my &lt;a href="/development/projects/epydemic/"&gt;library&lt;/a&gt; for epidemic (and other) process
simulation on networks (and hopefully other combinatorial structures
soon…) recently passed 100,000.
&lt;/p&gt;

&lt;p&gt;
That number comes from the project's &lt;a href="https://www.pepy.tech/projects/epydemic"&gt;PePy page&lt;/a&gt;, which tracks
downloads from the main &lt;a href="https://pypi.org/project/epydemic/"&gt;PyPi page&lt;/a&gt; as used by &lt;code&gt;pip&lt;/code&gt;. I can't say
whether or not that number is accurate. Quite honestly it's at least
98,000 more than I ever expected, but 100,000 feels like something
of a milestone to be pleased about.
&lt;/p&gt;

&lt;p&gt;
&lt;code&gt;epydemic&lt;/code&gt; came about because of a lack of standard tooling for doing
epidemic simulation. This involves a lot of stochastic simulation,
which is quite tricky code to write and to make efficient. Testing
the system actually involved us thinking more deeply about the
effectiveness of unit testing for stochastic code, which then led to
a &lt;a href="/softcopy/stochastic-testing-21.pdf"&gt;presentation at UK Systems in 2021&lt;/a&gt; explaining the problems we'd
had.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>epydemic</category><category>software</category><guid>https://simondobson.org/2024/05/08/epydemic-100000/</guid><pubDate>Wed, 08 May 2024 15:21:29 GMT</pubDate></item><item><title>TIL: The loudest Lisp program in the world</title><link>https://simondobson.org/2024/05/03/til-the-loudest-lisp-program-in-the-world/</link><dc:creator>Simon Dobson</dc:creator><description>&lt;div id="outline-container-org69b2fff" class="outline-2"&gt;
&lt;h2 id="org69b2fff"&gt;TIL: The loudest Lisp program in the world&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org69b2fff"&gt;
&lt;p&gt;
Today I learned about a program that generates the sounds that help
people navigate as they exit long tunnels when an emergency such as
a fire has destroyed the visibility.
&lt;/p&gt;

&lt;p&gt;
&lt;a href="https://blog.funcall.org//lisp%20psychoacoustics/2024/05/01/worlds-loudest-lisp-program/"&gt;The World's Loudest Lisp Program to the Rescue&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;
This describes the challenges of building a software system that has
to work unmonitored once deployed, for years, as well as
withstanding a fairly rugged environment where, for example, the
installedc hardware will be periodically sprayed with a
high-pressure hose as the walls get cleaned. Overall the system is
soft real-time, but has to cope with component failure, network
partitions, consensus, and all the usual distributed systems
challenges, while be guaranteed to work when needed.
&lt;/p&gt;

&lt;p&gt;
The developers built the system in Common Lisp, which wouldn't be
the normal go-to choice for an embedded system. But their argument
was that they could better handle complex and changing requirements
by retaining a high level of abstraction, and that development was
overall far faster than using C. Modern Common Lisp compilers are so
efficient that there's no significant performance hit at deployment.
They made use of complicated components like planners (for which
Lisp is an ideal choice), and built a set of macros to wrap-up the
handling of industrial control and robust communications.
&lt;/p&gt;

&lt;p&gt;
It's a great read.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>lisp</category><category>programming</category><category>til</category><guid>https://simondobson.org/2024/05/03/til-the-loudest-lisp-program-in-the-world/</guid><pubDate>Fri, 03 May 2024 08:50:16 GMT</pubDate></item><item><title>Beyond</title><link>https://simondobson.org/goodreads/beyond/</link><dc:creator>Simon Dobson</dc:creator><description>&lt;div&gt;
    &lt;div&gt;
      &lt;img src="https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1639479657l/58849383._SX98_.jpg" style="float: left; margin-right: 10px"&gt;
      &lt;h2&gt;
	Stephen Walker
	  (2021)
      &lt;/h2&gt;
    &lt;/div&gt;
    &lt;p&gt;
      The space race from the American perspective is well-known: this book presents the same race from the Soviet perspective.&lt;br&gt;&lt;br&gt;The differences are profound, of course, mainly driven by the Soviet programme occurring in complete secrecy to duck the risks that failure would have on the programme's image. The American programme by contrast was conducted in the open at least as far as the actual "shots" were concerned – and their public failures did indeed endanger their ability to continue, at least in part because neither public nor politicians understood the process of engineering or just how &lt;i&gt;hard&lt;/i&gt; each shot was. The deep irony is that the Soviet successes appeared so sudden and dramatic they led directly to the deepening of the American programme and commitment or more money and effort than the Soviets seemed able to maintain to capitalise on their early lead.&lt;br&gt;&lt;br&gt;There are plenty of revelations even for those with a detailed interest in space history. I remember hearing all through the 1970s that the Soviet spacecraft landed on land, but it turns out that in many cases the cosmonauts were bailing-out and parachuting to earth instead, with this being intensively covered-up even to the extent of falsifying reports to the international organisation responsible for certifying "firsts" in space.&lt;br&gt;&lt;br&gt;While the achievements of both programmes were profound, they were perhaps doomed in the long term by a lack of vision for what they were actually &lt;i&gt;for&lt;/i&gt;. The benefits of space technology are now obvious; those of exploration perhaps less so, although it doesn't (in my opinion) require much suspension of disbelief to feel that science-driven activities lead almost inevitably to enormously valuable spin-offs. The fact that we can't quantify these &lt;i&gt;a priori&lt;/i&gt; shouldn't (in my opinion, again) stop us keeping the faith in the value of experiments that advance our science &lt;i&gt;and&lt;/i&gt; engineering in ways that wouldn't otherwise happen.
      &lt;/p&gt;&lt;p&gt;
	5/5.
	  Finished Sunday 21 April, 2024.
	&lt;/p&gt;&lt;p&gt;
	  (Originally published on &lt;a href="https://www.goodreads.com/review/show/6395477584?utm_medium=api&amp;amp;utm_source=rss"&gt;Goodreads&lt;/a&gt;.)
  &lt;/p&gt;&lt;/div&gt;</description><category>bonanza</category><category>books</category><category>reviews</category><guid>https://simondobson.org/goodreads/beyond/</guid><pubDate>Sun, 21 Apr 2024 00:00:00 GMT</pubDate></item><item><title>The Emergence of Numerical Weather Prediction: Richardson's Dream</title><link>https://simondobson.org/goodreads/the-emergence-of-numerical-weather-prediction-richardsons-dream/</link><dc:creator>Simon Dobson</dc:creator><description>&lt;div&gt;
    &lt;div&gt;
      &lt;img src="https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1348662620l/275229._SX98_.jpg" style="float: left; margin-right: 10px"&gt;
      &lt;h2&gt;
	Peter   Lynch
	  (2006)
      &lt;/h2&gt;
    &lt;/div&gt;
    &lt;p&gt;
      A very technical examination of the world's first numerical weather "prediction" – although in fact it was really a "postdiction", taking detailed data and using it to compute a scenario that could be compared against a known ground truth. It was an incredible achievement anyway, performing manually and at low resolution the calculations now routinely performed by computers.&lt;br&gt;&lt;br&gt;Richardson was the person who saw that this would be possible, realising that the physics and mathematics could be solved even though the computational capabilities didn't exist. In this he foresaw the emergence of the modern power of data, where the existence of more and better data transforms both the way we do science and the sciene that we do. It's something that should put him alongside Turing and Von Neumann as visionaries of what computation could achieve.
      &lt;/p&gt;&lt;p&gt;
	4/5.
	  Finished Sunday 21 April, 2024.
	&lt;/p&gt;&lt;p&gt;
	  (Originally published on &lt;a href="https://www.goodreads.com/review/show/6326779093?utm_medium=api&amp;amp;utm_source=rss"&gt;Goodreads&lt;/a&gt;.)
  &lt;/p&gt;&lt;/div&gt;</description><category>books</category><category>reviews</category><guid>https://simondobson.org/goodreads/the-emergence-of-numerical-weather-prediction-richardsons-dream/</guid><pubDate>Sun, 21 Apr 2024 00:00:00 GMT</pubDate></item><item><title>TIL: Cognitohazards</title><link>https://simondobson.org/2024/04/12/til-cognitohazards/</link><dc:creator>Simon Dobson</dc:creator><description>&lt;div id="outline-container-org65a04c2" class="outline-2"&gt;
&lt;h2 id="org65a04c2"&gt;TIL: Cognitohazards&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org65a04c2"&gt;
&lt;p&gt;
Could social media posts be actively damaging to our mental health?
– literally, not just figuratively? That's the premise of &lt;a href="https://www.theguardian.com/technology/2024/apr/09/techscape-deepfakes-cognitohazards-science-fiction"&gt;a
TechScape article in The Guardian&lt;/a&gt;, that draws on both science
fiction and psychological research.
&lt;/p&gt;

&lt;p&gt;
In Neal Stephenson's "Snow crash" there is a plot device of an image
in a metaverse that, when viewed, crashes the viewer's brain. We
haven't seen this in social media (yet), but there's an increasing
concern about deepfake images and other forms of misinformation.
Research suggests that such images are damaging &lt;i&gt;even if viewers
know that they're fakes&lt;/i&gt;, which suggests that techniques like
content-labelling images as AI-generated are insufficient to remove
their harm. Other examples include massively engaging artificial
images such as the "pong wars" animation of two simultaneous
"Breakout" games going on between two algorithms: something that
shouldn't be as engaging as it is (as I can attest to myself).
&lt;/p&gt;

&lt;p&gt;
Social media attention grabbing at an industrial scale might
therefore constitute a &lt;i&gt;cognitohazard&lt;/i&gt;, a way of hacking people's
brains simply by being viewed.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>internet</category><category>social media</category><category>til</category><guid>https://simondobson.org/2024/04/12/til-cognitohazards/</guid><pubDate>Fri, 12 Apr 2024 11:18:58 GMT</pubDate></item></channel></rss>